{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=False, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "device = \"cpu\" # the PYTORCH_ENABLE_MPS_FALLBACK=1 did not work, so I hardcode it.\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        attention_logits = torch.matmul(q, k.permute(0,1,3,2)) / np.sqrt(self.head_dims)\n",
    "    \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "\n",
    "        attention_weights = torch.softmax(attention_logits, dim=-1)\n",
    "        \n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        \n",
    "        matmul = torch.matmul(attention_weights, v) \n",
    "        permute = matmul.permute(0,2,1,3) \n",
    "        attn_out = permute.reshape(b,n,self.proj_dims) \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.relu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.layernorm = nn.LayerNorm(self.hidden_dims)\n",
    "        self.self_attention = SelfAttention(self.hidden_dims, head_dims=self.hidden_dims//self.num_heads, num_heads=self.num_heads, bias=bias)\n",
    "        self.mlp = MLP(self.hidden_dims, self.hidden_dims, self.hidden_dims, bias=bias)\n",
    "        \n",
    "        \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        attention = self.self_attention(x)\n",
    "        x = x + attention\n",
    "        x = self.layernorm(x)\n",
    "        mlp = self.mlp(x)\n",
    "        x = x + mlp\n",
    "        return x\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size()) \n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size()) \n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.032 | Acc: 4.000% (1/25)\n",
      "Loss: 2.981 | Acc: 8.000% (4/50)\n",
      "Loss: 2.870 | Acc: 8.000% (6/75)\n",
      "Loss: 2.830 | Acc: 9.000% (9/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 9.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.606 | Acc: 12.000% (3/25)\n",
      "Loss: 2.921 | Acc: 8.000% (4/50)\n",
      "Loss: 2.843 | Acc: 12.000% (9/75)\n",
      "Loss: 2.767 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.420 | Acc: 12.000% (3/25)\n",
      "Loss: 2.401 | Acc: 10.000% (5/50)\n",
      "Loss: 2.535 | Acc: 10.667% (8/75)\n",
      "Loss: 2.532 | Acc: 13.000% (13/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 13.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.310 | Acc: 28.000% (7/25)\n",
      "Loss: 2.288 | Acc: 22.000% (11/50)\n",
      "Loss: 2.253 | Acc: 24.000% (18/75)\n",
      "Loss: 2.261 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.238 | Acc: 16.000% (4/25)\n",
      "Loss: 2.295 | Acc: 16.000% (8/50)\n",
      "Loss: 2.211 | Acc: 18.667% (14/75)\n",
      "Loss: 2.139 | Acc: 23.000% (23/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 23.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.462 | Acc: 16.000% (4/25)\n",
      "Loss: 2.460 | Acc: 14.000% (7/50)\n",
      "Loss: 2.316 | Acc: 16.000% (12/75)\n",
      "Loss: 2.248 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.782 | Acc: 44.000% (11/25)\n",
      "Loss: 1.950 | Acc: 34.000% (17/50)\n",
      "Loss: 2.013 | Acc: 29.333% (22/75)\n",
      "Loss: 2.055 | Acc: 24.000% (24/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 24.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.451 | Acc: 12.000% (3/25)\n",
      "Loss: 2.430 | Acc: 20.000% (10/50)\n",
      "Loss: 2.288 | Acc: 21.333% (16/75)\n",
      "Loss: 2.243 | Acc: 24.000% (24/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 24.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 2.051 | Acc: 32.000% (8/25)\n",
      "Loss: 1.949 | Acc: 34.000% (17/50)\n",
      "Loss: 1.927 | Acc: 30.667% (23/75)\n",
      "Loss: 1.881 | Acc: 32.000% (32/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 32.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.337 | Acc: 36.000% (9/25)\n",
      "Loss: 2.483 | Acc: 22.000% (11/50)\n",
      "Loss: 2.375 | Acc: 20.000% (15/75)\n",
      "Loss: 2.339 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.500 | Acc: 36.000% (9/25)\n",
      "Loss: 1.650 | Acc: 34.000% (17/50)\n",
      "Loss: 1.755 | Acc: 29.333% (22/75)\n",
      "Loss: 1.777 | Acc: 29.000% (29/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 29.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.261 | Acc: 28.000% (7/25)\n",
      "Loss: 2.301 | Acc: 22.000% (11/50)\n",
      "Loss: 2.291 | Acc: 21.333% (16/75)\n",
      "Loss: 2.375 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.922 | Acc: 28.000% (7/25)\n",
      "Loss: 1.912 | Acc: 28.000% (14/50)\n",
      "Loss: 1.828 | Acc: 28.000% (21/75)\n",
      "Loss: 1.765 | Acc: 28.000% (28/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 28.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.439 | Acc: 12.000% (3/25)\n",
      "Loss: 2.564 | Acc: 10.000% (5/50)\n",
      "Loss: 2.465 | Acc: 14.667% (11/75)\n",
      "Loss: 2.387 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.634 | Acc: 44.000% (11/25)\n",
      "Loss: 1.608 | Acc: 38.000% (19/50)\n",
      "Loss: 1.668 | Acc: 37.333% (28/75)\n",
      "Loss: 1.707 | Acc: 33.000% (33/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 33.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.661 | Acc: 24.000% (6/25)\n",
      "Loss: 2.664 | Acc: 20.000% (10/50)\n",
      "Loss: 2.537 | Acc: 20.000% (15/75)\n",
      "Loss: 2.491 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.478 | Acc: 44.000% (11/25)\n",
      "Loss: 1.503 | Acc: 44.000% (22/50)\n",
      "Loss: 1.524 | Acc: 42.667% (32/75)\n",
      "Loss: 1.504 | Acc: 44.000% (44/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 44.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.218 | Acc: 28.000% (7/25)\n",
      "Loss: 2.313 | Acc: 22.000% (11/50)\n",
      "Loss: 2.298 | Acc: 24.000% (18/75)\n",
      "Loss: 2.325 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 1.486 | Acc: 36.000% (9/25)\n",
      "Loss: 1.263 | Acc: 56.000% (28/50)\n",
      "Loss: 1.345 | Acc: 49.333% (37/75)\n",
      "Loss: 1.392 | Acc: 47.000% (47/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 47.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.410 | Acc: 24.000% (6/25)\n",
      "Loss: 2.561 | Acc: 22.000% (11/50)\n",
      "Loss: 2.488 | Acc: 25.333% (19/75)\n",
      "Loss: 2.431 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 1.283 | Acc: 56.000% (14/25)\n",
      "Loss: 1.217 | Acc: 54.000% (27/50)\n",
      "Loss: 1.224 | Acc: 53.333% (40/75)\n",
      "Loss: 1.261 | Acc: 50.000% (50/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 50.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.507 | Acc: 32.000% (8/25)\n",
      "Loss: 2.684 | Acc: 24.000% (12/50)\n",
      "Loss: 2.523 | Acc: 25.333% (19/75)\n",
      "Loss: 2.440 | Acc: 27.000% (27/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 27.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.860 | Acc: 64.000% (16/25)\n",
      "Loss: 0.928 | Acc: 66.000% (33/50)\n",
      "Loss: 0.958 | Acc: 66.667% (50/75)\n",
      "Loss: 0.972 | Acc: 65.000% (65/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 65.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.538 | Acc: 20.000% (5/25)\n",
      "Loss: 2.650 | Acc: 22.000% (11/50)\n",
      "Loss: 2.711 | Acc: 25.333% (19/75)\n",
      "Loss: 2.732 | Acc: 24.000% (24/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 24.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.711 | Acc: 76.000% (19/25)\n",
      "Loss: 0.720 | Acc: 74.000% (37/50)\n",
      "Loss: 0.787 | Acc: 72.000% (54/75)\n",
      "Loss: 0.786 | Acc: 74.000% (74/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 74.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.921 | Acc: 16.000% (4/25)\n",
      "Loss: 2.912 | Acc: 20.000% (10/50)\n",
      "Loss: 2.757 | Acc: 20.000% (15/75)\n",
      "Loss: 2.752 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.548 | Acc: 88.000% (22/25)\n",
      "Loss: 0.636 | Acc: 80.000% (40/50)\n",
      "Loss: 0.553 | Acc: 82.667% (62/75)\n",
      "Loss: 0.571 | Acc: 81.000% (81/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 81.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.948 | Acc: 28.000% (7/25)\n",
      "Loss: 3.316 | Acc: 20.000% (10/50)\n",
      "Loss: 3.419 | Acc: 20.000% (15/75)\n",
      "Loss: 3.332 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.445 | Acc: 80.000% (20/25)\n",
      "Loss: 0.578 | Acc: 78.000% (39/50)\n",
      "Loss: 0.499 | Acc: 81.333% (61/75)\n",
      "Loss: 0.484 | Acc: 83.000% (83/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 83.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.332 | Acc: 20.000% (5/25)\n",
      "Loss: 3.240 | Acc: 16.000% (8/50)\n",
      "Loss: 3.156 | Acc: 21.333% (16/75)\n",
      "Loss: 3.129 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Final train set accuracy is 83.0\n",
      "Final val set accuracy is 21.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.149 | Acc: 10.938% (7/64)\n",
      "Loss: 3.353 | Acc: 10.156% (13/128)\n",
      "Loss: 3.276 | Acc: 9.375% (18/192)\n",
      "Loss: 3.066 | Acc: 11.719% (30/256)\n",
      "Loss: 3.012 | Acc: 12.188% (39/320)\n",
      "Loss: 2.946 | Acc: 12.240% (47/384)\n",
      "Loss: 2.906 | Acc: 12.277% (55/448)\n",
      "Loss: 2.840 | Acc: 12.305% (63/512)\n",
      "Loss: 2.792 | Acc: 12.153% (70/576)\n",
      "Loss: 2.757 | Acc: 12.031% (77/640)\n",
      "Loss: 2.717 | Acc: 12.926% (91/704)\n",
      "Loss: 2.677 | Acc: 13.411% (103/768)\n",
      "Loss: 2.651 | Acc: 13.101% (109/832)\n",
      "Loss: 2.624 | Acc: 13.170% (118/896)\n",
      "Loss: 2.607 | Acc: 13.021% (125/960)\n",
      "Loss: 2.580 | Acc: 13.379% (137/1024)\n",
      "Loss: 2.560 | Acc: 13.603% (148/1088)\n",
      "Loss: 2.537 | Acc: 14.497% (167/1152)\n",
      "Loss: 2.520 | Acc: 14.803% (180/1216)\n",
      "Loss: 2.499 | Acc: 15.156% (194/1280)\n",
      "Loss: 2.487 | Acc: 15.253% (205/1344)\n",
      "Loss: 2.469 | Acc: 15.625% (220/1408)\n",
      "Loss: 2.459 | Acc: 15.761% (232/1472)\n",
      "Loss: 2.444 | Acc: 16.146% (248/1536)\n",
      "Loss: 2.432 | Acc: 16.312% (261/1600)\n",
      "Loss: 2.421 | Acc: 16.406% (273/1664)\n",
      "Loss: 2.413 | Acc: 16.377% (283/1728)\n",
      "Loss: 2.395 | Acc: 16.685% (299/1792)\n",
      "Loss: 2.385 | Acc: 16.595% (308/1856)\n",
      "Loss: 2.376 | Acc: 16.719% (321/1920)\n",
      "Loss: 2.367 | Acc: 16.835% (334/1984)\n",
      "Loss: 2.359 | Acc: 16.797% (344/2048)\n",
      "Loss: 2.349 | Acc: 16.951% (358/2112)\n",
      "Loss: 2.339 | Acc: 16.820% (366/2176)\n",
      "Loss: 2.331 | Acc: 16.830% (377/2240)\n",
      "Loss: 2.329 | Acc: 16.753% (386/2304)\n",
      "Loss: 2.322 | Acc: 16.934% (401/2368)\n",
      "Loss: 2.313 | Acc: 17.393% (423/2432)\n",
      "Loss: 2.305 | Acc: 17.708% (442/2496)\n",
      "Loss: 2.297 | Acc: 17.773% (455/2560)\n",
      "Loss: 2.290 | Acc: 17.988% (472/2624)\n",
      "Loss: 2.281 | Acc: 18.118% (487/2688)\n",
      "Loss: 2.272 | Acc: 18.459% (508/2752)\n",
      "Loss: 2.270 | Acc: 18.466% (520/2816)\n",
      "Loss: 2.266 | Acc: 18.785% (541/2880)\n",
      "Loss: 2.262 | Acc: 18.886% (556/2944)\n",
      "Loss: 2.259 | Acc: 18.850% (567/3008)\n",
      "Loss: 2.252 | Acc: 19.141% (588/3072)\n",
      "Loss: 2.249 | Acc: 19.196% (602/3136)\n",
      "Loss: 2.245 | Acc: 19.281% (617/3200)\n",
      "Loss: 2.240 | Acc: 19.730% (644/3264)\n",
      "Loss: 2.236 | Acc: 19.802% (659/3328)\n",
      "Loss: 2.235 | Acc: 19.752% (670/3392)\n",
      "Loss: 2.231 | Acc: 19.792% (684/3456)\n",
      "Loss: 2.229 | Acc: 19.943% (702/3520)\n",
      "Loss: 2.226 | Acc: 19.978% (716/3584)\n",
      "Loss: 2.220 | Acc: 19.984% (729/3648)\n",
      "Loss: 2.217 | Acc: 20.043% (744/3712)\n",
      "Loss: 2.212 | Acc: 20.154% (761/3776)\n",
      "Loss: 2.210 | Acc: 20.130% (773/3840)\n",
      "Loss: 2.204 | Acc: 20.261% (791/3904)\n",
      "Loss: 2.202 | Acc: 20.312% (806/3968)\n",
      "Loss: 2.200 | Acc: 20.486% (826/4032)\n",
      "Loss: 2.194 | Acc: 20.581% (843/4096)\n",
      "Loss: 2.191 | Acc: 20.601% (857/4160)\n",
      "Loss: 2.186 | Acc: 20.762% (877/4224)\n",
      "Loss: 2.185 | Acc: 20.872% (895/4288)\n",
      "Loss: 2.180 | Acc: 21.048% (916/4352)\n",
      "Loss: 2.176 | Acc: 21.264% (939/4416)\n",
      "Loss: 2.173 | Acc: 21.406% (959/4480)\n",
      "Loss: 2.169 | Acc: 21.479% (976/4544)\n",
      "Loss: 2.167 | Acc: 21.506% (991/4608)\n",
      "Loss: 2.164 | Acc: 21.597% (1009/4672)\n",
      "Loss: 2.160 | Acc: 21.769% (1031/4736)\n",
      "Loss: 2.157 | Acc: 21.771% (1045/4800)\n",
      "Loss: 2.154 | Acc: 21.875% (1064/4864)\n",
      "Loss: 2.149 | Acc: 21.895% (1079/4928)\n",
      "Loss: 2.148 | Acc: 21.955% (1096/4992)\n",
      "Loss: 2.146 | Acc: 21.934% (1109/5056)\n",
      "Loss: 2.142 | Acc: 22.031% (1128/5120)\n",
      "Loss: 2.139 | Acc: 22.242% (1153/5184)\n",
      "Loss: 2.138 | Acc: 22.218% (1166/5248)\n",
      "Loss: 2.136 | Acc: 22.270% (1183/5312)\n",
      "Loss: 2.134 | Acc: 22.284% (1198/5376)\n",
      "Loss: 2.131 | Acc: 22.353% (1216/5440)\n",
      "Loss: 2.126 | Acc: 22.493% (1238/5504)\n",
      "Loss: 2.124 | Acc: 22.647% (1261/5568)\n",
      "Loss: 2.120 | Acc: 22.798% (1284/5632)\n",
      "Loss: 2.119 | Acc: 22.770% (1297/5696)\n",
      "Loss: 2.117 | Acc: 22.917% (1320/5760)\n",
      "Loss: 2.119 | Acc: 22.819% (1329/5824)\n",
      "Loss: 2.116 | Acc: 22.826% (1344/5888)\n",
      "Loss: 2.114 | Acc: 22.849% (1360/5952)\n",
      "Loss: 2.111 | Acc: 22.856% (1375/6016)\n",
      "Loss: 2.109 | Acc: 22.829% (1388/6080)\n",
      "Loss: 2.107 | Acc: 22.819% (1402/6144)\n",
      "Loss: 2.104 | Acc: 22.922% (1423/6208)\n",
      "Loss: 2.103 | Acc: 22.911% (1437/6272)\n",
      "Loss: 2.102 | Acc: 22.838% (1447/6336)\n",
      "Loss: 2.100 | Acc: 22.906% (1466/6400)\n",
      "Loss: 2.099 | Acc: 22.927% (1482/6464)\n",
      "Loss: 2.098 | Acc: 22.917% (1496/6528)\n",
      "Loss: 2.096 | Acc: 22.922% (1511/6592)\n",
      "Loss: 2.094 | Acc: 22.942% (1527/6656)\n",
      "Loss: 2.092 | Acc: 23.006% (1546/6720)\n",
      "Loss: 2.093 | Acc: 23.025% (1562/6784)\n",
      "Loss: 2.091 | Acc: 23.029% (1577/6848)\n",
      "Loss: 2.089 | Acc: 23.105% (1597/6912)\n",
      "Loss: 2.087 | Acc: 23.093% (1611/6976)\n",
      "Loss: 2.085 | Acc: 23.139% (1629/7040)\n",
      "Loss: 2.082 | Acc: 23.297% (1655/7104)\n",
      "Loss: 2.080 | Acc: 23.410% (1678/7168)\n",
      "Loss: 2.079 | Acc: 23.438% (1695/7232)\n",
      "Loss: 2.076 | Acc: 23.479% (1713/7296)\n",
      "Loss: 2.074 | Acc: 23.465% (1727/7360)\n",
      "Loss: 2.072 | Acc: 23.532% (1747/7424)\n",
      "Loss: 2.071 | Acc: 23.531% (1762/7488)\n",
      "Loss: 2.069 | Acc: 23.543% (1778/7552)\n",
      "Loss: 2.067 | Acc: 23.648% (1801/7616)\n",
      "Loss: 2.066 | Acc: 23.685% (1819/7680)\n",
      "Loss: 2.063 | Acc: 23.709% (1836/7744)\n",
      "Loss: 2.062 | Acc: 23.796% (1858/7808)\n",
      "Loss: 2.062 | Acc: 23.806% (1874/7872)\n",
      "Loss: 2.060 | Acc: 23.841% (1892/7936)\n",
      "Loss: 2.060 | Acc: 23.825% (1906/8000)\n",
      "Loss: 2.061 | Acc: 23.785% (1918/8064)\n",
      "Loss: 2.061 | Acc: 23.794% (1934/8128)\n",
      "Loss: 2.060 | Acc: 23.804% (1950/8192)\n",
      "Loss: 2.059 | Acc: 23.813% (1966/8256)\n",
      "Loss: 2.059 | Acc: 23.822% (1982/8320)\n",
      "Loss: 2.060 | Acc: 23.760% (1992/8384)\n",
      "Loss: 2.061 | Acc: 23.804% (2011/8448)\n",
      "Loss: 2.060 | Acc: 23.837% (2029/8512)\n",
      "Loss: 2.060 | Acc: 23.787% (2040/8576)\n",
      "Loss: 2.059 | Acc: 23.750% (2052/8640)\n",
      "Loss: 2.057 | Acc: 23.782% (2070/8704)\n",
      "Loss: 2.056 | Acc: 23.734% (2081/8768)\n",
      "Loss: 2.055 | Acc: 23.777% (2100/8832)\n",
      "Loss: 2.055 | Acc: 23.808% (2118/8896)\n",
      "Loss: 2.052 | Acc: 23.873% (2139/8960)\n",
      "Loss: 2.052 | Acc: 23.848% (2152/9024)\n",
      "Loss: 2.051 | Acc: 23.878% (2170/9088)\n",
      "Loss: 2.050 | Acc: 23.907% (2188/9152)\n",
      "Loss: 2.049 | Acc: 23.947% (2207/9216)\n",
      "Loss: 2.048 | Acc: 23.998% (2227/9280)\n",
      "Loss: 2.048 | Acc: 24.005% (2243/9344)\n",
      "Loss: 2.045 | Acc: 24.128% (2270/9408)\n",
      "Loss: 2.044 | Acc: 24.124% (2285/9472)\n",
      "Loss: 2.043 | Acc: 24.140% (2302/9536)\n",
      "Loss: 2.039 | Acc: 24.260% (2329/9600)\n",
      "Loss: 2.038 | Acc: 24.338% (2352/9664)\n",
      "Loss: 2.036 | Acc: 24.383% (2372/9728)\n",
      "Loss: 2.034 | Acc: 24.459% (2395/9792)\n",
      "Loss: 2.033 | Acc: 24.513% (2416/9856)\n",
      "Loss: 2.031 | Acc: 24.607% (2441/9920)\n",
      "Loss: 2.031 | Acc: 24.619% (2458/9984)\n",
      "Loss: 2.029 | Acc: 24.682% (2480/10048)\n",
      "Loss: 2.027 | Acc: 24.733% (2501/10112)\n",
      "Loss: 2.027 | Acc: 24.754% (2519/10176)\n",
      "Loss: 2.026 | Acc: 24.834% (2543/10240)\n",
      "Loss: 2.023 | Acc: 24.942% (2570/10304)\n",
      "Loss: 2.021 | Acc: 24.971% (2589/10368)\n",
      "Loss: 2.019 | Acc: 25.029% (2611/10432)\n",
      "Loss: 2.016 | Acc: 25.181% (2643/10496)\n",
      "Loss: 2.015 | Acc: 25.256% (2667/10560)\n",
      "Loss: 2.014 | Acc: 25.301% (2688/10624)\n",
      "Loss: 2.012 | Acc: 25.402% (2715/10688)\n",
      "Loss: 2.011 | Acc: 25.400% (2731/10752)\n",
      "Loss: 2.011 | Acc: 25.435% (2751/10816)\n",
      "Loss: 2.010 | Acc: 25.441% (2768/10880)\n",
      "Loss: 2.009 | Acc: 25.530% (2794/10944)\n",
      "Loss: 2.010 | Acc: 25.590% (2817/11008)\n",
      "Loss: 2.009 | Acc: 25.623% (2837/11072)\n",
      "Loss: 2.008 | Acc: 25.665% (2858/11136)\n",
      "Loss: 2.007 | Acc: 25.723% (2881/11200)\n",
      "Loss: 2.005 | Acc: 25.808% (2907/11264)\n",
      "Loss: 2.004 | Acc: 25.812% (2924/11328)\n",
      "Loss: 2.003 | Acc: 25.834% (2943/11392)\n",
      "Loss: 2.002 | Acc: 25.829% (2959/11456)\n",
      "Loss: 2.001 | Acc: 25.885% (2982/11520)\n",
      "Loss: 1.999 | Acc: 25.958% (3007/11584)\n",
      "Loss: 1.997 | Acc: 25.996% (3028/11648)\n",
      "Loss: 1.997 | Acc: 26.025% (3048/11712)\n",
      "Loss: 1.996 | Acc: 26.070% (3070/11776)\n",
      "Loss: 1.994 | Acc: 26.106% (3091/11840)\n",
      "Loss: 1.994 | Acc: 26.126% (3110/11904)\n",
      "Loss: 1.993 | Acc: 26.170% (3132/11968)\n",
      "Loss: 1.992 | Acc: 26.180% (3150/12032)\n",
      "Loss: 1.991 | Acc: 26.240% (3174/12096)\n",
      "Loss: 1.991 | Acc: 26.225% (3189/12160)\n",
      "Loss: 1.988 | Acc: 26.276% (3212/12224)\n",
      "Loss: 1.988 | Acc: 26.351% (3238/12288)\n",
      "Loss: 1.987 | Acc: 26.441% (3266/12352)\n",
      "Loss: 1.985 | Acc: 26.482% (3288/12416)\n",
      "Loss: 1.984 | Acc: 26.579% (3317/12480)\n",
      "Loss: 1.982 | Acc: 26.634% (3341/12544)\n",
      "Loss: 1.982 | Acc: 26.658% (3361/12608)\n",
      "Loss: 1.980 | Acc: 26.697% (3383/12672)\n",
      "Loss: 1.979 | Acc: 26.727% (3404/12736)\n",
      "Loss: 1.978 | Acc: 26.727% (3421/12800)\n",
      "Loss: 1.978 | Acc: 26.734% (3439/12864)\n",
      "Loss: 1.977 | Acc: 26.779% (3462/12928)\n",
      "Loss: 1.976 | Acc: 26.832% (3486/12992)\n",
      "Loss: 1.975 | Acc: 26.877% (3509/13056)\n",
      "Loss: 1.974 | Acc: 26.921% (3532/13120)\n",
      "Loss: 1.973 | Acc: 26.942% (3552/13184)\n",
      "Loss: 1.972 | Acc: 26.970% (3573/13248)\n",
      "Loss: 1.972 | Acc: 27.013% (3596/13312)\n",
      "Loss: 1.970 | Acc: 27.063% (3620/13376)\n",
      "Loss: 1.969 | Acc: 27.083% (3640/13440)\n",
      "Loss: 1.968 | Acc: 27.133% (3664/13504)\n",
      "Loss: 1.967 | Acc: 27.137% (3682/13568)\n",
      "Loss: 1.966 | Acc: 27.179% (3705/13632)\n",
      "Loss: 1.964 | Acc: 27.249% (3732/13696)\n",
      "Loss: 1.964 | Acc: 27.260% (3751/13760)\n",
      "Loss: 1.962 | Acc: 27.279% (3771/13824)\n",
      "Loss: 1.961 | Acc: 27.355% (3799/13888)\n",
      "Loss: 1.961 | Acc: 27.365% (3818/13952)\n",
      "Loss: 1.959 | Acc: 27.447% (3847/14016)\n",
      "Loss: 1.958 | Acc: 27.464% (3867/14080)\n",
      "Loss: 1.958 | Acc: 27.460% (3884/14144)\n",
      "Loss: 1.958 | Acc: 27.421% (3896/14208)\n",
      "Loss: 1.958 | Acc: 27.403% (3911/14272)\n",
      "Loss: 1.957 | Acc: 27.400% (3928/14336)\n",
      "Loss: 1.956 | Acc: 27.438% (3951/14400)\n",
      "Loss: 1.956 | Acc: 27.441% (3969/14464)\n",
      "Loss: 1.955 | Acc: 27.450% (3988/14528)\n",
      "Loss: 1.954 | Acc: 27.447% (4005/14592)\n",
      "Loss: 1.954 | Acc: 27.463% (4025/14656)\n",
      "Loss: 1.953 | Acc: 27.500% (4048/14720)\n",
      "Loss: 1.952 | Acc: 27.550% (4073/14784)\n",
      "Loss: 1.952 | Acc: 27.539% (4089/14848)\n",
      "Loss: 1.950 | Acc: 27.568% (4111/14912)\n",
      "Loss: 1.949 | Acc: 27.631% (4138/14976)\n",
      "Loss: 1.948 | Acc: 27.653% (4159/15040)\n",
      "Loss: 1.947 | Acc: 27.695% (4183/15104)\n",
      "Loss: 1.945 | Acc: 27.729% (4206/15168)\n",
      "Loss: 1.945 | Acc: 27.744% (4226/15232)\n",
      "Loss: 1.944 | Acc: 27.746% (4244/15296)\n",
      "Loss: 1.943 | Acc: 27.747% (4262/15360)\n",
      "Loss: 1.943 | Acc: 27.736% (4278/15424)\n",
      "Loss: 1.942 | Acc: 27.789% (4304/15488)\n",
      "Loss: 1.941 | Acc: 27.855% (4332/15552)\n",
      "Loss: 1.941 | Acc: 27.869% (4352/15616)\n",
      "Loss: 1.939 | Acc: 27.927% (4379/15680)\n",
      "Loss: 1.939 | Acc: 27.915% (4395/15744)\n",
      "Loss: 1.938 | Acc: 27.916% (4413/15808)\n",
      "Loss: 1.938 | Acc: 27.904% (4429/15872)\n",
      "Loss: 1.937 | Acc: 27.949% (4454/15936)\n",
      "Loss: 1.936 | Acc: 28.006% (4481/16000)\n",
      "Loss: 1.935 | Acc: 28.013% (4500/16064)\n",
      "Loss: 1.935 | Acc: 28.051% (4524/16128)\n",
      "Loss: 1.934 | Acc: 28.088% (4548/16192)\n",
      "Loss: 1.932 | Acc: 28.131% (4573/16256)\n",
      "Loss: 1.932 | Acc: 28.162% (4596/16320)\n",
      "Loss: 1.931 | Acc: 28.217% (4623/16384)\n",
      "Loss: 1.930 | Acc: 28.222% (4642/16448)\n",
      "Loss: 1.930 | Acc: 28.228% (4661/16512)\n",
      "Loss: 1.929 | Acc: 28.252% (4683/16576)\n",
      "Loss: 1.928 | Acc: 28.275% (4705/16640)\n",
      "Loss: 1.927 | Acc: 28.323% (4731/16704)\n",
      "Loss: 1.926 | Acc: 28.340% (4752/16768)\n",
      "Loss: 1.925 | Acc: 28.392% (4779/16832)\n",
      "Loss: 1.924 | Acc: 28.421% (4802/16896)\n",
      "Loss: 1.925 | Acc: 28.426% (4821/16960)\n",
      "Loss: 1.924 | Acc: 28.483% (4849/17024)\n",
      "Loss: 1.923 | Acc: 28.488% (4868/17088)\n",
      "Loss: 1.922 | Acc: 28.533% (4894/17152)\n",
      "Loss: 1.921 | Acc: 28.537% (4913/17216)\n",
      "Loss: 1.921 | Acc: 28.571% (4937/17280)\n",
      "Loss: 1.920 | Acc: 28.609% (4962/17344)\n",
      "Loss: 1.919 | Acc: 28.688% (4994/17408)\n",
      "Loss: 1.918 | Acc: 28.715% (5017/17472)\n",
      "Loss: 1.917 | Acc: 28.764% (5044/17536)\n",
      "Loss: 1.917 | Acc: 28.750% (5060/17600)\n",
      "Loss: 1.917 | Acc: 28.725% (5074/17664)\n",
      "Loss: 1.916 | Acc: 28.762% (5099/17728)\n",
      "Loss: 1.915 | Acc: 28.805% (5125/17792)\n",
      "Loss: 1.915 | Acc: 28.819% (5146/17856)\n",
      "Loss: 1.913 | Acc: 28.884% (5176/17920)\n",
      "Loss: 1.913 | Acc: 28.887% (5195/17984)\n",
      "Loss: 1.913 | Acc: 28.884% (5213/18048)\n",
      "Loss: 1.913 | Acc: 28.898% (5234/18112)\n",
      "Loss: 1.913 | Acc: 28.884% (5250/18176)\n",
      "Loss: 1.912 | Acc: 28.893% (5270/18240)\n",
      "Loss: 1.913 | Acc: 28.884% (5287/18304)\n",
      "Loss: 1.912 | Acc: 28.931% (5314/18368)\n",
      "Loss: 1.912 | Acc: 28.917% (5330/18432)\n",
      "Loss: 1.911 | Acc: 28.952% (5355/18496)\n",
      "Loss: 1.910 | Acc: 28.955% (5374/18560)\n",
      "Loss: 1.910 | Acc: 28.963% (5394/18624)\n",
      "Loss: 1.909 | Acc: 28.997% (5419/18688)\n",
      "Loss: 1.908 | Acc: 29.021% (5442/18752)\n",
      "Loss: 1.907 | Acc: 29.055% (5467/18816)\n",
      "Loss: 1.906 | Acc: 29.073% (5489/18880)\n",
      "Loss: 1.906 | Acc: 29.091% (5511/18944)\n",
      "Loss: 1.906 | Acc: 29.109% (5533/19008)\n",
      "Loss: 1.906 | Acc: 29.121% (5554/19072)\n",
      "Loss: 1.905 | Acc: 29.160% (5580/19136)\n",
      "Loss: 1.905 | Acc: 29.177% (5602/19200)\n",
      "Loss: 1.904 | Acc: 29.210% (5627/19264)\n",
      "Loss: 1.904 | Acc: 29.222% (5648/19328)\n",
      "Loss: 1.904 | Acc: 29.203% (5663/19392)\n",
      "Loss: 1.903 | Acc: 29.230% (5687/19456)\n",
      "Loss: 1.903 | Acc: 29.262% (5712/19520)\n",
      "Loss: 1.901 | Acc: 29.284% (5735/19584)\n",
      "Loss: 1.901 | Acc: 29.316% (5760/19648)\n",
      "Loss: 1.901 | Acc: 29.337% (5783/19712)\n",
      "Loss: 1.901 | Acc: 29.369% (5808/19776)\n",
      "Loss: 1.900 | Acc: 29.415% (5836/19840)\n",
      "Loss: 1.899 | Acc: 29.426% (5857/19904)\n",
      "Loss: 1.898 | Acc: 29.487% (5888/19968)\n",
      "Loss: 1.897 | Acc: 29.513% (5912/20032)\n",
      "Loss: 1.897 | Acc: 29.543% (5937/20096)\n",
      "Loss: 1.896 | Acc: 29.568% (5961/20160)\n",
      "Loss: 1.895 | Acc: 29.589% (5984/20224)\n",
      "Loss: 1.894 | Acc: 29.618% (6009/20288)\n",
      "Loss: 1.893 | Acc: 29.624% (6029/20352)\n",
      "Loss: 1.893 | Acc: 29.648% (6053/20416)\n",
      "Loss: 1.893 | Acc: 29.668% (6076/20480)\n",
      "Loss: 1.892 | Acc: 29.692% (6100/20544)\n",
      "Loss: 1.891 | Acc: 29.697% (6120/20608)\n",
      "Loss: 1.890 | Acc: 29.760% (6152/20672)\n",
      "Loss: 1.889 | Acc: 29.774% (6174/20736)\n",
      "Loss: 1.889 | Acc: 29.779% (6194/20800)\n",
      "Loss: 1.888 | Acc: 29.783% (6214/20864)\n",
      "Loss: 1.888 | Acc: 29.817% (6240/20928)\n",
      "Loss: 1.888 | Acc: 29.807% (6257/20992)\n",
      "Loss: 1.887 | Acc: 29.806% (6276/21056)\n",
      "Loss: 1.887 | Acc: 29.801% (6294/21120)\n",
      "Loss: 1.886 | Acc: 29.820% (6317/21184)\n",
      "Loss: 1.886 | Acc: 29.848% (6342/21248)\n",
      "Loss: 1.886 | Acc: 29.861% (6364/21312)\n",
      "Loss: 1.885 | Acc: 29.898% (6391/21376)\n",
      "Loss: 1.884 | Acc: 29.925% (6416/21440)\n",
      "Loss: 1.884 | Acc: 29.953% (6441/21504)\n",
      "Loss: 1.883 | Acc: 29.952% (6460/21568)\n",
      "Loss: 1.883 | Acc: 29.969% (6483/21632)\n",
      "Loss: 1.882 | Acc: 30.006% (6510/21696)\n",
      "Loss: 1.881 | Acc: 30.041% (6537/21760)\n",
      "Loss: 1.880 | Acc: 30.082% (6565/21824)\n",
      "Loss: 1.880 | Acc: 30.140% (6597/21888)\n",
      "Loss: 1.879 | Acc: 30.161% (6621/21952)\n",
      "Loss: 1.879 | Acc: 30.160% (6640/22016)\n",
      "Loss: 1.878 | Acc: 30.159% (6659/22080)\n",
      "Loss: 1.878 | Acc: 30.162% (6679/22144)\n",
      "Loss: 1.877 | Acc: 30.156% (6697/22208)\n",
      "Loss: 1.877 | Acc: 30.172% (6720/22272)\n",
      "Loss: 1.876 | Acc: 30.189% (6743/22336)\n",
      "Loss: 1.875 | Acc: 30.201% (6765/22400)\n",
      "Loss: 1.875 | Acc: 30.226% (6790/22464)\n",
      "Loss: 1.874 | Acc: 30.282% (6822/22528)\n",
      "Loss: 1.873 | Acc: 30.312% (6848/22592)\n",
      "Loss: 1.872 | Acc: 30.350% (6876/22656)\n",
      "Loss: 1.871 | Acc: 30.352% (6896/22720)\n",
      "Loss: 1.871 | Acc: 30.368% (6919/22784)\n",
      "Loss: 1.870 | Acc: 30.401% (6946/22848)\n",
      "Loss: 1.869 | Acc: 30.443% (6975/22912)\n",
      "Loss: 1.869 | Acc: 30.458% (6998/22976)\n",
      "Loss: 1.867 | Acc: 30.503% (7028/23040)\n",
      "Loss: 1.867 | Acc: 30.506% (7048/23104)\n",
      "Loss: 1.866 | Acc: 30.516% (7070/23168)\n",
      "Loss: 1.866 | Acc: 30.544% (7096/23232)\n",
      "Loss: 1.865 | Acc: 30.550% (7117/23296)\n",
      "Loss: 1.864 | Acc: 30.578% (7143/23360)\n",
      "Loss: 1.863 | Acc: 30.622% (7173/23424)\n",
      "Loss: 1.863 | Acc: 30.662% (7202/23488)\n",
      "Loss: 1.862 | Acc: 30.685% (7227/23552)\n",
      "Loss: 1.862 | Acc: 30.687% (7247/23616)\n",
      "Loss: 1.861 | Acc: 30.735% (7278/23680)\n",
      "Loss: 1.861 | Acc: 30.749% (7301/23744)\n",
      "Loss: 1.861 | Acc: 30.746% (7320/23808)\n",
      "Loss: 1.861 | Acc: 30.752% (7341/23872)\n",
      "Loss: 1.861 | Acc: 30.778% (7367/23936)\n",
      "Loss: 1.861 | Acc: 30.792% (7390/24000)\n",
      "Loss: 1.861 | Acc: 30.793% (7410/24064)\n",
      "Loss: 1.860 | Acc: 30.856% (7445/24128)\n",
      "Loss: 1.860 | Acc: 30.874% (7469/24192)\n",
      "Loss: 1.860 | Acc: 30.871% (7488/24256)\n",
      "Loss: 1.859 | Acc: 30.880% (7510/24320)\n",
      "Loss: 1.859 | Acc: 30.881% (7530/24384)\n",
      "Loss: 1.859 | Acc: 30.894% (7553/24448)\n",
      "Loss: 1.859 | Acc: 30.899% (7574/24512)\n",
      "Loss: 1.859 | Acc: 30.912% (7597/24576)\n",
      "Loss: 1.858 | Acc: 30.925% (7620/24640)\n",
      "Loss: 1.857 | Acc: 30.946% (7645/24704)\n",
      "Loss: 1.857 | Acc: 30.955% (7667/24768)\n",
      "Loss: 1.857 | Acc: 30.988% (7695/24832)\n",
      "Loss: 1.856 | Acc: 30.997% (7717/24896)\n",
      "Loss: 1.856 | Acc: 31.022% (7743/24960)\n",
      "Loss: 1.855 | Acc: 31.014% (7761/25024)\n",
      "Loss: 1.855 | Acc: 31.059% (7792/25088)\n",
      "Loss: 1.854 | Acc: 31.059% (7812/25152)\n",
      "Loss: 1.854 | Acc: 31.087% (7839/25216)\n",
      "Loss: 1.854 | Acc: 31.100% (7862/25280)\n",
      "Loss: 1.854 | Acc: 31.124% (7888/25344)\n",
      "Loss: 1.853 | Acc: 31.144% (7913/25408)\n",
      "Loss: 1.853 | Acc: 31.148% (7934/25472)\n",
      "Loss: 1.853 | Acc: 31.140% (7952/25536)\n",
      "Loss: 1.853 | Acc: 31.145% (7973/25600)\n",
      "Loss: 1.852 | Acc: 31.153% (7995/25664)\n",
      "Loss: 1.852 | Acc: 31.157% (8016/25728)\n",
      "Loss: 1.852 | Acc: 31.165% (8038/25792)\n",
      "Loss: 1.852 | Acc: 31.192% (8065/25856)\n",
      "Loss: 1.852 | Acc: 31.181% (8082/25920)\n",
      "Loss: 1.851 | Acc: 31.219% (8112/25984)\n",
      "Loss: 1.851 | Acc: 31.246% (8139/26048)\n",
      "Loss: 1.851 | Acc: 31.254% (8161/26112)\n",
      "Loss: 1.850 | Acc: 31.277% (8187/26176)\n",
      "Loss: 1.849 | Acc: 31.319% (8218/26240)\n",
      "Loss: 1.849 | Acc: 31.330% (8241/26304)\n",
      "Loss: 1.849 | Acc: 31.349% (8266/26368)\n",
      "Loss: 1.848 | Acc: 31.371% (8292/26432)\n",
      "Loss: 1.847 | Acc: 31.390% (8317/26496)\n",
      "Loss: 1.848 | Acc: 31.404% (8341/26560)\n",
      "Loss: 1.847 | Acc: 31.427% (8367/26624)\n",
      "Loss: 1.847 | Acc: 31.449% (8393/26688)\n",
      "Loss: 1.847 | Acc: 31.448% (8413/26752)\n",
      "Loss: 1.846 | Acc: 31.455% (8435/26816)\n",
      "Loss: 1.845 | Acc: 31.473% (8460/26880)\n",
      "Loss: 1.845 | Acc: 31.502% (8488/26944)\n",
      "Loss: 1.845 | Acc: 31.502% (8508/27008)\n",
      "Loss: 1.844 | Acc: 31.542% (8539/27072)\n",
      "Loss: 1.843 | Acc: 31.582% (8570/27136)\n",
      "Loss: 1.843 | Acc: 31.592% (8593/27200)\n",
      "Loss: 1.843 | Acc: 31.613% (8619/27264)\n",
      "Loss: 1.842 | Acc: 31.645% (8648/27328)\n",
      "Loss: 1.841 | Acc: 31.688% (8680/27392)\n",
      "Loss: 1.842 | Acc: 31.698% (8703/27456)\n",
      "Loss: 1.841 | Acc: 31.719% (8729/27520)\n",
      "Loss: 1.841 | Acc: 31.718% (8749/27584)\n",
      "Loss: 1.841 | Acc: 31.713% (8768/27648)\n",
      "Loss: 1.841 | Acc: 31.730% (8793/27712)\n",
      "Loss: 1.840 | Acc: 31.750% (8819/27776)\n",
      "Loss: 1.839 | Acc: 31.771% (8845/27840)\n",
      "Loss: 1.839 | Acc: 31.816% (8878/27904)\n",
      "Loss: 1.838 | Acc: 31.811% (8897/27968)\n",
      "Loss: 1.838 | Acc: 31.807% (8916/28032)\n",
      "Loss: 1.838 | Acc: 31.812% (8938/28096)\n",
      "Loss: 1.837 | Acc: 31.839% (8966/28160)\n",
      "Loss: 1.837 | Acc: 31.863% (8993/28224)\n",
      "Loss: 1.836 | Acc: 31.879% (9018/28288)\n",
      "Loss: 1.836 | Acc: 31.874% (9037/28352)\n",
      "Loss: 1.836 | Acc: 31.880% (9059/28416)\n",
      "Loss: 1.836 | Acc: 31.893% (9083/28480)\n",
      "Loss: 1.835 | Acc: 31.912% (9109/28544)\n",
      "Loss: 1.834 | Acc: 31.932% (9135/28608)\n",
      "Loss: 1.834 | Acc: 31.948% (9160/28672)\n",
      "Loss: 1.834 | Acc: 31.956% (9183/28736)\n",
      "Loss: 1.833 | Acc: 31.969% (9207/28800)\n",
      "Loss: 1.833 | Acc: 31.984% (9232/28864)\n",
      "Loss: 1.833 | Acc: 31.986% (9253/28928)\n",
      "Loss: 1.832 | Acc: 32.033% (9287/28992)\n",
      "Loss: 1.831 | Acc: 32.069% (9318/29056)\n",
      "Loss: 1.830 | Acc: 32.091% (9345/29120)\n",
      "Loss: 1.830 | Acc: 32.120% (9374/29184)\n",
      "Loss: 1.829 | Acc: 32.139% (9400/29248)\n",
      "Loss: 1.829 | Acc: 32.144% (9422/29312)\n",
      "Loss: 1.829 | Acc: 32.156% (9446/29376)\n",
      "Loss: 1.828 | Acc: 32.167% (9470/29440)\n",
      "Loss: 1.827 | Acc: 32.192% (9498/29504)\n",
      "Loss: 1.827 | Acc: 32.187% (9517/29568)\n",
      "Loss: 1.826 | Acc: 32.222% (9548/29632)\n",
      "Loss: 1.826 | Acc: 32.237% (9573/29696)\n",
      "Loss: 1.826 | Acc: 32.251% (9598/29760)\n",
      "Loss: 1.825 | Acc: 32.256% (9620/29824)\n",
      "Loss: 1.825 | Acc: 32.287% (9650/29888)\n",
      "Loss: 1.824 | Acc: 32.305% (9676/29952)\n",
      "Loss: 1.824 | Acc: 32.319% (9701/30016)\n",
      "Loss: 1.823 | Acc: 32.337% (9727/30080)\n",
      "Loss: 1.823 | Acc: 32.331% (9746/30144)\n",
      "Loss: 1.822 | Acc: 32.356% (9774/30208)\n",
      "Loss: 1.821 | Acc: 32.393% (9806/30272)\n",
      "Loss: 1.821 | Acc: 32.400% (9829/30336)\n",
      "Loss: 1.821 | Acc: 32.424% (9857/30400)\n",
      "Loss: 1.821 | Acc: 32.438% (9882/30464)\n",
      "Loss: 1.821 | Acc: 32.433% (9901/30528)\n",
      "Loss: 1.820 | Acc: 32.456% (9929/30592)\n",
      "Loss: 1.819 | Acc: 32.477% (9956/30656)\n",
      "Loss: 1.819 | Acc: 32.513% (9988/30720)\n",
      "Loss: 1.818 | Acc: 32.546% (10019/30784)\n",
      "Loss: 1.818 | Acc: 32.573% (10048/30848)\n",
      "Loss: 1.817 | Acc: 32.596% (10076/30912)\n",
      "Loss: 1.817 | Acc: 32.603% (10099/30976)\n",
      "Loss: 1.816 | Acc: 32.632% (10129/31040)\n",
      "Loss: 1.815 | Acc: 32.661% (10159/31104)\n",
      "Loss: 1.815 | Acc: 32.655% (10178/31168)\n",
      "Loss: 1.815 | Acc: 32.672% (10204/31232)\n",
      "Loss: 1.814 | Acc: 32.707% (10236/31296)\n",
      "Loss: 1.814 | Acc: 32.717% (10260/31360)\n",
      "Loss: 1.813 | Acc: 32.727% (10284/31424)\n",
      "Loss: 1.813 | Acc: 32.730% (10306/31488)\n",
      "Loss: 1.812 | Acc: 32.759% (10336/31552)\n",
      "Loss: 1.812 | Acc: 32.768% (10360/31616)\n",
      "Loss: 1.812 | Acc: 32.800% (10391/31680)\n",
      "Loss: 1.812 | Acc: 32.812% (10416/31744)\n",
      "Loss: 1.811 | Acc: 32.825% (10441/31808)\n",
      "Loss: 1.810 | Acc: 32.847% (10469/31872)\n",
      "Loss: 1.810 | Acc: 32.859% (10494/31936)\n",
      "Loss: 1.810 | Acc: 32.862% (10516/32000)\n",
      "Loss: 1.810 | Acc: 32.856% (10535/32064)\n",
      "Loss: 1.810 | Acc: 32.865% (10559/32128)\n",
      "Loss: 1.810 | Acc: 32.875% (10583/32192)\n",
      "Loss: 1.809 | Acc: 32.871% (10603/32256)\n",
      "Loss: 1.810 | Acc: 32.865% (10622/32320)\n",
      "Loss: 1.809 | Acc: 32.893% (10652/32384)\n",
      "Loss: 1.809 | Acc: 32.893% (10673/32448)\n",
      "Loss: 1.809 | Acc: 32.896% (10695/32512)\n",
      "Loss: 1.809 | Acc: 32.889% (10714/32576)\n",
      "Loss: 1.809 | Acc: 32.901% (10739/32640)\n",
      "Loss: 1.809 | Acc: 32.898% (10759/32704)\n",
      "Loss: 1.808 | Acc: 32.901% (10781/32768)\n",
      "Loss: 1.808 | Acc: 32.916% (10807/32832)\n",
      "Loss: 1.808 | Acc: 32.928% (10832/32896)\n",
      "Loss: 1.808 | Acc: 32.952% (10861/32960)\n",
      "Loss: 1.807 | Acc: 32.970% (10888/33024)\n",
      "Loss: 1.807 | Acc: 32.988% (10915/33088)\n",
      "Loss: 1.807 | Acc: 32.990% (10937/33152)\n",
      "Loss: 1.807 | Acc: 33.008% (10964/33216)\n",
      "Loss: 1.806 | Acc: 33.041% (10996/33280)\n",
      "Loss: 1.806 | Acc: 33.070% (11027/33344)\n",
      "Loss: 1.806 | Acc: 33.058% (11044/33408)\n",
      "Loss: 1.806 | Acc: 33.066% (11068/33472)\n",
      "Loss: 1.806 | Acc: 33.066% (11089/33536)\n",
      "Loss: 1.805 | Acc: 33.086% (11117/33600)\n",
      "Loss: 1.805 | Acc: 33.092% (11140/33664)\n",
      "Loss: 1.805 | Acc: 33.106% (11166/33728)\n",
      "Loss: 1.804 | Acc: 33.120% (11192/33792)\n",
      "Loss: 1.804 | Acc: 33.114% (11211/33856)\n",
      "Loss: 1.804 | Acc: 33.146% (11243/33920)\n",
      "Loss: 1.803 | Acc: 33.148% (11265/33984)\n",
      "Loss: 1.803 | Acc: 33.168% (11293/34048)\n",
      "Loss: 1.803 | Acc: 33.176% (11317/34112)\n",
      "Loss: 1.803 | Acc: 33.181% (11340/34176)\n",
      "Loss: 1.803 | Acc: 33.195% (11366/34240)\n",
      "Loss: 1.802 | Acc: 33.209% (11392/34304)\n",
      "Loss: 1.802 | Acc: 33.231% (11421/34368)\n",
      "Loss: 1.802 | Acc: 33.245% (11447/34432)\n",
      "Loss: 1.801 | Acc: 33.253% (11471/34496)\n",
      "Loss: 1.801 | Acc: 33.273% (11499/34560)\n",
      "Loss: 1.800 | Acc: 33.295% (11528/34624)\n",
      "Loss: 1.800 | Acc: 33.314% (11556/34688)\n",
      "Loss: 1.799 | Acc: 33.325% (11581/34752)\n",
      "Loss: 1.799 | Acc: 33.335% (11606/34816)\n",
      "Loss: 1.799 | Acc: 33.340% (11629/34880)\n",
      "Loss: 1.799 | Acc: 33.353% (11655/34944)\n",
      "Loss: 1.799 | Acc: 33.355% (11677/35008)\n",
      "Loss: 1.799 | Acc: 33.357% (11699/35072)\n",
      "Loss: 1.798 | Acc: 33.379% (11728/35136)\n",
      "Loss: 1.798 | Acc: 33.381% (11750/35200)\n",
      "Loss: 1.798 | Acc: 33.400% (11778/35264)\n",
      "Loss: 1.797 | Acc: 33.404% (11801/35328)\n",
      "Loss: 1.797 | Acc: 33.414% (11826/35392)\n",
      "Loss: 1.796 | Acc: 33.419% (11849/35456)\n",
      "Loss: 1.796 | Acc: 33.438% (11877/35520)\n",
      "Loss: 1.795 | Acc: 33.450% (11903/35584)\n",
      "Loss: 1.795 | Acc: 33.466% (11930/35648)\n",
      "Loss: 1.795 | Acc: 33.490% (11960/35712)\n",
      "Loss: 1.795 | Acc: 33.483% (11979/35776)\n",
      "Loss: 1.794 | Acc: 33.504% (12008/35840)\n",
      "Loss: 1.794 | Acc: 33.509% (12031/35904)\n",
      "Loss: 1.793 | Acc: 33.527% (12059/35968)\n",
      "Loss: 1.793 | Acc: 33.540% (12085/36032)\n",
      "Loss: 1.793 | Acc: 33.569% (12117/36096)\n",
      "Loss: 1.792 | Acc: 33.590% (12146/36160)\n",
      "Loss: 1.792 | Acc: 33.613% (12176/36224)\n",
      "Loss: 1.792 | Acc: 33.634% (12205/36288)\n",
      "Loss: 1.792 | Acc: 33.635% (12227/36352)\n",
      "Loss: 1.791 | Acc: 33.636% (12249/36416)\n",
      "Loss: 1.791 | Acc: 33.640% (12272/36480)\n",
      "Loss: 1.791 | Acc: 33.655% (12299/36544)\n",
      "Loss: 1.791 | Acc: 33.662% (12323/36608)\n",
      "Loss: 1.790 | Acc: 33.682% (12352/36672)\n",
      "Loss: 1.790 | Acc: 33.681% (12373/36736)\n",
      "Loss: 1.790 | Acc: 33.701% (12402/36800)\n",
      "Loss: 1.789 | Acc: 33.727% (12433/36864)\n",
      "Loss: 1.789 | Acc: 33.733% (12457/36928)\n",
      "Loss: 1.789 | Acc: 33.742% (12482/36992)\n",
      "Loss: 1.789 | Acc: 33.754% (12508/37056)\n",
      "Loss: 1.788 | Acc: 33.747% (12527/37120)\n",
      "Loss: 1.788 | Acc: 33.751% (12550/37184)\n",
      "Loss: 1.788 | Acc: 33.758% (12574/37248)\n",
      "Loss: 1.788 | Acc: 33.767% (12599/37312)\n",
      "Loss: 1.788 | Acc: 33.781% (12626/37376)\n",
      "Loss: 1.787 | Acc: 33.782% (12648/37440)\n",
      "Loss: 1.787 | Acc: 33.778% (12668/37504)\n",
      "Loss: 1.787 | Acc: 33.784% (12692/37568)\n",
      "Loss: 1.787 | Acc: 33.790% (12716/37632)\n",
      "Loss: 1.787 | Acc: 33.789% (12737/37696)\n",
      "Loss: 1.787 | Acc: 33.795% (12761/37760)\n",
      "Loss: 1.786 | Acc: 33.801% (12785/37824)\n",
      "Loss: 1.786 | Acc: 33.821% (12814/37888)\n",
      "Loss: 1.786 | Acc: 33.832% (12840/37952)\n",
      "Loss: 1.786 | Acc: 33.849% (12868/38016)\n",
      "Loss: 1.786 | Acc: 33.863% (12895/38080)\n",
      "Loss: 1.785 | Acc: 33.874% (12921/38144)\n",
      "Loss: 1.785 | Acc: 33.883% (12946/38208)\n",
      "Loss: 1.785 | Acc: 33.913% (12979/38272)\n",
      "Loss: 1.784 | Acc: 33.919% (13003/38336)\n",
      "Loss: 1.784 | Acc: 33.935% (13031/38400)\n",
      "Loss: 1.783 | Acc: 33.967% (13065/38464)\n",
      "Loss: 1.783 | Acc: 33.970% (13088/38528)\n",
      "Loss: 1.783 | Acc: 33.984% (13115/38592)\n",
      "Loss: 1.783 | Acc: 34.002% (13144/38656)\n",
      "Loss: 1.782 | Acc: 34.001% (13165/38720)\n",
      "Loss: 1.782 | Acc: 34.019% (13194/38784)\n",
      "Loss: 1.782 | Acc: 34.025% (13218/38848)\n",
      "Loss: 1.781 | Acc: 34.041% (13246/38912)\n",
      "Loss: 1.781 | Acc: 34.054% (13273/38976)\n",
      "Loss: 1.781 | Acc: 34.068% (13300/39040)\n",
      "Loss: 1.780 | Acc: 34.073% (13324/39104)\n",
      "Loss: 1.780 | Acc: 34.076% (13347/39168)\n",
      "Loss: 1.780 | Acc: 34.074% (13368/39232)\n",
      "Loss: 1.780 | Acc: 34.075% (13390/39296)\n",
      "Loss: 1.780 | Acc: 34.093% (13419/39360)\n",
      "Loss: 1.779 | Acc: 34.109% (13447/39424)\n",
      "Loss: 1.779 | Acc: 34.122% (13474/39488)\n",
      "Loss: 1.779 | Acc: 34.137% (13502/39552)\n",
      "Loss: 1.778 | Acc: 34.165% (13535/39616)\n",
      "Loss: 1.778 | Acc: 34.166% (13557/39680)\n",
      "Loss: 1.777 | Acc: 34.184% (13586/39744)\n",
      "Loss: 1.777 | Acc: 34.187% (13609/39808)\n",
      "Loss: 1.777 | Acc: 34.189% (13632/39872)\n",
      "Loss: 1.777 | Acc: 34.212% (13663/39936)\n",
      "Loss: 1.777 | Acc: 34.222% (13689/40000)\n",
      "Loss: 1.776 | Acc: 34.243% (13719/40064)\n",
      "Loss: 1.776 | Acc: 34.250% (13744/40128)\n",
      "Loss: 1.776 | Acc: 34.258% (13769/40192)\n",
      "Loss: 1.776 | Acc: 34.256% (13790/40256)\n",
      "Loss: 1.776 | Acc: 34.258% (13813/40320)\n",
      "Loss: 1.775 | Acc: 34.273% (13841/40384)\n",
      "Loss: 1.775 | Acc: 34.291% (13870/40448)\n",
      "Loss: 1.774 | Acc: 34.294% (13893/40512)\n",
      "Loss: 1.774 | Acc: 34.304% (13919/40576)\n",
      "Loss: 1.774 | Acc: 34.311% (13944/40640)\n",
      "Loss: 1.773 | Acc: 34.333% (13975/40704)\n",
      "Loss: 1.773 | Acc: 34.336% (13998/40768)\n",
      "Loss: 1.773 | Acc: 34.360% (14030/40832)\n",
      "Loss: 1.773 | Acc: 34.365% (14054/40896)\n",
      "Loss: 1.772 | Acc: 34.380% (14082/40960)\n",
      "Loss: 1.772 | Acc: 34.377% (14103/41024)\n",
      "Loss: 1.772 | Acc: 34.390% (14130/41088)\n",
      "Loss: 1.772 | Acc: 34.399% (14156/41152)\n",
      "Loss: 1.771 | Acc: 34.404% (14180/41216)\n",
      "Loss: 1.771 | Acc: 34.411% (14205/41280)\n",
      "Loss: 1.771 | Acc: 34.421% (14231/41344)\n",
      "Loss: 1.772 | Acc: 34.438% (14260/41408)\n",
      "Loss: 1.771 | Acc: 34.440% (14283/41472)\n",
      "Loss: 1.771 | Acc: 34.442% (14306/41536)\n",
      "Loss: 1.771 | Acc: 34.447% (14330/41600)\n",
      "Loss: 1.771 | Acc: 34.449% (14353/41664)\n",
      "Loss: 1.771 | Acc: 34.454% (14377/41728)\n",
      "Loss: 1.771 | Acc: 34.449% (14397/41792)\n",
      "Loss: 1.770 | Acc: 34.471% (14428/41856)\n",
      "Loss: 1.770 | Acc: 34.475% (14452/41920)\n",
      "Loss: 1.770 | Acc: 34.487% (14479/41984)\n",
      "Loss: 1.770 | Acc: 34.496% (14505/42048)\n",
      "Loss: 1.769 | Acc: 34.506% (14531/42112)\n",
      "Loss: 1.769 | Acc: 34.510% (14555/42176)\n",
      "Loss: 1.769 | Acc: 34.519% (14581/42240)\n",
      "Loss: 1.769 | Acc: 34.514% (14601/42304)\n",
      "Loss: 1.769 | Acc: 34.524% (14627/42368)\n",
      "Loss: 1.768 | Acc: 34.542% (14657/42432)\n",
      "Loss: 1.768 | Acc: 34.559% (14686/42496)\n",
      "Loss: 1.768 | Acc: 34.549% (14704/42560)\n",
      "Loss: 1.768 | Acc: 34.556% (14729/42624)\n",
      "Loss: 1.768 | Acc: 34.558% (14752/42688)\n",
      "Loss: 1.768 | Acc: 34.576% (14782/42752)\n",
      "Loss: 1.767 | Acc: 34.597% (14813/42816)\n",
      "Loss: 1.767 | Acc: 34.608% (14840/42880)\n",
      "Loss: 1.767 | Acc: 34.608% (14862/42944)\n",
      "Loss: 1.767 | Acc: 34.610% (14885/43008)\n",
      "Loss: 1.767 | Acc: 34.619% (14911/43072)\n",
      "Loss: 1.767 | Acc: 34.630% (14938/43136)\n",
      "Loss: 1.766 | Acc: 34.639% (14964/43200)\n",
      "Loss: 1.766 | Acc: 34.655% (14993/43264)\n",
      "Loss: 1.766 | Acc: 34.666% (15020/43328)\n",
      "Loss: 1.766 | Acc: 34.695% (15055/43392)\n",
      "Loss: 1.765 | Acc: 34.699% (15079/43456)\n",
      "Loss: 1.765 | Acc: 34.701% (15102/43520)\n",
      "Loss: 1.765 | Acc: 34.715% (15130/43584)\n",
      "Loss: 1.765 | Acc: 34.707% (15149/43648)\n",
      "Loss: 1.765 | Acc: 34.718% (15176/43712)\n",
      "Loss: 1.764 | Acc: 34.738% (15207/43776)\n",
      "Loss: 1.764 | Acc: 34.772% (15244/43840)\n",
      "Loss: 1.764 | Acc: 34.780% (15270/43904)\n",
      "Loss: 1.763 | Acc: 34.784% (15294/43968)\n",
      "Loss: 1.763 | Acc: 34.791% (15319/44032)\n",
      "Loss: 1.763 | Acc: 34.790% (15341/44096)\n",
      "Loss: 1.763 | Acc: 34.803% (15369/44160)\n",
      "Loss: 1.763 | Acc: 34.823% (15400/44224)\n",
      "Loss: 1.763 | Acc: 34.831% (15426/44288)\n",
      "Loss: 1.763 | Acc: 34.851% (15457/44352)\n",
      "Loss: 1.763 | Acc: 34.841% (15475/44416)\n",
      "Loss: 1.762 | Acc: 34.852% (15502/44480)\n",
      "Loss: 1.762 | Acc: 34.860% (15528/44544)\n",
      "Loss: 1.762 | Acc: 34.877% (15558/44608)\n",
      "Loss: 1.762 | Acc: 34.888% (15585/44672)\n",
      "Loss: 1.761 | Acc: 34.903% (15614/44736)\n",
      "Loss: 1.762 | Acc: 34.886% (15629/44800)\n",
      "Loss: 1.762 | Acc: 34.899% (15657/44864)\n",
      "Loss: 1.761 | Acc: 34.903% (15681/44928)\n",
      "Loss: 1.761 | Acc: 34.904% (15704/44992)\n",
      "Loss: 1.761 | Acc: 34.919% (15733/45056)\n",
      "Loss: 1.760 | Acc: 34.934% (15762/45120)\n",
      "Loss: 1.760 | Acc: 34.933% (15784/45184)\n",
      "Loss: 1.760 | Acc: 34.952% (15815/45248)\n",
      "Loss: 1.760 | Acc: 34.953% (15838/45312)\n",
      "Loss: 1.760 | Acc: 34.968% (15867/45376)\n",
      "Loss: 1.760 | Acc: 34.965% (15888/45440)\n",
      "Loss: 1.760 | Acc: 34.964% (15910/45504)\n",
      "Loss: 1.760 | Acc: 34.963% (15932/45568)\n",
      "Loss: 1.760 | Acc: 34.973% (15959/45632)\n",
      "Loss: 1.760 | Acc: 34.968% (15979/45696)\n",
      "Loss: 1.760 | Acc: 34.969% (16002/45760)\n",
      "Loss: 1.759 | Acc: 34.975% (16027/45824)\n",
      "Loss: 1.759 | Acc: 34.976% (16050/45888)\n",
      "Loss: 1.759 | Acc: 34.991% (16079/45952)\n",
      "Loss: 1.759 | Acc: 35.001% (16106/46016)\n",
      "Loss: 1.758 | Acc: 35.015% (16135/46080)\n",
      "Loss: 1.758 | Acc: 35.029% (16164/46144)\n",
      "Loss: 1.758 | Acc: 35.033% (16188/46208)\n",
      "Loss: 1.758 | Acc: 35.032% (16210/46272)\n",
      "Loss: 1.758 | Acc: 35.029% (16231/46336)\n",
      "Loss: 1.758 | Acc: 35.034% (16256/46400)\n",
      "Loss: 1.758 | Acc: 35.055% (16288/46464)\n",
      "Loss: 1.758 | Acc: 35.074% (16319/46528)\n",
      "Loss: 1.758 | Acc: 35.077% (16343/46592)\n",
      "Loss: 1.757 | Acc: 35.080% (16367/46656)\n",
      "Loss: 1.757 | Acc: 35.081% (16390/46720)\n",
      "Loss: 1.757 | Acc: 35.080% (16412/46784)\n",
      "Loss: 1.757 | Acc: 35.082% (16435/46848)\n",
      "Loss: 1.757 | Acc: 35.085% (16459/46912)\n",
      "Loss: 1.757 | Acc: 35.092% (16485/46976)\n",
      "Loss: 1.757 | Acc: 35.104% (16513/47040)\n",
      "Loss: 1.757 | Acc: 35.114% (16540/47104)\n",
      "Loss: 1.757 | Acc: 35.117% (16564/47168)\n",
      "Loss: 1.756 | Acc: 35.144% (16599/47232)\n",
      "Loss: 1.756 | Acc: 35.159% (16629/47296)\n",
      "Loss: 1.756 | Acc: 35.173% (16658/47360)\n",
      "Loss: 1.756 | Acc: 35.174% (16681/47424)\n",
      "Loss: 1.756 | Acc: 35.186% (16709/47488)\n",
      "Loss: 1.755 | Acc: 35.204% (16740/47552)\n",
      "Loss: 1.755 | Acc: 35.228% (16774/47616)\n",
      "Loss: 1.755 | Acc: 35.235% (16800/47680)\n",
      "Loss: 1.755 | Acc: 35.242% (16826/47744)\n",
      "Loss: 1.755 | Acc: 35.247% (16851/47808)\n",
      "Loss: 1.755 | Acc: 35.248% (16874/47872)\n",
      "Loss: 1.755 | Acc: 35.268% (16906/47936)\n",
      "Loss: 1.755 | Acc: 35.265% (16927/48000)\n",
      "Loss: 1.754 | Acc: 35.284% (16959/48064)\n",
      "Loss: 1.755 | Acc: 35.283% (16981/48128)\n",
      "Loss: 1.754 | Acc: 35.282% (17003/48192)\n",
      "Loss: 1.754 | Acc: 35.285% (17027/48256)\n",
      "Loss: 1.754 | Acc: 35.294% (17054/48320)\n",
      "Loss: 1.754 | Acc: 35.305% (17082/48384)\n",
      "Loss: 1.753 | Acc: 35.318% (17111/48448)\n",
      "Loss: 1.754 | Acc: 35.317% (17133/48512)\n",
      "Loss: 1.753 | Acc: 35.324% (17159/48576)\n",
      "Loss: 1.753 | Acc: 35.331% (17185/48640)\n",
      "Loss: 1.753 | Acc: 35.332% (17208/48704)\n",
      "Loss: 1.753 | Acc: 35.343% (17236/48768)\n",
      "Loss: 1.753 | Acc: 35.348% (17261/48832)\n",
      "Loss: 1.753 | Acc: 35.357% (17288/48896)\n",
      "Loss: 1.753 | Acc: 35.362% (17313/48960)\n",
      "Loss: 1.752 | Acc: 35.361% (17327/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 35.361224489795916\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.546 | Acc: 34.375% (22/64)\n",
      "Loss: 1.531 | Acc: 39.844% (51/128)\n",
      "Loss: 1.547 | Acc: 40.625% (78/192)\n",
      "Loss: 1.593 | Acc: 37.891% (97/256)\n",
      "Loss: 1.576 | Acc: 39.375% (126/320)\n",
      "Loss: 1.611 | Acc: 38.542% (148/384)\n",
      "Loss: 1.635 | Acc: 37.054% (166/448)\n",
      "Loss: 1.624 | Acc: 37.695% (193/512)\n",
      "Loss: 1.619 | Acc: 38.194% (220/576)\n",
      "Loss: 1.611 | Acc: 38.281% (245/640)\n",
      "Loss: 1.625 | Acc: 37.926% (267/704)\n",
      "Loss: 1.629 | Acc: 38.151% (293/768)\n",
      "Loss: 1.629 | Acc: 37.740% (314/832)\n",
      "Loss: 1.623 | Acc: 38.058% (341/896)\n",
      "Loss: 1.618 | Acc: 38.542% (370/960)\n",
      "Loss: 1.612 | Acc: 39.258% (402/1024)\n",
      "Loss: 1.619 | Acc: 38.695% (421/1088)\n",
      "Loss: 1.612 | Acc: 38.976% (449/1152)\n",
      "Loss: 1.614 | Acc: 38.980% (474/1216)\n",
      "Loss: 1.615 | Acc: 38.906% (498/1280)\n",
      "Loss: 1.618 | Acc: 38.616% (519/1344)\n",
      "Loss: 1.622 | Acc: 38.849% (547/1408)\n",
      "Loss: 1.620 | Acc: 38.927% (573/1472)\n",
      "Loss: 1.617 | Acc: 38.997% (599/1536)\n",
      "Loss: 1.614 | Acc: 39.250% (628/1600)\n",
      "Loss: 1.617 | Acc: 39.243% (653/1664)\n",
      "Loss: 1.613 | Acc: 39.583% (684/1728)\n",
      "Loss: 1.611 | Acc: 39.732% (712/1792)\n",
      "Loss: 1.612 | Acc: 39.817% (739/1856)\n",
      "Loss: 1.612 | Acc: 40.000% (768/1920)\n",
      "Loss: 1.610 | Acc: 40.121% (796/1984)\n",
      "Loss: 1.610 | Acc: 40.234% (824/2048)\n",
      "Loss: 1.606 | Acc: 40.294% (851/2112)\n",
      "Loss: 1.609 | Acc: 40.303% (877/2176)\n",
      "Loss: 1.608 | Acc: 40.223% (901/2240)\n",
      "Loss: 1.609 | Acc: 40.061% (923/2304)\n",
      "Loss: 1.614 | Acc: 39.738% (941/2368)\n",
      "Loss: 1.613 | Acc: 39.762% (967/2432)\n",
      "Loss: 1.612 | Acc: 39.824% (994/2496)\n",
      "Loss: 1.612 | Acc: 39.883% (1021/2560)\n",
      "Loss: 1.612 | Acc: 39.977% (1049/2624)\n",
      "Loss: 1.613 | Acc: 39.769% (1069/2688)\n",
      "Loss: 1.611 | Acc: 40.007% (1101/2752)\n",
      "Loss: 1.612 | Acc: 39.950% (1125/2816)\n",
      "Loss: 1.612 | Acc: 40.035% (1153/2880)\n",
      "Loss: 1.613 | Acc: 39.844% (1173/2944)\n",
      "Loss: 1.611 | Acc: 39.860% (1199/3008)\n",
      "Loss: 1.609 | Acc: 39.876% (1225/3072)\n",
      "Loss: 1.609 | Acc: 39.764% (1247/3136)\n",
      "Loss: 1.607 | Acc: 39.812% (1274/3200)\n",
      "Loss: 1.606 | Acc: 39.951% (1304/3264)\n",
      "Loss: 1.607 | Acc: 39.964% (1330/3328)\n",
      "Loss: 1.607 | Acc: 39.800% (1350/3392)\n",
      "Loss: 1.608 | Acc: 39.873% (1378/3456)\n",
      "Loss: 1.610 | Acc: 39.744% (1399/3520)\n",
      "Loss: 1.608 | Acc: 39.872% (1429/3584)\n",
      "Loss: 1.610 | Acc: 39.611% (1445/3648)\n",
      "Loss: 1.607 | Acc: 39.709% (1474/3712)\n",
      "Loss: 1.608 | Acc: 39.619% (1496/3776)\n",
      "Loss: 1.607 | Acc: 39.557% (1519/3840)\n",
      "Loss: 1.607 | Acc: 39.626% (1547/3904)\n",
      "Loss: 1.608 | Acc: 39.743% (1577/3968)\n",
      "Loss: 1.608 | Acc: 39.831% (1606/4032)\n",
      "Loss: 1.609 | Acc: 39.746% (1628/4096)\n",
      "Loss: 1.611 | Acc: 39.760% (1654/4160)\n",
      "Loss: 1.609 | Acc: 39.938% (1687/4224)\n",
      "Loss: 1.610 | Acc: 39.949% (1713/4288)\n",
      "Loss: 1.613 | Acc: 40.028% (1742/4352)\n",
      "Loss: 1.610 | Acc: 40.217% (1776/4416)\n",
      "Loss: 1.611 | Acc: 40.179% (1800/4480)\n",
      "Loss: 1.610 | Acc: 40.251% (1829/4544)\n",
      "Loss: 1.611 | Acc: 40.256% (1855/4608)\n",
      "Loss: 1.611 | Acc: 40.283% (1882/4672)\n",
      "Loss: 1.609 | Acc: 40.351% (1911/4736)\n",
      "Loss: 1.611 | Acc: 40.271% (1933/4800)\n",
      "Loss: 1.611 | Acc: 40.296% (1960/4864)\n",
      "Loss: 1.611 | Acc: 40.260% (1984/4928)\n",
      "Loss: 1.613 | Acc: 40.164% (2005/4992)\n",
      "Loss: 1.613 | Acc: 40.091% (2027/5056)\n",
      "Loss: 1.615 | Acc: 40.137% (2055/5120)\n",
      "Loss: 1.615 | Acc: 40.162% (2082/5184)\n",
      "Loss: 1.617 | Acc: 40.130% (2106/5248)\n",
      "Loss: 1.617 | Acc: 40.173% (2134/5312)\n",
      "Loss: 1.618 | Acc: 40.123% (2157/5376)\n",
      "Loss: 1.619 | Acc: 40.074% (2180/5440)\n",
      "Loss: 1.619 | Acc: 40.062% (2205/5504)\n",
      "Loss: 1.622 | Acc: 40.032% (2229/5568)\n",
      "Loss: 1.622 | Acc: 40.075% (2257/5632)\n",
      "Loss: 1.621 | Acc: 40.256% (2293/5696)\n",
      "Loss: 1.619 | Acc: 40.347% (2324/5760)\n",
      "Loss: 1.619 | Acc: 40.350% (2350/5824)\n",
      "Loss: 1.620 | Acc: 40.251% (2370/5888)\n",
      "Loss: 1.621 | Acc: 40.255% (2396/5952)\n",
      "Loss: 1.620 | Acc: 40.326% (2426/6016)\n",
      "Loss: 1.619 | Acc: 40.345% (2453/6080)\n",
      "Loss: 1.620 | Acc: 40.332% (2478/6144)\n",
      "Loss: 1.621 | Acc: 40.222% (2497/6208)\n",
      "Loss: 1.621 | Acc: 40.258% (2525/6272)\n",
      "Loss: 1.621 | Acc: 40.215% (2548/6336)\n",
      "Loss: 1.621 | Acc: 40.141% (2569/6400)\n",
      "Loss: 1.622 | Acc: 40.114% (2593/6464)\n",
      "Loss: 1.622 | Acc: 40.119% (2619/6528)\n",
      "Loss: 1.623 | Acc: 40.109% (2644/6592)\n",
      "Loss: 1.622 | Acc: 40.129% (2671/6656)\n",
      "Loss: 1.622 | Acc: 40.164% (2699/6720)\n",
      "Loss: 1.621 | Acc: 40.198% (2727/6784)\n",
      "Loss: 1.619 | Acc: 40.289% (2759/6848)\n",
      "Loss: 1.621 | Acc: 40.220% (2780/6912)\n",
      "Loss: 1.622 | Acc: 40.152% (2801/6976)\n",
      "Loss: 1.624 | Acc: 40.085% (2822/7040)\n",
      "Loss: 1.624 | Acc: 40.090% (2848/7104)\n",
      "Loss: 1.624 | Acc: 40.123% (2876/7168)\n",
      "Loss: 1.625 | Acc: 40.003% (2893/7232)\n",
      "Loss: 1.624 | Acc: 40.077% (2924/7296)\n",
      "Loss: 1.623 | Acc: 40.095% (2951/7360)\n",
      "Loss: 1.623 | Acc: 40.019% (2971/7424)\n",
      "Loss: 1.622 | Acc: 40.037% (2998/7488)\n",
      "Loss: 1.621 | Acc: 40.069% (3026/7552)\n",
      "Loss: 1.622 | Acc: 39.995% (3046/7616)\n",
      "Loss: 1.622 | Acc: 39.948% (3068/7680)\n",
      "Loss: 1.621 | Acc: 39.992% (3097/7744)\n",
      "Loss: 1.620 | Acc: 40.061% (3128/7808)\n",
      "Loss: 1.621 | Acc: 39.990% (3148/7872)\n",
      "Loss: 1.622 | Acc: 39.932% (3169/7936)\n",
      "Loss: 1.624 | Acc: 39.875% (3190/8000)\n",
      "Loss: 1.624 | Acc: 39.893% (3217/8064)\n",
      "Loss: 1.624 | Acc: 39.862% (3240/8128)\n",
      "Loss: 1.623 | Acc: 39.868% (3266/8192)\n",
      "Loss: 1.624 | Acc: 39.874% (3292/8256)\n",
      "Loss: 1.626 | Acc: 39.760% (3308/8320)\n",
      "Loss: 1.627 | Acc: 39.766% (3334/8384)\n",
      "Loss: 1.627 | Acc: 39.761% (3359/8448)\n",
      "Loss: 1.628 | Acc: 39.767% (3385/8512)\n",
      "Loss: 1.628 | Acc: 39.762% (3410/8576)\n",
      "Loss: 1.628 | Acc: 39.769% (3436/8640)\n",
      "Loss: 1.628 | Acc: 39.706% (3456/8704)\n",
      "Loss: 1.629 | Acc: 39.690% (3480/8768)\n",
      "Loss: 1.629 | Acc: 39.685% (3505/8832)\n",
      "Loss: 1.627 | Acc: 39.714% (3533/8896)\n",
      "Loss: 1.626 | Acc: 39.743% (3561/8960)\n",
      "Loss: 1.626 | Acc: 39.805% (3592/9024)\n",
      "Loss: 1.626 | Acc: 39.756% (3613/9088)\n",
      "Loss: 1.626 | Acc: 39.729% (3636/9152)\n",
      "Loss: 1.625 | Acc: 39.811% (3669/9216)\n",
      "Loss: 1.624 | Acc: 39.860% (3699/9280)\n",
      "Loss: 1.625 | Acc: 39.854% (3724/9344)\n",
      "Loss: 1.625 | Acc: 39.881% (3752/9408)\n",
      "Loss: 1.626 | Acc: 39.865% (3776/9472)\n",
      "Loss: 1.626 | Acc: 39.912% (3806/9536)\n",
      "Loss: 1.625 | Acc: 39.969% (3837/9600)\n",
      "Loss: 1.625 | Acc: 39.983% (3864/9664)\n",
      "Loss: 1.625 | Acc: 39.967% (3888/9728)\n",
      "Loss: 1.626 | Acc: 39.951% (3912/9792)\n",
      "Loss: 1.625 | Acc: 39.955% (3938/9856)\n",
      "Loss: 1.626 | Acc: 39.889% (3957/9920)\n",
      "Loss: 1.625 | Acc: 39.874% (3981/9984)\n",
      "Loss: 1.623 | Acc: 39.890% (3989/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 39.89\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.655 | Acc: 37.500% (24/64)\n",
      "Loss: 1.689 | Acc: 36.719% (47/128)\n",
      "Loss: 1.663 | Acc: 39.583% (76/192)\n",
      "Loss: 1.662 | Acc: 40.234% (103/256)\n",
      "Loss: 1.654 | Acc: 41.562% (133/320)\n",
      "Loss: 1.650 | Acc: 40.104% (154/384)\n",
      "Loss: 1.638 | Acc: 40.402% (181/448)\n",
      "Loss: 1.653 | Acc: 39.844% (204/512)\n",
      "Loss: 1.641 | Acc: 39.931% (230/576)\n",
      "Loss: 1.634 | Acc: 40.156% (257/640)\n",
      "Loss: 1.619 | Acc: 40.483% (285/704)\n",
      "Loss: 1.621 | Acc: 40.625% (312/768)\n",
      "Loss: 1.625 | Acc: 40.986% (341/832)\n",
      "Loss: 1.628 | Acc: 40.625% (364/896)\n",
      "Loss: 1.627 | Acc: 40.729% (391/960)\n",
      "Loss: 1.627 | Acc: 40.723% (417/1024)\n",
      "Loss: 1.614 | Acc: 40.901% (445/1088)\n",
      "Loss: 1.617 | Acc: 41.233% (475/1152)\n",
      "Loss: 1.614 | Acc: 41.365% (503/1216)\n",
      "Loss: 1.615 | Acc: 41.016% (525/1280)\n",
      "Loss: 1.609 | Acc: 41.518% (558/1344)\n",
      "Loss: 1.610 | Acc: 41.122% (579/1408)\n",
      "Loss: 1.606 | Acc: 41.304% (608/1472)\n",
      "Loss: 1.609 | Acc: 41.081% (631/1536)\n",
      "Loss: 1.605 | Acc: 41.312% (661/1600)\n",
      "Loss: 1.615 | Acc: 41.106% (684/1664)\n",
      "Loss: 1.627 | Acc: 40.799% (705/1728)\n",
      "Loss: 1.631 | Acc: 40.848% (732/1792)\n",
      "Loss: 1.633 | Acc: 40.625% (754/1856)\n",
      "Loss: 1.631 | Acc: 40.677% (781/1920)\n",
      "Loss: 1.629 | Acc: 40.927% (812/1984)\n",
      "Loss: 1.624 | Acc: 41.064% (841/2048)\n",
      "Loss: 1.628 | Acc: 41.146% (869/2112)\n",
      "Loss: 1.632 | Acc: 41.131% (895/2176)\n",
      "Loss: 1.633 | Acc: 41.116% (921/2240)\n",
      "Loss: 1.635 | Acc: 40.972% (944/2304)\n",
      "Loss: 1.637 | Acc: 40.752% (965/2368)\n",
      "Loss: 1.632 | Acc: 40.872% (994/2432)\n",
      "Loss: 1.630 | Acc: 40.946% (1022/2496)\n",
      "Loss: 1.625 | Acc: 41.172% (1054/2560)\n",
      "Loss: 1.619 | Acc: 41.692% (1094/2624)\n",
      "Loss: 1.612 | Acc: 41.964% (1128/2688)\n",
      "Loss: 1.610 | Acc: 42.006% (1156/2752)\n",
      "Loss: 1.608 | Acc: 42.010% (1183/2816)\n",
      "Loss: 1.603 | Acc: 42.361% (1220/2880)\n",
      "Loss: 1.604 | Acc: 42.221% (1243/2944)\n",
      "Loss: 1.610 | Acc: 42.021% (1264/3008)\n",
      "Loss: 1.609 | Acc: 42.090% (1293/3072)\n",
      "Loss: 1.607 | Acc: 42.219% (1324/3136)\n",
      "Loss: 1.611 | Acc: 42.125% (1348/3200)\n",
      "Loss: 1.607 | Acc: 42.218% (1378/3264)\n",
      "Loss: 1.605 | Acc: 42.157% (1403/3328)\n",
      "Loss: 1.606 | Acc: 42.099% (1428/3392)\n",
      "Loss: 1.605 | Acc: 42.130% (1456/3456)\n",
      "Loss: 1.600 | Acc: 42.273% (1488/3520)\n",
      "Loss: 1.600 | Acc: 42.299% (1516/3584)\n",
      "Loss: 1.596 | Acc: 42.407% (1547/3648)\n",
      "Loss: 1.595 | Acc: 42.457% (1576/3712)\n",
      "Loss: 1.597 | Acc: 42.320% (1598/3776)\n",
      "Loss: 1.594 | Acc: 42.344% (1626/3840)\n",
      "Loss: 1.592 | Acc: 42.392% (1655/3904)\n",
      "Loss: 1.591 | Acc: 42.389% (1682/3968)\n",
      "Loss: 1.592 | Acc: 42.312% (1706/4032)\n",
      "Loss: 1.595 | Acc: 42.261% (1731/4096)\n",
      "Loss: 1.597 | Acc: 42.260% (1758/4160)\n",
      "Loss: 1.598 | Acc: 42.306% (1787/4224)\n",
      "Loss: 1.600 | Acc: 42.188% (1809/4288)\n",
      "Loss: 1.598 | Acc: 42.165% (1835/4352)\n",
      "Loss: 1.602 | Acc: 42.029% (1856/4416)\n",
      "Loss: 1.606 | Acc: 41.830% (1874/4480)\n",
      "Loss: 1.606 | Acc: 41.747% (1897/4544)\n",
      "Loss: 1.605 | Acc: 41.732% (1923/4608)\n",
      "Loss: 1.605 | Acc: 41.759% (1951/4672)\n",
      "Loss: 1.607 | Acc: 41.765% (1978/4736)\n",
      "Loss: 1.609 | Acc: 41.688% (2001/4800)\n",
      "Loss: 1.609 | Acc: 41.735% (2030/4864)\n",
      "Loss: 1.610 | Acc: 41.721% (2056/4928)\n",
      "Loss: 1.612 | Acc: 41.727% (2083/4992)\n",
      "Loss: 1.615 | Acc: 41.574% (2102/5056)\n",
      "Loss: 1.615 | Acc: 41.602% (2130/5120)\n",
      "Loss: 1.615 | Acc: 41.667% (2160/5184)\n",
      "Loss: 1.616 | Acc: 41.673% (2187/5248)\n",
      "Loss: 1.616 | Acc: 41.773% (2219/5312)\n",
      "Loss: 1.614 | Acc: 41.853% (2250/5376)\n",
      "Loss: 1.613 | Acc: 41.912% (2280/5440)\n",
      "Loss: 1.611 | Acc: 41.879% (2305/5504)\n",
      "Loss: 1.611 | Acc: 41.882% (2332/5568)\n",
      "Loss: 1.612 | Acc: 41.815% (2355/5632)\n",
      "Loss: 1.610 | Acc: 41.836% (2383/5696)\n",
      "Loss: 1.608 | Acc: 41.944% (2416/5760)\n",
      "Loss: 1.608 | Acc: 41.947% (2443/5824)\n",
      "Loss: 1.608 | Acc: 41.933% (2469/5888)\n",
      "Loss: 1.609 | Acc: 41.885% (2493/5952)\n",
      "Loss: 1.608 | Acc: 41.855% (2518/6016)\n",
      "Loss: 1.609 | Acc: 41.842% (2544/6080)\n",
      "Loss: 1.610 | Acc: 41.797% (2568/6144)\n",
      "Loss: 1.611 | Acc: 41.753% (2592/6208)\n",
      "Loss: 1.610 | Acc: 41.789% (2621/6272)\n",
      "Loss: 1.611 | Acc: 41.746% (2645/6336)\n",
      "Loss: 1.609 | Acc: 41.766% (2673/6400)\n",
      "Loss: 1.610 | Acc: 41.754% (2699/6464)\n",
      "Loss: 1.608 | Acc: 41.850% (2732/6528)\n",
      "Loss: 1.609 | Acc: 41.869% (2760/6592)\n",
      "Loss: 1.611 | Acc: 41.767% (2780/6656)\n",
      "Loss: 1.611 | Acc: 41.756% (2806/6720)\n",
      "Loss: 1.611 | Acc: 41.760% (2833/6784)\n",
      "Loss: 1.610 | Acc: 41.720% (2857/6848)\n",
      "Loss: 1.609 | Acc: 41.753% (2886/6912)\n",
      "Loss: 1.610 | Acc: 41.757% (2913/6976)\n",
      "Loss: 1.609 | Acc: 41.804% (2943/7040)\n",
      "Loss: 1.606 | Acc: 41.920% (2978/7104)\n",
      "Loss: 1.607 | Acc: 41.881% (3002/7168)\n",
      "Loss: 1.609 | Acc: 41.773% (3021/7232)\n",
      "Loss: 1.610 | Acc: 41.708% (3043/7296)\n",
      "Loss: 1.610 | Acc: 41.685% (3068/7360)\n",
      "Loss: 1.610 | Acc: 41.689% (3095/7424)\n",
      "Loss: 1.611 | Acc: 41.520% (3109/7488)\n",
      "Loss: 1.611 | Acc: 41.525% (3136/7552)\n",
      "Loss: 1.613 | Acc: 41.465% (3158/7616)\n",
      "Loss: 1.612 | Acc: 41.445% (3183/7680)\n",
      "Loss: 1.613 | Acc: 41.400% (3206/7744)\n",
      "Loss: 1.613 | Acc: 41.393% (3232/7808)\n",
      "Loss: 1.612 | Acc: 41.413% (3260/7872)\n",
      "Loss: 1.612 | Acc: 41.368% (3283/7936)\n",
      "Loss: 1.612 | Acc: 41.388% (3311/8000)\n",
      "Loss: 1.613 | Acc: 41.369% (3336/8064)\n",
      "Loss: 1.613 | Acc: 41.351% (3361/8128)\n",
      "Loss: 1.612 | Acc: 41.382% (3390/8192)\n",
      "Loss: 1.611 | Acc: 41.388% (3417/8256)\n",
      "Loss: 1.611 | Acc: 41.406% (3445/8320)\n",
      "Loss: 1.609 | Acc: 41.508% (3480/8384)\n",
      "Loss: 1.611 | Acc: 41.477% (3504/8448)\n",
      "Loss: 1.610 | Acc: 41.447% (3528/8512)\n",
      "Loss: 1.610 | Acc: 41.488% (3558/8576)\n",
      "Loss: 1.610 | Acc: 41.458% (3582/8640)\n",
      "Loss: 1.610 | Acc: 41.475% (3610/8704)\n",
      "Loss: 1.610 | Acc: 41.492% (3638/8768)\n",
      "Loss: 1.611 | Acc: 41.429% (3659/8832)\n",
      "Loss: 1.610 | Acc: 41.468% (3689/8896)\n",
      "Loss: 1.610 | Acc: 41.462% (3715/8960)\n",
      "Loss: 1.611 | Acc: 41.390% (3735/9024)\n",
      "Loss: 1.611 | Acc: 41.428% (3765/9088)\n",
      "Loss: 1.610 | Acc: 41.423% (3791/9152)\n",
      "Loss: 1.611 | Acc: 41.406% (3816/9216)\n",
      "Loss: 1.611 | Acc: 41.379% (3840/9280)\n",
      "Loss: 1.612 | Acc: 41.396% (3868/9344)\n",
      "Loss: 1.611 | Acc: 41.433% (3898/9408)\n",
      "Loss: 1.610 | Acc: 41.375% (3919/9472)\n",
      "Loss: 1.610 | Acc: 41.391% (3947/9536)\n",
      "Loss: 1.612 | Acc: 41.302% (3965/9600)\n",
      "Loss: 1.612 | Acc: 41.287% (3990/9664)\n",
      "Loss: 1.611 | Acc: 41.314% (4019/9728)\n",
      "Loss: 1.610 | Acc: 41.401% (4054/9792)\n",
      "Loss: 1.610 | Acc: 41.356% (4076/9856)\n",
      "Loss: 1.610 | Acc: 41.391% (4106/9920)\n",
      "Loss: 1.609 | Acc: 41.466% (4140/9984)\n",
      "Loss: 1.608 | Acc: 41.471% (4167/10048)\n",
      "Loss: 1.609 | Acc: 41.406% (4187/10112)\n",
      "Loss: 1.609 | Acc: 41.372% (4210/10176)\n",
      "Loss: 1.609 | Acc: 41.357% (4235/10240)\n",
      "Loss: 1.607 | Acc: 41.421% (4268/10304)\n",
      "Loss: 1.607 | Acc: 41.435% (4296/10368)\n",
      "Loss: 1.608 | Acc: 41.430% (4322/10432)\n",
      "Loss: 1.609 | Acc: 41.444% (4350/10496)\n",
      "Loss: 1.609 | Acc: 41.458% (4378/10560)\n",
      "Loss: 1.608 | Acc: 41.500% (4409/10624)\n",
      "Loss: 1.607 | Acc: 41.504% (4436/10688)\n",
      "Loss: 1.606 | Acc: 41.509% (4463/10752)\n",
      "Loss: 1.607 | Acc: 41.494% (4488/10816)\n",
      "Loss: 1.607 | Acc: 41.489% (4514/10880)\n",
      "Loss: 1.606 | Acc: 41.566% (4549/10944)\n",
      "Loss: 1.606 | Acc: 41.497% (4568/11008)\n",
      "Loss: 1.608 | Acc: 41.420% (4586/11072)\n",
      "Loss: 1.608 | Acc: 41.406% (4611/11136)\n",
      "Loss: 1.607 | Acc: 41.411% (4638/11200)\n",
      "Loss: 1.607 | Acc: 41.397% (4663/11264)\n",
      "Loss: 1.608 | Acc: 41.393% (4689/11328)\n",
      "Loss: 1.608 | Acc: 41.345% (4710/11392)\n",
      "Loss: 1.608 | Acc: 41.332% (4735/11456)\n",
      "Loss: 1.607 | Acc: 41.380% (4767/11520)\n",
      "Loss: 1.606 | Acc: 41.411% (4797/11584)\n",
      "Loss: 1.607 | Acc: 41.355% (4817/11648)\n",
      "Loss: 1.607 | Acc: 41.359% (4844/11712)\n",
      "Loss: 1.607 | Acc: 41.372% (4872/11776)\n",
      "Loss: 1.606 | Acc: 41.394% (4901/11840)\n",
      "Loss: 1.606 | Acc: 41.423% (4931/11904)\n",
      "Loss: 1.606 | Acc: 41.410% (4956/11968)\n",
      "Loss: 1.606 | Acc: 41.431% (4985/12032)\n",
      "Loss: 1.606 | Acc: 41.452% (5014/12096)\n",
      "Loss: 1.606 | Acc: 41.439% (5039/12160)\n",
      "Loss: 1.607 | Acc: 41.419% (5063/12224)\n",
      "Loss: 1.607 | Acc: 41.447% (5093/12288)\n",
      "Loss: 1.605 | Acc: 41.467% (5122/12352)\n",
      "Loss: 1.604 | Acc: 41.519% (5155/12416)\n",
      "Loss: 1.604 | Acc: 41.498% (5179/12480)\n",
      "Loss: 1.604 | Acc: 41.550% (5212/12544)\n",
      "Loss: 1.604 | Acc: 41.545% (5238/12608)\n",
      "Loss: 1.603 | Acc: 41.572% (5268/12672)\n",
      "Loss: 1.604 | Acc: 41.528% (5289/12736)\n",
      "Loss: 1.603 | Acc: 41.500% (5312/12800)\n",
      "Loss: 1.602 | Acc: 41.550% (5345/12864)\n",
      "Loss: 1.602 | Acc: 41.553% (5372/12928)\n",
      "Loss: 1.601 | Acc: 41.564% (5400/12992)\n",
      "Loss: 1.600 | Acc: 41.598% (5431/13056)\n",
      "Loss: 1.601 | Acc: 41.578% (5455/13120)\n",
      "Loss: 1.600 | Acc: 41.626% (5488/13184)\n",
      "Loss: 1.600 | Acc: 41.674% (5521/13248)\n",
      "Loss: 1.599 | Acc: 41.729% (5555/13312)\n",
      "Loss: 1.599 | Acc: 41.784% (5589/13376)\n",
      "Loss: 1.600 | Acc: 41.719% (5607/13440)\n",
      "Loss: 1.601 | Acc: 41.706% (5632/13504)\n",
      "Loss: 1.601 | Acc: 41.716% (5660/13568)\n",
      "Loss: 1.600 | Acc: 41.755% (5692/13632)\n",
      "Loss: 1.601 | Acc: 41.742% (5717/13696)\n",
      "Loss: 1.601 | Acc: 41.715% (5740/13760)\n",
      "Loss: 1.601 | Acc: 41.717% (5767/13824)\n",
      "Loss: 1.601 | Acc: 41.719% (5794/13888)\n",
      "Loss: 1.601 | Acc: 41.743% (5824/13952)\n",
      "Loss: 1.601 | Acc: 41.774% (5855/14016)\n",
      "Loss: 1.600 | Acc: 41.783% (5883/14080)\n",
      "Loss: 1.600 | Acc: 41.806% (5913/14144)\n",
      "Loss: 1.600 | Acc: 41.836% (5944/14208)\n",
      "Loss: 1.600 | Acc: 41.851% (5973/14272)\n",
      "Loss: 1.600 | Acc: 41.818% (5995/14336)\n",
      "Loss: 1.600 | Acc: 41.792% (6018/14400)\n",
      "Loss: 1.600 | Acc: 41.800% (6046/14464)\n",
      "Loss: 1.600 | Acc: 41.843% (6079/14528)\n",
      "Loss: 1.600 | Acc: 41.769% (6095/14592)\n",
      "Loss: 1.599 | Acc: 41.764% (6121/14656)\n",
      "Loss: 1.599 | Acc: 41.746% (6145/14720)\n",
      "Loss: 1.599 | Acc: 41.741% (6171/14784)\n",
      "Loss: 1.599 | Acc: 41.777% (6203/14848)\n",
      "Loss: 1.599 | Acc: 41.731% (6223/14912)\n",
      "Loss: 1.599 | Acc: 41.740% (6251/14976)\n",
      "Loss: 1.599 | Acc: 41.749% (6279/15040)\n",
      "Loss: 1.598 | Acc: 41.724% (6302/15104)\n",
      "Loss: 1.599 | Acc: 41.680% (6322/15168)\n",
      "Loss: 1.599 | Acc: 41.689% (6350/15232)\n",
      "Loss: 1.599 | Acc: 41.710% (6380/15296)\n",
      "Loss: 1.599 | Acc: 41.706% (6406/15360)\n",
      "Loss: 1.600 | Acc: 41.701% (6432/15424)\n",
      "Loss: 1.600 | Acc: 41.736% (6464/15488)\n",
      "Loss: 1.600 | Acc: 41.712% (6487/15552)\n",
      "Loss: 1.600 | Acc: 41.688% (6510/15616)\n",
      "Loss: 1.598 | Acc: 41.716% (6541/15680)\n",
      "Loss: 1.599 | Acc: 41.724% (6569/15744)\n",
      "Loss: 1.598 | Acc: 41.751% (6600/15808)\n",
      "Loss: 1.598 | Acc: 41.791% (6633/15872)\n",
      "Loss: 1.598 | Acc: 41.767% (6656/15936)\n",
      "Loss: 1.597 | Acc: 41.769% (6683/16000)\n",
      "Loss: 1.599 | Acc: 41.739% (6705/16064)\n",
      "Loss: 1.598 | Acc: 41.753% (6734/16128)\n",
      "Loss: 1.597 | Acc: 41.805% (6769/16192)\n",
      "Loss: 1.597 | Acc: 41.806% (6796/16256)\n",
      "Loss: 1.597 | Acc: 41.832% (6827/16320)\n",
      "Loss: 1.597 | Acc: 41.827% (6853/16384)\n",
      "Loss: 1.596 | Acc: 41.859% (6885/16448)\n",
      "Loss: 1.596 | Acc: 41.879% (6915/16512)\n",
      "Loss: 1.595 | Acc: 41.934% (6951/16576)\n",
      "Loss: 1.595 | Acc: 41.929% (6977/16640)\n",
      "Loss: 1.596 | Acc: 41.906% (7000/16704)\n",
      "Loss: 1.595 | Acc: 41.931% (7031/16768)\n",
      "Loss: 1.595 | Acc: 41.926% (7057/16832)\n",
      "Loss: 1.595 | Acc: 41.939% (7086/16896)\n",
      "Loss: 1.595 | Acc: 41.916% (7109/16960)\n",
      "Loss: 1.595 | Acc: 41.947% (7141/17024)\n",
      "Loss: 1.594 | Acc: 41.942% (7167/17088)\n",
      "Loss: 1.594 | Acc: 41.960% (7197/17152)\n",
      "Loss: 1.595 | Acc: 41.955% (7223/17216)\n",
      "Loss: 1.595 | Acc: 41.904% (7241/17280)\n",
      "Loss: 1.595 | Acc: 41.888% (7265/17344)\n",
      "Loss: 1.594 | Acc: 41.918% (7297/17408)\n",
      "Loss: 1.595 | Acc: 41.924% (7325/17472)\n",
      "Loss: 1.595 | Acc: 41.937% (7354/17536)\n",
      "Loss: 1.595 | Acc: 41.932% (7380/17600)\n",
      "Loss: 1.595 | Acc: 41.933% (7407/17664)\n",
      "Loss: 1.594 | Acc: 41.968% (7440/17728)\n",
      "Loss: 1.595 | Acc: 41.923% (7459/17792)\n",
      "Loss: 1.596 | Acc: 41.885% (7479/17856)\n",
      "Loss: 1.596 | Acc: 41.875% (7504/17920)\n",
      "Loss: 1.595 | Acc: 41.893% (7534/17984)\n",
      "Loss: 1.595 | Acc: 41.905% (7563/18048)\n",
      "Loss: 1.595 | Acc: 41.878% (7585/18112)\n",
      "Loss: 1.595 | Acc: 41.857% (7608/18176)\n",
      "Loss: 1.595 | Acc: 41.875% (7638/18240)\n",
      "Loss: 1.595 | Acc: 41.876% (7665/18304)\n",
      "Loss: 1.595 | Acc: 41.877% (7692/18368)\n",
      "Loss: 1.595 | Acc: 41.851% (7714/18432)\n",
      "Loss: 1.596 | Acc: 41.809% (7733/18496)\n",
      "Loss: 1.595 | Acc: 41.805% (7759/18560)\n",
      "Loss: 1.595 | Acc: 41.763% (7778/18624)\n",
      "Loss: 1.595 | Acc: 41.749% (7802/18688)\n",
      "Loss: 1.596 | Acc: 41.729% (7825/18752)\n",
      "Loss: 1.595 | Acc: 41.741% (7854/18816)\n",
      "Loss: 1.595 | Acc: 41.753% (7883/18880)\n",
      "Loss: 1.596 | Acc: 41.734% (7906/18944)\n",
      "Loss: 1.596 | Acc: 41.719% (7930/19008)\n",
      "Loss: 1.596 | Acc: 41.710% (7955/19072)\n",
      "Loss: 1.595 | Acc: 41.722% (7984/19136)\n",
      "Loss: 1.595 | Acc: 41.708% (8008/19200)\n",
      "Loss: 1.595 | Acc: 41.725% (8038/19264)\n",
      "Loss: 1.595 | Acc: 41.680% (8056/19328)\n",
      "Loss: 1.595 | Acc: 41.682% (8083/19392)\n",
      "Loss: 1.595 | Acc: 41.694% (8112/19456)\n",
      "Loss: 1.595 | Acc: 41.680% (8136/19520)\n",
      "Loss: 1.595 | Acc: 41.692% (8165/19584)\n",
      "Loss: 1.594 | Acc: 41.750% (8203/19648)\n",
      "Loss: 1.594 | Acc: 41.761% (8232/19712)\n",
      "Loss: 1.594 | Acc: 41.748% (8256/19776)\n",
      "Loss: 1.594 | Acc: 41.724% (8278/19840)\n",
      "Loss: 1.594 | Acc: 41.725% (8305/19904)\n",
      "Loss: 1.594 | Acc: 41.732% (8333/19968)\n",
      "Loss: 1.593 | Acc: 41.733% (8360/20032)\n",
      "Loss: 1.593 | Acc: 41.745% (8389/20096)\n",
      "Loss: 1.593 | Acc: 41.746% (8416/20160)\n",
      "Loss: 1.592 | Acc: 41.747% (8443/20224)\n",
      "Loss: 1.592 | Acc: 41.739% (8468/20288)\n",
      "Loss: 1.592 | Acc: 41.745% (8496/20352)\n",
      "Loss: 1.592 | Acc: 41.752% (8524/20416)\n",
      "Loss: 1.592 | Acc: 41.729% (8546/20480)\n",
      "Loss: 1.591 | Acc: 41.769% (8581/20544)\n",
      "Loss: 1.591 | Acc: 41.775% (8609/20608)\n",
      "Loss: 1.591 | Acc: 41.791% (8639/20672)\n",
      "Loss: 1.591 | Acc: 41.792% (8666/20736)\n",
      "Loss: 1.591 | Acc: 41.812% (8697/20800)\n",
      "Loss: 1.590 | Acc: 41.833% (8728/20864)\n",
      "Loss: 1.590 | Acc: 41.848% (8758/20928)\n",
      "Loss: 1.589 | Acc: 41.859% (8787/20992)\n",
      "Loss: 1.589 | Acc: 41.884% (8819/21056)\n",
      "Loss: 1.589 | Acc: 41.899% (8849/21120)\n",
      "Loss: 1.589 | Acc: 41.909% (8878/21184)\n",
      "Loss: 1.590 | Acc: 41.877% (8898/21248)\n",
      "Loss: 1.590 | Acc: 41.840% (8917/21312)\n",
      "Loss: 1.591 | Acc: 41.818% (8939/21376)\n",
      "Loss: 1.591 | Acc: 41.810% (8964/21440)\n",
      "Loss: 1.591 | Acc: 41.829% (8995/21504)\n",
      "Loss: 1.592 | Acc: 41.770% (9009/21568)\n",
      "Loss: 1.592 | Acc: 41.790% (9040/21632)\n",
      "Loss: 1.592 | Acc: 41.773% (9063/21696)\n",
      "Loss: 1.592 | Acc: 41.769% (9089/21760)\n",
      "Loss: 1.593 | Acc: 41.766% (9115/21824)\n",
      "Loss: 1.593 | Acc: 41.772% (9143/21888)\n",
      "Loss: 1.593 | Acc: 41.750% (9165/21952)\n",
      "Loss: 1.593 | Acc: 41.761% (9194/22016)\n",
      "Loss: 1.593 | Acc: 41.735% (9215/22080)\n",
      "Loss: 1.593 | Acc: 41.745% (9244/22144)\n",
      "Loss: 1.593 | Acc: 41.746% (9271/22208)\n",
      "Loss: 1.593 | Acc: 41.743% (9297/22272)\n",
      "Loss: 1.594 | Acc: 41.722% (9319/22336)\n",
      "Loss: 1.594 | Acc: 41.723% (9346/22400)\n",
      "Loss: 1.593 | Acc: 41.756% (9380/22464)\n",
      "Loss: 1.593 | Acc: 41.775% (9411/22528)\n",
      "Loss: 1.593 | Acc: 41.771% (9437/22592)\n",
      "Loss: 1.593 | Acc: 41.746% (9458/22656)\n",
      "Loss: 1.593 | Acc: 41.739% (9483/22720)\n",
      "Loss: 1.593 | Acc: 41.744% (9511/22784)\n",
      "Loss: 1.593 | Acc: 41.759% (9541/22848)\n",
      "Loss: 1.593 | Acc: 41.751% (9566/22912)\n",
      "Loss: 1.594 | Acc: 41.739% (9590/22976)\n",
      "Loss: 1.594 | Acc: 41.727% (9614/23040)\n",
      "Loss: 1.594 | Acc: 41.733% (9642/23104)\n",
      "Loss: 1.593 | Acc: 41.730% (9668/23168)\n",
      "Loss: 1.594 | Acc: 41.710% (9690/23232)\n",
      "Loss: 1.594 | Acc: 41.720% (9719/23296)\n",
      "Loss: 1.594 | Acc: 41.729% (9748/23360)\n",
      "Loss: 1.594 | Acc: 41.726% (9774/23424)\n",
      "Loss: 1.594 | Acc: 41.698% (9794/23488)\n",
      "Loss: 1.594 | Acc: 41.720% (9826/23552)\n",
      "Loss: 1.594 | Acc: 41.747% (9859/23616)\n",
      "Loss: 1.594 | Acc: 41.744% (9885/23680)\n",
      "Loss: 1.594 | Acc: 41.762% (9916/23744)\n",
      "Loss: 1.594 | Acc: 41.776% (9946/23808)\n",
      "Loss: 1.594 | Acc: 41.781% (9974/23872)\n",
      "Loss: 1.595 | Acc: 41.757% (9995/23936)\n",
      "Loss: 1.594 | Acc: 41.775% (10026/24000)\n",
      "Loss: 1.594 | Acc: 41.772% (10052/24064)\n",
      "Loss: 1.594 | Acc: 41.765% (10077/24128)\n",
      "Loss: 1.594 | Acc: 41.758% (10102/24192)\n",
      "Loss: 1.594 | Acc: 41.775% (10133/24256)\n",
      "Loss: 1.594 | Acc: 41.785% (10162/24320)\n",
      "Loss: 1.594 | Acc: 41.749% (10180/24384)\n",
      "Loss: 1.594 | Acc: 41.705% (10196/24448)\n",
      "Loss: 1.594 | Acc: 41.714% (10225/24512)\n",
      "Loss: 1.594 | Acc: 41.715% (10252/24576)\n",
      "Loss: 1.594 | Acc: 41.733% (10283/24640)\n",
      "Loss: 1.593 | Acc: 41.734% (10310/24704)\n",
      "Loss: 1.593 | Acc: 41.719% (10333/24768)\n",
      "Loss: 1.593 | Acc: 41.728% (10362/24832)\n",
      "Loss: 1.593 | Acc: 41.726% (10388/24896)\n",
      "Loss: 1.593 | Acc: 41.735% (10417/24960)\n",
      "Loss: 1.593 | Acc: 41.748% (10447/25024)\n",
      "Loss: 1.592 | Acc: 41.757% (10476/25088)\n",
      "Loss: 1.592 | Acc: 41.774% (10507/25152)\n",
      "Loss: 1.592 | Acc: 41.775% (10534/25216)\n",
      "Loss: 1.592 | Acc: 41.800% (10567/25280)\n",
      "Loss: 1.592 | Acc: 41.781% (10589/25344)\n",
      "Loss: 1.592 | Acc: 41.751% (10608/25408)\n",
      "Loss: 1.592 | Acc: 41.760% (10637/25472)\n",
      "Loss: 1.592 | Acc: 41.757% (10663/25536)\n",
      "Loss: 1.592 | Acc: 41.754% (10689/25600)\n",
      "Loss: 1.592 | Acc: 41.747% (10714/25664)\n",
      "Loss: 1.592 | Acc: 41.744% (10740/25728)\n",
      "Loss: 1.592 | Acc: 41.722% (10761/25792)\n",
      "Loss: 1.592 | Acc: 41.723% (10788/25856)\n",
      "Loss: 1.592 | Acc: 41.736% (10818/25920)\n",
      "Loss: 1.592 | Acc: 41.760% (10851/25984)\n",
      "Loss: 1.592 | Acc: 41.769% (10880/26048)\n",
      "Loss: 1.592 | Acc: 41.782% (10910/26112)\n",
      "Loss: 1.592 | Acc: 41.771% (10934/26176)\n",
      "Loss: 1.592 | Acc: 41.795% (10967/26240)\n",
      "Loss: 1.591 | Acc: 41.815% (10999/26304)\n",
      "Loss: 1.591 | Acc: 41.816% (11026/26368)\n",
      "Loss: 1.591 | Acc: 41.839% (11059/26432)\n",
      "Loss: 1.590 | Acc: 41.863% (11092/26496)\n",
      "Loss: 1.590 | Acc: 41.875% (11122/26560)\n",
      "Loss: 1.590 | Acc: 41.880% (11150/26624)\n",
      "Loss: 1.590 | Acc: 41.884% (11178/26688)\n",
      "Loss: 1.589 | Acc: 41.918% (11214/26752)\n",
      "Loss: 1.589 | Acc: 41.912% (11239/26816)\n",
      "Loss: 1.590 | Acc: 41.897% (11262/26880)\n",
      "Loss: 1.589 | Acc: 41.898% (11289/26944)\n",
      "Loss: 1.589 | Acc: 41.906% (11318/27008)\n",
      "Loss: 1.589 | Acc: 41.918% (11348/27072)\n",
      "Loss: 1.589 | Acc: 41.941% (11381/27136)\n",
      "Loss: 1.589 | Acc: 41.923% (11403/27200)\n",
      "Loss: 1.589 | Acc: 41.916% (11428/27264)\n",
      "Loss: 1.589 | Acc: 41.906% (11452/27328)\n",
      "Loss: 1.589 | Acc: 41.906% (11479/27392)\n",
      "Loss: 1.588 | Acc: 41.933% (11513/27456)\n",
      "Loss: 1.588 | Acc: 41.922% (11537/27520)\n",
      "Loss: 1.588 | Acc: 41.923% (11564/27584)\n",
      "Loss: 1.587 | Acc: 41.934% (11594/27648)\n",
      "Loss: 1.588 | Acc: 41.928% (11619/27712)\n",
      "Loss: 1.588 | Acc: 41.914% (11642/27776)\n",
      "Loss: 1.588 | Acc: 41.943% (11677/27840)\n",
      "Loss: 1.588 | Acc: 41.944% (11704/27904)\n",
      "Loss: 1.587 | Acc: 41.966% (11737/27968)\n",
      "Loss: 1.588 | Acc: 41.952% (11760/28032)\n",
      "Loss: 1.588 | Acc: 41.935% (11782/28096)\n",
      "Loss: 1.588 | Acc: 41.957% (11815/28160)\n",
      "Loss: 1.588 | Acc: 41.957% (11842/28224)\n",
      "Loss: 1.588 | Acc: 41.958% (11869/28288)\n",
      "Loss: 1.588 | Acc: 41.955% (11895/28352)\n",
      "Loss: 1.588 | Acc: 41.966% (11925/28416)\n",
      "Loss: 1.588 | Acc: 41.970% (11953/28480)\n",
      "Loss: 1.588 | Acc: 41.963% (11978/28544)\n",
      "Loss: 1.588 | Acc: 41.967% (12006/28608)\n",
      "Loss: 1.587 | Acc: 41.982% (12037/28672)\n",
      "Loss: 1.587 | Acc: 41.986% (12065/28736)\n",
      "Loss: 1.586 | Acc: 42.000% (12096/28800)\n",
      "Loss: 1.586 | Acc: 41.990% (12120/28864)\n",
      "Loss: 1.586 | Acc: 41.987% (12146/28928)\n",
      "Loss: 1.587 | Acc: 41.963% (12166/28992)\n",
      "Loss: 1.587 | Acc: 41.947% (12188/29056)\n",
      "Loss: 1.587 | Acc: 41.944% (12214/29120)\n",
      "Loss: 1.587 | Acc: 41.961% (12246/29184)\n",
      "Loss: 1.587 | Acc: 41.948% (12269/29248)\n",
      "Loss: 1.586 | Acc: 41.976% (12304/29312)\n",
      "Loss: 1.586 | Acc: 41.973% (12330/29376)\n",
      "Loss: 1.587 | Acc: 41.970% (12356/29440)\n",
      "Loss: 1.587 | Acc: 41.954% (12378/29504)\n",
      "Loss: 1.587 | Acc: 41.961% (12407/29568)\n",
      "Loss: 1.587 | Acc: 41.968% (12436/29632)\n",
      "Loss: 1.587 | Acc: 41.985% (12468/29696)\n",
      "Loss: 1.587 | Acc: 41.962% (12488/29760)\n",
      "Loss: 1.587 | Acc: 41.973% (12518/29824)\n",
      "Loss: 1.587 | Acc: 41.963% (12542/29888)\n",
      "Loss: 1.587 | Acc: 41.947% (12564/29952)\n",
      "Loss: 1.587 | Acc: 41.941% (12589/30016)\n",
      "Loss: 1.587 | Acc: 41.951% (12619/30080)\n",
      "Loss: 1.587 | Acc: 41.945% (12644/30144)\n",
      "Loss: 1.588 | Acc: 41.916% (12662/30208)\n",
      "Loss: 1.588 | Acc: 41.900% (12684/30272)\n",
      "Loss: 1.588 | Acc: 41.914% (12715/30336)\n",
      "Loss: 1.588 | Acc: 41.918% (12743/30400)\n",
      "Loss: 1.588 | Acc: 41.922% (12771/30464)\n",
      "Loss: 1.588 | Acc: 41.922% (12798/30528)\n",
      "Loss: 1.588 | Acc: 41.903% (12819/30592)\n",
      "Loss: 1.588 | Acc: 41.913% (12849/30656)\n",
      "Loss: 1.588 | Acc: 41.891% (12869/30720)\n",
      "Loss: 1.588 | Acc: 41.889% (12895/30784)\n",
      "Loss: 1.588 | Acc: 41.883% (12920/30848)\n",
      "Loss: 1.588 | Acc: 41.893% (12950/30912)\n",
      "Loss: 1.589 | Acc: 41.871% (12970/30976)\n",
      "Loss: 1.589 | Acc: 41.875% (12998/31040)\n",
      "Loss: 1.589 | Acc: 41.882% (13027/31104)\n",
      "Loss: 1.589 | Acc: 41.873% (13051/31168)\n",
      "Loss: 1.588 | Acc: 41.887% (13082/31232)\n",
      "Loss: 1.588 | Acc: 41.894% (13111/31296)\n",
      "Loss: 1.588 | Acc: 41.885% (13135/31360)\n",
      "Loss: 1.588 | Acc: 41.901% (13167/31424)\n",
      "Loss: 1.588 | Acc: 41.899% (13193/31488)\n",
      "Loss: 1.588 | Acc: 41.896% (13219/31552)\n",
      "Loss: 1.588 | Acc: 41.906% (13249/31616)\n",
      "Loss: 1.588 | Acc: 41.900% (13274/31680)\n",
      "Loss: 1.588 | Acc: 41.898% (13300/31744)\n",
      "Loss: 1.588 | Acc: 41.908% (13330/31808)\n",
      "Loss: 1.588 | Acc: 41.893% (13352/31872)\n",
      "Loss: 1.588 | Acc: 41.893% (13379/31936)\n",
      "Loss: 1.588 | Acc: 41.919% (13414/32000)\n",
      "Loss: 1.587 | Acc: 41.944% (13449/32064)\n",
      "Loss: 1.587 | Acc: 41.942% (13475/32128)\n",
      "Loss: 1.587 | Acc: 41.939% (13501/32192)\n",
      "Loss: 1.588 | Acc: 41.924% (13523/32256)\n",
      "Loss: 1.588 | Acc: 41.921% (13549/32320)\n",
      "Loss: 1.587 | Acc: 41.919% (13575/32384)\n",
      "Loss: 1.587 | Acc: 41.922% (13603/32448)\n",
      "Loss: 1.587 | Acc: 41.923% (13630/32512)\n",
      "Loss: 1.586 | Acc: 41.933% (13660/32576)\n",
      "Loss: 1.586 | Acc: 41.964% (13697/32640)\n",
      "Loss: 1.586 | Acc: 41.977% (13728/32704)\n",
      "Loss: 1.585 | Acc: 41.974% (13754/32768)\n",
      "Loss: 1.586 | Acc: 41.965% (13778/32832)\n",
      "Loss: 1.586 | Acc: 41.978% (13809/32896)\n",
      "Loss: 1.585 | Acc: 41.978% (13836/32960)\n",
      "Loss: 1.585 | Acc: 42.003% (13871/33024)\n",
      "Loss: 1.585 | Acc: 42.006% (13899/33088)\n",
      "Loss: 1.585 | Acc: 42.003% (13925/33152)\n",
      "Loss: 1.584 | Acc: 42.022% (13958/33216)\n",
      "Loss: 1.584 | Acc: 42.013% (13982/33280)\n",
      "Loss: 1.584 | Acc: 42.020% (14011/33344)\n",
      "Loss: 1.584 | Acc: 42.032% (14042/33408)\n",
      "Loss: 1.583 | Acc: 42.041% (14072/33472)\n",
      "Loss: 1.583 | Acc: 42.056% (14104/33536)\n",
      "Loss: 1.583 | Acc: 42.080% (14139/33600)\n",
      "Loss: 1.583 | Acc: 42.098% (14172/33664)\n",
      "Loss: 1.582 | Acc: 42.110% (14203/33728)\n",
      "Loss: 1.583 | Acc: 42.105% (14228/33792)\n",
      "Loss: 1.583 | Acc: 42.102% (14254/33856)\n",
      "Loss: 1.582 | Acc: 42.117% (14286/33920)\n",
      "Loss: 1.582 | Acc: 42.120% (14314/33984)\n",
      "Loss: 1.582 | Acc: 42.120% (14341/34048)\n",
      "Loss: 1.582 | Acc: 42.100% (14361/34112)\n",
      "Loss: 1.583 | Acc: 42.079% (14381/34176)\n",
      "Loss: 1.583 | Acc: 42.088% (14411/34240)\n",
      "Loss: 1.583 | Acc: 42.071% (14432/34304)\n",
      "Loss: 1.583 | Acc: 42.077% (14461/34368)\n",
      "Loss: 1.583 | Acc: 42.060% (14482/34432)\n",
      "Loss: 1.583 | Acc: 42.048% (14505/34496)\n",
      "Loss: 1.583 | Acc: 42.054% (14534/34560)\n",
      "Loss: 1.583 | Acc: 42.075% (14568/34624)\n",
      "Loss: 1.583 | Acc: 42.087% (14599/34688)\n",
      "Loss: 1.582 | Acc: 42.095% (14629/34752)\n",
      "Loss: 1.582 | Acc: 42.104% (14659/34816)\n",
      "Loss: 1.582 | Acc: 42.113% (14689/34880)\n",
      "Loss: 1.582 | Acc: 42.122% (14719/34944)\n",
      "Loss: 1.582 | Acc: 42.119% (14745/35008)\n",
      "Loss: 1.582 | Acc: 42.128% (14775/35072)\n",
      "Loss: 1.582 | Acc: 42.139% (14806/35136)\n",
      "Loss: 1.582 | Acc: 42.148% (14836/35200)\n",
      "Loss: 1.582 | Acc: 42.136% (14859/35264)\n",
      "Loss: 1.582 | Acc: 42.142% (14888/35328)\n",
      "Loss: 1.582 | Acc: 42.123% (14908/35392)\n",
      "Loss: 1.582 | Acc: 42.148% (14944/35456)\n",
      "Loss: 1.582 | Acc: 42.145% (14970/35520)\n",
      "Loss: 1.581 | Acc: 42.148% (14998/35584)\n",
      "Loss: 1.581 | Acc: 42.151% (15026/35648)\n",
      "Loss: 1.581 | Acc: 42.162% (15057/35712)\n",
      "Loss: 1.581 | Acc: 42.162% (15084/35776)\n",
      "Loss: 1.581 | Acc: 42.165% (15112/35840)\n",
      "Loss: 1.581 | Acc: 42.176% (15143/35904)\n",
      "Loss: 1.581 | Acc: 42.157% (15163/35968)\n",
      "Loss: 1.581 | Acc: 42.151% (15188/36032)\n",
      "Loss: 1.581 | Acc: 42.132% (15208/36096)\n",
      "Loss: 1.581 | Acc: 42.121% (15231/36160)\n",
      "Loss: 1.581 | Acc: 42.116% (15256/36224)\n",
      "Loss: 1.581 | Acc: 42.116% (15283/36288)\n",
      "Loss: 1.581 | Acc: 42.132% (15316/36352)\n",
      "Loss: 1.580 | Acc: 42.130% (15342/36416)\n",
      "Loss: 1.580 | Acc: 42.133% (15370/36480)\n",
      "Loss: 1.581 | Acc: 42.116% (15391/36544)\n",
      "Loss: 1.581 | Acc: 42.108% (15415/36608)\n",
      "Loss: 1.581 | Acc: 42.111% (15443/36672)\n",
      "Loss: 1.581 | Acc: 42.109% (15469/36736)\n",
      "Loss: 1.581 | Acc: 42.095% (15491/36800)\n",
      "Loss: 1.581 | Acc: 42.098% (15519/36864)\n",
      "Loss: 1.580 | Acc: 42.114% (15552/36928)\n",
      "Loss: 1.580 | Acc: 42.150% (15592/36992)\n",
      "Loss: 1.580 | Acc: 42.150% (15619/37056)\n",
      "Loss: 1.580 | Acc: 42.131% (15639/37120)\n",
      "Loss: 1.580 | Acc: 42.142% (15670/37184)\n",
      "Loss: 1.580 | Acc: 42.131% (15693/37248)\n",
      "Loss: 1.581 | Acc: 42.120% (15716/37312)\n",
      "Loss: 1.581 | Acc: 42.118% (15742/37376)\n",
      "Loss: 1.581 | Acc: 42.123% (15771/37440)\n",
      "Loss: 1.581 | Acc: 42.108% (15792/37504)\n",
      "Loss: 1.581 | Acc: 42.097% (15815/37568)\n",
      "Loss: 1.581 | Acc: 42.076% (15834/37632)\n",
      "Loss: 1.581 | Acc: 42.081% (15863/37696)\n",
      "Loss: 1.581 | Acc: 42.108% (15900/37760)\n",
      "Loss: 1.581 | Acc: 42.121% (15932/37824)\n",
      "Loss: 1.581 | Acc: 42.127% (15961/37888)\n",
      "Loss: 1.580 | Acc: 42.137% (15992/37952)\n",
      "Loss: 1.580 | Acc: 42.143% (16021/38016)\n",
      "Loss: 1.580 | Acc: 42.169% (16058/38080)\n",
      "Loss: 1.579 | Acc: 42.172% (16086/38144)\n",
      "Loss: 1.579 | Acc: 42.195% (16122/38208)\n",
      "Loss: 1.579 | Acc: 42.193% (16148/38272)\n",
      "Loss: 1.579 | Acc: 42.208% (16181/38336)\n",
      "Loss: 1.579 | Acc: 42.195% (16203/38400)\n",
      "Loss: 1.579 | Acc: 42.206% (16234/38464)\n",
      "Loss: 1.579 | Acc: 42.203% (16260/38528)\n",
      "Loss: 1.579 | Acc: 42.200% (16286/38592)\n",
      "Loss: 1.579 | Acc: 42.200% (16313/38656)\n",
      "Loss: 1.579 | Acc: 42.206% (16342/38720)\n",
      "Loss: 1.579 | Acc: 42.190% (16363/38784)\n",
      "Loss: 1.579 | Acc: 42.190% (16390/38848)\n",
      "Loss: 1.579 | Acc: 42.172% (16410/38912)\n",
      "Loss: 1.579 | Acc: 42.167% (16435/38976)\n",
      "Loss: 1.579 | Acc: 42.172% (16464/39040)\n",
      "Loss: 1.579 | Acc: 42.167% (16489/39104)\n",
      "Loss: 1.579 | Acc: 42.144% (16507/39168)\n",
      "Loss: 1.579 | Acc: 42.137% (16531/39232)\n",
      "Loss: 1.579 | Acc: 42.144% (16561/39296)\n",
      "Loss: 1.579 | Acc: 42.142% (16587/39360)\n",
      "Loss: 1.579 | Acc: 42.149% (16617/39424)\n",
      "Loss: 1.579 | Acc: 42.155% (16646/39488)\n",
      "Loss: 1.579 | Acc: 42.155% (16673/39552)\n",
      "Loss: 1.579 | Acc: 42.142% (16695/39616)\n",
      "Loss: 1.579 | Acc: 42.137% (16720/39680)\n",
      "Loss: 1.579 | Acc: 42.155% (16754/39744)\n",
      "Loss: 1.579 | Acc: 42.172% (16788/39808)\n",
      "Loss: 1.579 | Acc: 42.185% (16820/39872)\n",
      "Loss: 1.579 | Acc: 42.170% (16841/39936)\n",
      "Loss: 1.579 | Acc: 42.180% (16872/40000)\n",
      "Loss: 1.579 | Acc: 42.183% (16900/40064)\n",
      "Loss: 1.579 | Acc: 42.188% (16929/40128)\n",
      "Loss: 1.578 | Acc: 42.202% (16962/40192)\n",
      "Loss: 1.579 | Acc: 42.195% (16986/40256)\n",
      "Loss: 1.578 | Acc: 42.200% (17015/40320)\n",
      "Loss: 1.578 | Acc: 42.215% (17048/40384)\n",
      "Loss: 1.578 | Acc: 42.215% (17075/40448)\n",
      "Loss: 1.578 | Acc: 42.222% (17105/40512)\n",
      "Loss: 1.578 | Acc: 42.222% (17132/40576)\n",
      "Loss: 1.579 | Acc: 42.210% (17154/40640)\n",
      "Loss: 1.578 | Acc: 42.222% (17186/40704)\n",
      "Loss: 1.578 | Acc: 42.219% (17212/40768)\n",
      "Loss: 1.578 | Acc: 42.224% (17241/40832)\n",
      "Loss: 1.578 | Acc: 42.222% (17267/40896)\n",
      "Loss: 1.578 | Acc: 42.231% (17298/40960)\n",
      "Loss: 1.578 | Acc: 42.229% (17324/41024)\n",
      "Loss: 1.578 | Acc: 42.226% (17350/41088)\n",
      "Loss: 1.578 | Acc: 42.209% (17370/41152)\n",
      "Loss: 1.578 | Acc: 42.221% (17402/41216)\n",
      "Loss: 1.578 | Acc: 42.224% (17430/41280)\n",
      "Loss: 1.578 | Acc: 42.219% (17455/41344)\n",
      "Loss: 1.579 | Acc: 42.221% (17483/41408)\n",
      "Loss: 1.579 | Acc: 42.219% (17509/41472)\n",
      "Loss: 1.579 | Acc: 42.219% (17536/41536)\n",
      "Loss: 1.579 | Acc: 42.216% (17562/41600)\n",
      "Loss: 1.578 | Acc: 42.231% (17595/41664)\n",
      "Loss: 1.578 | Acc: 42.233% (17623/41728)\n",
      "Loss: 1.578 | Acc: 42.226% (17647/41792)\n",
      "Loss: 1.579 | Acc: 42.211% (17668/41856)\n",
      "Loss: 1.579 | Acc: 42.209% (17694/41920)\n",
      "Loss: 1.579 | Acc: 42.204% (17719/41984)\n",
      "Loss: 1.579 | Acc: 42.204% (17746/42048)\n",
      "Loss: 1.579 | Acc: 42.202% (17772/42112)\n",
      "Loss: 1.579 | Acc: 42.204% (17800/42176)\n",
      "Loss: 1.579 | Acc: 42.192% (17822/42240)\n",
      "Loss: 1.579 | Acc: 42.190% (17848/42304)\n",
      "Loss: 1.579 | Acc: 42.192% (17876/42368)\n",
      "Loss: 1.580 | Acc: 42.178% (17897/42432)\n",
      "Loss: 1.580 | Acc: 42.180% (17925/42496)\n",
      "Loss: 1.579 | Acc: 42.180% (17952/42560)\n",
      "Loss: 1.579 | Acc: 42.183% (17980/42624)\n",
      "Loss: 1.579 | Acc: 42.185% (18008/42688)\n",
      "Loss: 1.579 | Acc: 42.190% (18037/42752)\n",
      "Loss: 1.579 | Acc: 42.195% (18066/42816)\n",
      "Loss: 1.579 | Acc: 42.188% (18090/42880)\n",
      "Loss: 1.579 | Acc: 42.192% (18119/42944)\n",
      "Loss: 1.579 | Acc: 42.206% (18152/43008)\n",
      "Loss: 1.579 | Acc: 42.201% (18177/43072)\n",
      "Loss: 1.579 | Acc: 42.194% (18201/43136)\n",
      "Loss: 1.579 | Acc: 42.208% (18234/43200)\n",
      "Loss: 1.579 | Acc: 42.204% (18259/43264)\n",
      "Loss: 1.579 | Acc: 42.199% (18284/43328)\n",
      "Loss: 1.579 | Acc: 42.208% (18315/43392)\n",
      "Loss: 1.579 | Acc: 42.227% (18350/43456)\n",
      "Loss: 1.579 | Acc: 42.229% (18378/43520)\n",
      "Loss: 1.579 | Acc: 42.243% (18411/43584)\n",
      "Loss: 1.579 | Acc: 42.259% (18445/43648)\n",
      "Loss: 1.579 | Acc: 42.265% (18475/43712)\n",
      "Loss: 1.579 | Acc: 42.258% (18499/43776)\n",
      "Loss: 1.579 | Acc: 42.256% (18525/43840)\n",
      "Loss: 1.578 | Acc: 42.267% (18557/43904)\n",
      "Loss: 1.578 | Acc: 42.274% (18587/43968)\n",
      "Loss: 1.578 | Acc: 42.285% (18619/44032)\n",
      "Loss: 1.578 | Acc: 42.280% (18644/44096)\n",
      "Loss: 1.578 | Acc: 42.276% (18669/44160)\n",
      "Loss: 1.578 | Acc: 42.276% (18696/44224)\n",
      "Loss: 1.578 | Acc: 42.280% (18725/44288)\n",
      "Loss: 1.578 | Acc: 42.282% (18753/44352)\n",
      "Loss: 1.578 | Acc: 42.293% (18785/44416)\n",
      "Loss: 1.578 | Acc: 42.289% (18810/44480)\n",
      "Loss: 1.578 | Acc: 42.304% (18844/44544)\n",
      "Loss: 1.577 | Acc: 42.315% (18876/44608)\n",
      "Loss: 1.577 | Acc: 42.324% (18907/44672)\n",
      "Loss: 1.577 | Acc: 42.337% (18940/44736)\n",
      "Loss: 1.577 | Acc: 42.337% (18967/44800)\n",
      "Loss: 1.577 | Acc: 42.337% (18994/44864)\n",
      "Loss: 1.577 | Acc: 42.334% (19020/44928)\n",
      "Loss: 1.577 | Acc: 42.341% (19050/44992)\n",
      "Loss: 1.577 | Acc: 42.323% (19069/45056)\n",
      "Loss: 1.577 | Acc: 42.329% (19099/45120)\n",
      "Loss: 1.578 | Acc: 42.323% (19123/45184)\n",
      "Loss: 1.577 | Acc: 42.336% (19156/45248)\n",
      "Loss: 1.577 | Acc: 42.338% (19184/45312)\n",
      "Loss: 1.577 | Acc: 42.337% (19211/45376)\n",
      "Loss: 1.577 | Acc: 42.333% (19236/45440)\n",
      "Loss: 1.577 | Acc: 42.341% (19267/45504)\n",
      "Loss: 1.577 | Acc: 42.332% (19290/45568)\n",
      "Loss: 1.577 | Acc: 42.330% (19316/45632)\n",
      "Loss: 1.577 | Acc: 42.330% (19343/45696)\n",
      "Loss: 1.577 | Acc: 42.330% (19370/45760)\n",
      "Loss: 1.577 | Acc: 42.338% (19401/45824)\n",
      "Loss: 1.577 | Acc: 42.355% (19436/45888)\n",
      "Loss: 1.577 | Acc: 42.351% (19461/45952)\n",
      "Loss: 1.577 | Acc: 42.348% (19487/46016)\n",
      "Loss: 1.577 | Acc: 42.352% (19516/46080)\n",
      "Loss: 1.577 | Acc: 42.352% (19543/46144)\n",
      "Loss: 1.577 | Acc: 42.371% (19579/46208)\n",
      "Loss: 1.576 | Acc: 42.376% (19608/46272)\n",
      "Loss: 1.576 | Acc: 42.369% (19632/46336)\n",
      "Loss: 1.577 | Acc: 42.362% (19656/46400)\n",
      "Loss: 1.577 | Acc: 42.366% (19685/46464)\n",
      "Loss: 1.577 | Acc: 42.357% (19708/46528)\n",
      "Loss: 1.577 | Acc: 42.351% (19732/46592)\n",
      "Loss: 1.577 | Acc: 42.355% (19761/46656)\n",
      "Loss: 1.577 | Acc: 42.352% (19787/46720)\n",
      "Loss: 1.577 | Acc: 42.350% (19813/46784)\n",
      "Loss: 1.577 | Acc: 42.354% (19842/46848)\n",
      "Loss: 1.576 | Acc: 42.364% (19874/46912)\n",
      "Loss: 1.576 | Acc: 42.364% (19901/46976)\n",
      "Loss: 1.577 | Acc: 42.353% (19923/47040)\n",
      "Loss: 1.577 | Acc: 42.357% (19952/47104)\n",
      "Loss: 1.577 | Acc: 42.347% (19974/47168)\n",
      "Loss: 1.576 | Acc: 42.359% (20007/47232)\n",
      "Loss: 1.576 | Acc: 42.359% (20034/47296)\n",
      "Loss: 1.576 | Acc: 42.350% (20057/47360)\n",
      "Loss: 1.577 | Acc: 42.339% (20079/47424)\n",
      "Loss: 1.577 | Acc: 42.348% (20110/47488)\n",
      "Loss: 1.577 | Acc: 42.341% (20134/47552)\n",
      "Loss: 1.577 | Acc: 42.343% (20162/47616)\n",
      "Loss: 1.577 | Acc: 42.351% (20193/47680)\n",
      "Loss: 1.577 | Acc: 42.361% (20225/47744)\n",
      "Loss: 1.577 | Acc: 42.355% (20249/47808)\n",
      "Loss: 1.577 | Acc: 42.342% (20270/47872)\n",
      "Loss: 1.577 | Acc: 42.338% (20295/47936)\n",
      "Loss: 1.577 | Acc: 42.342% (20324/48000)\n",
      "Loss: 1.578 | Acc: 42.331% (20346/48064)\n",
      "Loss: 1.578 | Acc: 42.320% (20368/48128)\n",
      "Loss: 1.578 | Acc: 42.312% (20391/48192)\n",
      "Loss: 1.578 | Acc: 42.324% (20424/48256)\n",
      "Loss: 1.578 | Acc: 42.334% (20456/48320)\n",
      "Loss: 1.578 | Acc: 42.332% (20482/48384)\n",
      "Loss: 1.577 | Acc: 42.340% (20513/48448)\n",
      "Loss: 1.577 | Acc: 42.340% (20540/48512)\n",
      "Loss: 1.578 | Acc: 42.342% (20568/48576)\n",
      "Loss: 1.578 | Acc: 42.350% (20599/48640)\n",
      "Loss: 1.578 | Acc: 42.350% (20626/48704)\n",
      "Loss: 1.578 | Acc: 42.343% (20650/48768)\n",
      "Loss: 1.578 | Acc: 42.345% (20678/48832)\n",
      "Loss: 1.578 | Acc: 42.355% (20710/48896)\n",
      "Loss: 1.578 | Acc: 42.353% (20736/48960)\n",
      "Loss: 1.577 | Acc: 42.357% (20755/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 42.357142857142854\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.478 | Acc: 42.188% (27/64)\n",
      "Loss: 1.462 | Acc: 42.188% (54/128)\n",
      "Loss: 1.545 | Acc: 40.625% (78/192)\n",
      "Loss: 1.597 | Acc: 39.062% (100/256)\n",
      "Loss: 1.584 | Acc: 39.688% (127/320)\n",
      "Loss: 1.610 | Acc: 39.062% (150/384)\n",
      "Loss: 1.621 | Acc: 38.393% (172/448)\n",
      "Loss: 1.621 | Acc: 38.477% (197/512)\n",
      "Loss: 1.620 | Acc: 38.194% (220/576)\n",
      "Loss: 1.608 | Acc: 39.219% (251/640)\n",
      "Loss: 1.617 | Acc: 38.210% (269/704)\n",
      "Loss: 1.630 | Acc: 38.281% (294/768)\n",
      "Loss: 1.635 | Acc: 38.582% (321/832)\n",
      "Loss: 1.630 | Acc: 38.839% (348/896)\n",
      "Loss: 1.616 | Acc: 39.896% (383/960)\n",
      "Loss: 1.609 | Acc: 40.527% (415/1024)\n",
      "Loss: 1.610 | Acc: 40.533% (441/1088)\n",
      "Loss: 1.601 | Acc: 40.712% (469/1152)\n",
      "Loss: 1.591 | Acc: 40.954% (498/1216)\n",
      "Loss: 1.603 | Acc: 40.312% (516/1280)\n",
      "Loss: 1.602 | Acc: 40.030% (538/1344)\n",
      "Loss: 1.597 | Acc: 40.199% (566/1408)\n",
      "Loss: 1.587 | Acc: 40.557% (597/1472)\n",
      "Loss: 1.594 | Acc: 40.169% (617/1536)\n",
      "Loss: 1.590 | Acc: 40.250% (644/1600)\n",
      "Loss: 1.592 | Acc: 40.084% (667/1664)\n",
      "Loss: 1.594 | Acc: 40.278% (696/1728)\n",
      "Loss: 1.598 | Acc: 40.346% (723/1792)\n",
      "Loss: 1.597 | Acc: 40.409% (750/1856)\n",
      "Loss: 1.596 | Acc: 40.521% (778/1920)\n",
      "Loss: 1.591 | Acc: 40.675% (807/1984)\n",
      "Loss: 1.594 | Acc: 40.674% (833/2048)\n",
      "Loss: 1.592 | Acc: 40.720% (860/2112)\n",
      "Loss: 1.593 | Acc: 40.671% (885/2176)\n",
      "Loss: 1.596 | Acc: 40.536% (908/2240)\n",
      "Loss: 1.596 | Acc: 40.538% (934/2304)\n",
      "Loss: 1.598 | Acc: 40.667% (963/2368)\n",
      "Loss: 1.597 | Acc: 40.625% (988/2432)\n",
      "Loss: 1.598 | Acc: 40.665% (1015/2496)\n",
      "Loss: 1.601 | Acc: 40.625% (1040/2560)\n",
      "Loss: 1.601 | Acc: 40.587% (1065/2624)\n",
      "Loss: 1.602 | Acc: 40.402% (1086/2688)\n",
      "Loss: 1.604 | Acc: 40.407% (1112/2752)\n",
      "Loss: 1.604 | Acc: 40.554% (1142/2816)\n",
      "Loss: 1.603 | Acc: 40.625% (1170/2880)\n",
      "Loss: 1.603 | Acc: 40.523% (1193/2944)\n",
      "Loss: 1.600 | Acc: 40.725% (1225/3008)\n",
      "Loss: 1.601 | Acc: 40.625% (1248/3072)\n",
      "Loss: 1.601 | Acc: 40.816% (1280/3136)\n",
      "Loss: 1.599 | Acc: 40.906% (1309/3200)\n",
      "Loss: 1.598 | Acc: 40.870% (1334/3264)\n",
      "Loss: 1.599 | Acc: 40.895% (1361/3328)\n",
      "Loss: 1.596 | Acc: 41.067% (1393/3392)\n",
      "Loss: 1.597 | Acc: 41.059% (1419/3456)\n",
      "Loss: 1.598 | Acc: 41.051% (1445/3520)\n",
      "Loss: 1.594 | Acc: 41.183% (1476/3584)\n",
      "Loss: 1.595 | Acc: 41.146% (1501/3648)\n",
      "Loss: 1.593 | Acc: 41.083% (1525/3712)\n",
      "Loss: 1.594 | Acc: 41.022% (1549/3776)\n",
      "Loss: 1.594 | Acc: 40.938% (1572/3840)\n",
      "Loss: 1.592 | Acc: 41.086% (1604/3904)\n",
      "Loss: 1.590 | Acc: 41.179% (1634/3968)\n",
      "Loss: 1.592 | Acc: 41.171% (1660/4032)\n",
      "Loss: 1.593 | Acc: 41.089% (1683/4096)\n",
      "Loss: 1.597 | Acc: 41.082% (1709/4160)\n",
      "Loss: 1.596 | Acc: 41.051% (1734/4224)\n",
      "Loss: 1.597 | Acc: 40.975% (1757/4288)\n",
      "Loss: 1.596 | Acc: 41.108% (1789/4352)\n",
      "Loss: 1.595 | Acc: 41.236% (1821/4416)\n",
      "Loss: 1.595 | Acc: 41.205% (1846/4480)\n",
      "Loss: 1.594 | Acc: 41.153% (1870/4544)\n",
      "Loss: 1.597 | Acc: 41.016% (1890/4608)\n",
      "Loss: 1.596 | Acc: 41.010% (1916/4672)\n",
      "Loss: 1.594 | Acc: 41.026% (1943/4736)\n",
      "Loss: 1.595 | Acc: 40.938% (1965/4800)\n",
      "Loss: 1.594 | Acc: 40.933% (1991/4864)\n",
      "Loss: 1.592 | Acc: 41.011% (2021/4928)\n",
      "Loss: 1.594 | Acc: 41.006% (2047/4992)\n",
      "Loss: 1.592 | Acc: 41.060% (2076/5056)\n",
      "Loss: 1.593 | Acc: 41.016% (2100/5120)\n",
      "Loss: 1.592 | Acc: 41.146% (2133/5184)\n",
      "Loss: 1.592 | Acc: 41.159% (2160/5248)\n",
      "Loss: 1.592 | Acc: 41.114% (2184/5312)\n",
      "Loss: 1.593 | Acc: 41.090% (2209/5376)\n",
      "Loss: 1.593 | Acc: 41.103% (2236/5440)\n",
      "Loss: 1.594 | Acc: 41.025% (2258/5504)\n",
      "Loss: 1.597 | Acc: 40.966% (2281/5568)\n",
      "Loss: 1.598 | Acc: 40.980% (2308/5632)\n",
      "Loss: 1.597 | Acc: 40.959% (2333/5696)\n",
      "Loss: 1.594 | Acc: 41.042% (2364/5760)\n",
      "Loss: 1.593 | Acc: 41.123% (2395/5824)\n",
      "Loss: 1.595 | Acc: 40.999% (2414/5888)\n",
      "Loss: 1.596 | Acc: 40.995% (2440/5952)\n",
      "Loss: 1.594 | Acc: 41.024% (2468/6016)\n",
      "Loss: 1.594 | Acc: 41.003% (2493/6080)\n",
      "Loss: 1.595 | Acc: 40.934% (2515/6144)\n",
      "Loss: 1.597 | Acc: 40.834% (2535/6208)\n",
      "Loss: 1.597 | Acc: 40.880% (2564/6272)\n",
      "Loss: 1.595 | Acc: 40.972% (2596/6336)\n",
      "Loss: 1.596 | Acc: 40.969% (2622/6400)\n",
      "Loss: 1.598 | Acc: 40.934% (2646/6464)\n",
      "Loss: 1.598 | Acc: 40.962% (2674/6528)\n",
      "Loss: 1.599 | Acc: 40.928% (2698/6592)\n",
      "Loss: 1.599 | Acc: 40.941% (2725/6656)\n",
      "Loss: 1.599 | Acc: 40.923% (2750/6720)\n",
      "Loss: 1.600 | Acc: 40.964% (2779/6784)\n",
      "Loss: 1.598 | Acc: 41.034% (2810/6848)\n",
      "Loss: 1.599 | Acc: 41.001% (2834/6912)\n",
      "Loss: 1.600 | Acc: 40.955% (2857/6976)\n",
      "Loss: 1.602 | Acc: 40.810% (2873/7040)\n",
      "Loss: 1.601 | Acc: 40.836% (2901/7104)\n",
      "Loss: 1.602 | Acc: 40.820% (2926/7168)\n",
      "Loss: 1.604 | Acc: 40.708% (2944/7232)\n",
      "Loss: 1.604 | Acc: 40.598% (2962/7296)\n",
      "Loss: 1.603 | Acc: 40.625% (2990/7360)\n",
      "Loss: 1.603 | Acc: 40.598% (3014/7424)\n",
      "Loss: 1.603 | Acc: 40.638% (3043/7488)\n",
      "Loss: 1.602 | Acc: 40.731% (3076/7552)\n",
      "Loss: 1.602 | Acc: 40.704% (3100/7616)\n",
      "Loss: 1.602 | Acc: 40.690% (3125/7680)\n",
      "Loss: 1.601 | Acc: 40.715% (3153/7744)\n",
      "Loss: 1.600 | Acc: 40.791% (3185/7808)\n",
      "Loss: 1.601 | Acc: 40.714% (3205/7872)\n",
      "Loss: 1.601 | Acc: 40.688% (3229/7936)\n",
      "Loss: 1.602 | Acc: 40.638% (3251/8000)\n",
      "Loss: 1.602 | Acc: 40.650% (3278/8064)\n",
      "Loss: 1.601 | Acc: 40.699% (3308/8128)\n",
      "Loss: 1.600 | Acc: 40.735% (3337/8192)\n",
      "Loss: 1.601 | Acc: 40.686% (3359/8256)\n",
      "Loss: 1.603 | Acc: 40.613% (3379/8320)\n",
      "Loss: 1.602 | Acc: 40.685% (3411/8384)\n",
      "Loss: 1.602 | Acc: 40.649% (3434/8448)\n",
      "Loss: 1.602 | Acc: 40.625% (3458/8512)\n",
      "Loss: 1.602 | Acc: 40.730% (3493/8576)\n",
      "Loss: 1.602 | Acc: 40.718% (3518/8640)\n",
      "Loss: 1.603 | Acc: 40.763% (3548/8704)\n",
      "Loss: 1.603 | Acc: 40.728% (3571/8768)\n",
      "Loss: 1.603 | Acc: 40.727% (3597/8832)\n",
      "Loss: 1.602 | Acc: 40.782% (3628/8896)\n",
      "Loss: 1.601 | Acc: 40.859% (3661/8960)\n",
      "Loss: 1.601 | Acc: 40.913% (3692/9024)\n",
      "Loss: 1.601 | Acc: 40.900% (3717/9088)\n",
      "Loss: 1.602 | Acc: 40.931% (3746/9152)\n",
      "Loss: 1.600 | Acc: 40.994% (3778/9216)\n",
      "Loss: 1.600 | Acc: 40.991% (3804/9280)\n",
      "Loss: 1.600 | Acc: 41.032% (3834/9344)\n",
      "Loss: 1.600 | Acc: 41.008% (3858/9408)\n",
      "Loss: 1.601 | Acc: 40.973% (3881/9472)\n",
      "Loss: 1.601 | Acc: 40.982% (3908/9536)\n",
      "Loss: 1.600 | Acc: 41.031% (3939/9600)\n",
      "Loss: 1.599 | Acc: 41.060% (3968/9664)\n",
      "Loss: 1.599 | Acc: 41.088% (3997/9728)\n",
      "Loss: 1.600 | Acc: 41.064% (4021/9792)\n",
      "Loss: 1.599 | Acc: 41.061% (4047/9856)\n",
      "Loss: 1.600 | Acc: 41.028% (4070/9920)\n",
      "Loss: 1.599 | Acc: 41.026% (4096/9984)\n",
      "Loss: 1.599 | Acc: 41.030% (4103/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 41.03\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.472 | Acc: 50.000% (32/64)\n",
      "Loss: 1.500 | Acc: 48.438% (62/128)\n",
      "Loss: 1.586 | Acc: 46.875% (90/192)\n",
      "Loss: 1.621 | Acc: 43.359% (111/256)\n",
      "Loss: 1.583 | Acc: 45.000% (144/320)\n",
      "Loss: 1.599 | Acc: 43.750% (168/384)\n",
      "Loss: 1.568 | Acc: 43.750% (196/448)\n",
      "Loss: 1.559 | Acc: 44.531% (228/512)\n",
      "Loss: 1.586 | Acc: 42.882% (247/576)\n",
      "Loss: 1.591 | Acc: 42.656% (273/640)\n",
      "Loss: 1.601 | Acc: 42.330% (298/704)\n",
      "Loss: 1.596 | Acc: 42.969% (330/768)\n",
      "Loss: 1.604 | Acc: 42.308% (352/832)\n",
      "Loss: 1.591 | Acc: 42.522% (381/896)\n",
      "Loss: 1.595 | Acc: 42.812% (411/960)\n",
      "Loss: 1.597 | Acc: 42.773% (438/1024)\n",
      "Loss: 1.593 | Acc: 43.107% (469/1088)\n",
      "Loss: 1.596 | Acc: 42.795% (493/1152)\n",
      "Loss: 1.597 | Acc: 42.763% (520/1216)\n",
      "Loss: 1.602 | Acc: 42.500% (544/1280)\n",
      "Loss: 1.592 | Acc: 42.485% (571/1344)\n",
      "Loss: 1.582 | Acc: 42.756% (602/1408)\n",
      "Loss: 1.587 | Acc: 42.731% (629/1472)\n",
      "Loss: 1.577 | Acc: 43.229% (664/1536)\n",
      "Loss: 1.576 | Acc: 43.250% (692/1600)\n",
      "Loss: 1.581 | Acc: 43.029% (716/1664)\n",
      "Loss: 1.575 | Acc: 43.171% (746/1728)\n",
      "Loss: 1.586 | Acc: 42.746% (766/1792)\n",
      "Loss: 1.589 | Acc: 42.672% (792/1856)\n",
      "Loss: 1.582 | Acc: 42.969% (825/1920)\n",
      "Loss: 1.584 | Acc: 43.246% (858/1984)\n",
      "Loss: 1.581 | Acc: 43.115% (883/2048)\n",
      "Loss: 1.580 | Acc: 43.134% (911/2112)\n",
      "Loss: 1.581 | Acc: 43.015% (936/2176)\n",
      "Loss: 1.582 | Acc: 42.946% (962/2240)\n",
      "Loss: 1.589 | Acc: 42.708% (984/2304)\n",
      "Loss: 1.591 | Acc: 42.610% (1009/2368)\n",
      "Loss: 1.590 | Acc: 42.640% (1037/2432)\n",
      "Loss: 1.587 | Acc: 42.748% (1067/2496)\n",
      "Loss: 1.583 | Acc: 42.695% (1093/2560)\n",
      "Loss: 1.585 | Acc: 42.645% (1119/2624)\n",
      "Loss: 1.588 | Acc: 42.634% (1146/2688)\n",
      "Loss: 1.582 | Acc: 42.733% (1176/2752)\n",
      "Loss: 1.579 | Acc: 42.862% (1207/2816)\n",
      "Loss: 1.584 | Acc: 42.604% (1227/2880)\n",
      "Loss: 1.584 | Acc: 42.663% (1256/2944)\n",
      "Loss: 1.584 | Acc: 42.520% (1279/3008)\n",
      "Loss: 1.581 | Acc: 42.676% (1311/3072)\n",
      "Loss: 1.579 | Acc: 42.666% (1338/3136)\n",
      "Loss: 1.579 | Acc: 42.531% (1361/3200)\n",
      "Loss: 1.579 | Acc: 42.463% (1386/3264)\n",
      "Loss: 1.575 | Acc: 42.698% (1421/3328)\n",
      "Loss: 1.571 | Acc: 42.748% (1450/3392)\n",
      "Loss: 1.572 | Acc: 42.708% (1476/3456)\n",
      "Loss: 1.575 | Acc: 42.727% (1504/3520)\n",
      "Loss: 1.576 | Acc: 42.578% (1526/3584)\n",
      "Loss: 1.576 | Acc: 42.544% (1552/3648)\n",
      "Loss: 1.574 | Acc: 42.672% (1584/3712)\n",
      "Loss: 1.575 | Acc: 42.691% (1612/3776)\n",
      "Loss: 1.575 | Acc: 42.760% (1642/3840)\n",
      "Loss: 1.577 | Acc: 42.777% (1670/3904)\n",
      "Loss: 1.578 | Acc: 42.692% (1694/3968)\n",
      "Loss: 1.576 | Acc: 42.832% (1727/4032)\n",
      "Loss: 1.577 | Acc: 42.798% (1753/4096)\n",
      "Loss: 1.577 | Acc: 42.788% (1780/4160)\n",
      "Loss: 1.578 | Acc: 42.779% (1807/4224)\n",
      "Loss: 1.578 | Acc: 42.771% (1834/4288)\n",
      "Loss: 1.573 | Acc: 43.015% (1872/4352)\n",
      "Loss: 1.576 | Acc: 42.957% (1897/4416)\n",
      "Loss: 1.576 | Acc: 42.991% (1926/4480)\n",
      "Loss: 1.574 | Acc: 43.024% (1955/4544)\n",
      "Loss: 1.573 | Acc: 42.904% (1977/4608)\n",
      "Loss: 1.571 | Acc: 43.044% (2011/4672)\n",
      "Loss: 1.571 | Acc: 43.074% (2040/4736)\n",
      "Loss: 1.569 | Acc: 43.146% (2071/4800)\n",
      "Loss: 1.567 | Acc: 43.318% (2107/4864)\n",
      "Loss: 1.568 | Acc: 43.304% (2134/4928)\n",
      "Loss: 1.566 | Acc: 43.369% (2165/4992)\n",
      "Loss: 1.564 | Acc: 43.493% (2199/5056)\n",
      "Loss: 1.564 | Acc: 43.457% (2225/5120)\n",
      "Loss: 1.564 | Acc: 43.480% (2254/5184)\n",
      "Loss: 1.565 | Acc: 43.483% (2282/5248)\n",
      "Loss: 1.564 | Acc: 43.505% (2311/5312)\n",
      "Loss: 1.563 | Acc: 43.620% (2345/5376)\n",
      "Loss: 1.564 | Acc: 43.493% (2366/5440)\n",
      "Loss: 1.562 | Acc: 43.586% (2399/5504)\n",
      "Loss: 1.559 | Acc: 43.678% (2432/5568)\n",
      "Loss: 1.559 | Acc: 43.697% (2461/5632)\n",
      "Loss: 1.561 | Acc: 43.680% (2488/5696)\n",
      "Loss: 1.559 | Acc: 43.750% (2520/5760)\n",
      "Loss: 1.557 | Acc: 43.819% (2552/5824)\n",
      "Loss: 1.557 | Acc: 43.767% (2577/5888)\n",
      "Loss: 1.556 | Acc: 43.834% (2609/5952)\n",
      "Loss: 1.553 | Acc: 43.916% (2642/6016)\n",
      "Loss: 1.553 | Acc: 43.947% (2672/6080)\n",
      "Loss: 1.552 | Acc: 43.994% (2703/6144)\n",
      "Loss: 1.551 | Acc: 43.992% (2731/6208)\n",
      "Loss: 1.550 | Acc: 44.085% (2765/6272)\n",
      "Loss: 1.549 | Acc: 44.050% (2791/6336)\n",
      "Loss: 1.548 | Acc: 44.062% (2820/6400)\n",
      "Loss: 1.547 | Acc: 44.106% (2851/6464)\n",
      "Loss: 1.548 | Acc: 44.072% (2877/6528)\n",
      "Loss: 1.549 | Acc: 44.069% (2905/6592)\n",
      "Loss: 1.549 | Acc: 44.050% (2932/6656)\n",
      "Loss: 1.549 | Acc: 44.062% (2961/6720)\n",
      "Loss: 1.549 | Acc: 44.104% (2992/6784)\n",
      "Loss: 1.548 | Acc: 44.100% (3020/6848)\n",
      "Loss: 1.551 | Acc: 43.996% (3041/6912)\n",
      "Loss: 1.549 | Acc: 44.080% (3075/6976)\n",
      "Loss: 1.550 | Acc: 44.062% (3102/7040)\n",
      "Loss: 1.550 | Acc: 43.989% (3125/7104)\n",
      "Loss: 1.547 | Acc: 44.099% (3161/7168)\n",
      "Loss: 1.549 | Acc: 44.013% (3183/7232)\n",
      "Loss: 1.549 | Acc: 43.956% (3207/7296)\n",
      "Loss: 1.549 | Acc: 43.913% (3232/7360)\n",
      "Loss: 1.547 | Acc: 44.046% (3270/7424)\n",
      "Loss: 1.549 | Acc: 44.004% (3295/7488)\n",
      "Loss: 1.550 | Acc: 43.988% (3322/7552)\n",
      "Loss: 1.549 | Acc: 44.105% (3359/7616)\n",
      "Loss: 1.548 | Acc: 44.076% (3385/7680)\n",
      "Loss: 1.548 | Acc: 44.073% (3413/7744)\n",
      "Loss: 1.546 | Acc: 44.211% (3452/7808)\n",
      "Loss: 1.546 | Acc: 44.245% (3483/7872)\n",
      "Loss: 1.546 | Acc: 44.178% (3506/7936)\n",
      "Loss: 1.545 | Acc: 44.225% (3538/8000)\n",
      "Loss: 1.544 | Acc: 44.345% (3576/8064)\n",
      "Loss: 1.544 | Acc: 44.341% (3604/8128)\n",
      "Loss: 1.543 | Acc: 44.397% (3637/8192)\n",
      "Loss: 1.542 | Acc: 44.513% (3675/8256)\n",
      "Loss: 1.544 | Acc: 44.459% (3699/8320)\n",
      "Loss: 1.544 | Acc: 44.442% (3726/8384)\n",
      "Loss: 1.542 | Acc: 44.448% (3755/8448)\n",
      "Loss: 1.541 | Acc: 44.478% (3786/8512)\n",
      "Loss: 1.542 | Acc: 44.438% (3811/8576)\n",
      "Loss: 1.542 | Acc: 44.456% (3841/8640)\n",
      "Loss: 1.542 | Acc: 44.462% (3870/8704)\n",
      "Loss: 1.543 | Acc: 44.480% (3900/8768)\n",
      "Loss: 1.545 | Acc: 44.407% (3922/8832)\n",
      "Loss: 1.546 | Acc: 44.413% (3951/8896)\n",
      "Loss: 1.546 | Acc: 44.408% (3979/8960)\n",
      "Loss: 1.545 | Acc: 44.470% (4013/9024)\n",
      "Loss: 1.546 | Acc: 44.454% (4040/9088)\n",
      "Loss: 1.546 | Acc: 44.460% (4069/9152)\n",
      "Loss: 1.545 | Acc: 44.455% (4097/9216)\n",
      "Loss: 1.545 | Acc: 44.429% (4123/9280)\n",
      "Loss: 1.545 | Acc: 44.478% (4156/9344)\n",
      "Loss: 1.545 | Acc: 44.494% (4186/9408)\n",
      "Loss: 1.545 | Acc: 44.478% (4213/9472)\n",
      "Loss: 1.544 | Acc: 44.526% (4246/9536)\n",
      "Loss: 1.544 | Acc: 44.510% (4273/9600)\n",
      "Loss: 1.546 | Acc: 44.412% (4292/9664)\n",
      "Loss: 1.545 | Acc: 44.418% (4321/9728)\n",
      "Loss: 1.546 | Acc: 44.383% (4346/9792)\n",
      "Loss: 1.545 | Acc: 44.399% (4376/9856)\n",
      "Loss: 1.544 | Acc: 44.405% (4405/9920)\n",
      "Loss: 1.545 | Acc: 44.371% (4430/9984)\n",
      "Loss: 1.544 | Acc: 44.397% (4461/10048)\n",
      "Loss: 1.545 | Acc: 44.393% (4489/10112)\n",
      "Loss: 1.544 | Acc: 44.438% (4522/10176)\n",
      "Loss: 1.543 | Acc: 44.463% (4553/10240)\n",
      "Loss: 1.543 | Acc: 44.458% (4581/10304)\n",
      "Loss: 1.542 | Acc: 44.502% (4614/10368)\n",
      "Loss: 1.544 | Acc: 44.450% (4637/10432)\n",
      "Loss: 1.545 | Acc: 44.407% (4661/10496)\n",
      "Loss: 1.544 | Acc: 44.413% (4690/10560)\n",
      "Loss: 1.546 | Acc: 44.371% (4714/10624)\n",
      "Loss: 1.545 | Acc: 44.414% (4747/10688)\n",
      "Loss: 1.545 | Acc: 44.373% (4771/10752)\n",
      "Loss: 1.546 | Acc: 44.351% (4797/10816)\n",
      "Loss: 1.547 | Acc: 44.329% (4823/10880)\n",
      "Loss: 1.546 | Acc: 44.344% (4853/10944)\n",
      "Loss: 1.546 | Acc: 44.331% (4880/11008)\n",
      "Loss: 1.546 | Acc: 44.328% (4908/11072)\n",
      "Loss: 1.547 | Acc: 44.280% (4931/11136)\n",
      "Loss: 1.546 | Acc: 44.259% (4957/11200)\n",
      "Loss: 1.546 | Acc: 44.274% (4987/11264)\n",
      "Loss: 1.546 | Acc: 44.288% (5017/11328)\n",
      "Loss: 1.547 | Acc: 44.242% (5040/11392)\n",
      "Loss: 1.546 | Acc: 44.300% (5075/11456)\n",
      "Loss: 1.547 | Acc: 44.227% (5095/11520)\n",
      "Loss: 1.548 | Acc: 44.182% (5118/11584)\n",
      "Loss: 1.546 | Acc: 44.265% (5156/11648)\n",
      "Loss: 1.546 | Acc: 44.245% (5182/11712)\n",
      "Loss: 1.546 | Acc: 44.226% (5208/11776)\n",
      "Loss: 1.547 | Acc: 44.223% (5236/11840)\n",
      "Loss: 1.546 | Acc: 44.254% (5268/11904)\n",
      "Loss: 1.546 | Acc: 44.276% (5299/11968)\n",
      "Loss: 1.547 | Acc: 44.240% (5323/12032)\n",
      "Loss: 1.547 | Acc: 44.254% (5353/12096)\n",
      "Loss: 1.546 | Acc: 44.317% (5389/12160)\n",
      "Loss: 1.547 | Acc: 44.274% (5412/12224)\n",
      "Loss: 1.545 | Acc: 44.312% (5445/12288)\n",
      "Loss: 1.545 | Acc: 44.349% (5478/12352)\n",
      "Loss: 1.545 | Acc: 44.322% (5503/12416)\n",
      "Loss: 1.544 | Acc: 44.351% (5535/12480)\n",
      "Loss: 1.545 | Acc: 44.340% (5562/12544)\n",
      "Loss: 1.546 | Acc: 44.242% (5578/12608)\n",
      "Loss: 1.546 | Acc: 44.200% (5601/12672)\n",
      "Loss: 1.547 | Acc: 44.166% (5625/12736)\n",
      "Loss: 1.547 | Acc: 44.164% (5653/12800)\n",
      "Loss: 1.548 | Acc: 44.139% (5678/12864)\n",
      "Loss: 1.548 | Acc: 44.121% (5704/12928)\n",
      "Loss: 1.547 | Acc: 44.181% (5740/12992)\n",
      "Loss: 1.547 | Acc: 44.156% (5765/13056)\n",
      "Loss: 1.547 | Acc: 44.116% (5788/13120)\n",
      "Loss: 1.547 | Acc: 44.137% (5819/13184)\n",
      "Loss: 1.548 | Acc: 44.120% (5845/13248)\n",
      "Loss: 1.548 | Acc: 44.133% (5875/13312)\n",
      "Loss: 1.548 | Acc: 44.161% (5907/13376)\n",
      "Loss: 1.548 | Acc: 44.115% (5929/13440)\n",
      "Loss: 1.549 | Acc: 44.076% (5952/13504)\n",
      "Loss: 1.549 | Acc: 44.060% (5978/13568)\n",
      "Loss: 1.548 | Acc: 44.065% (6007/13632)\n",
      "Loss: 1.547 | Acc: 44.108% (6041/13696)\n",
      "Loss: 1.547 | Acc: 44.113% (6070/13760)\n",
      "Loss: 1.546 | Acc: 44.104% (6097/13824)\n",
      "Loss: 1.545 | Acc: 44.168% (6134/13888)\n",
      "Loss: 1.545 | Acc: 44.180% (6164/13952)\n",
      "Loss: 1.545 | Acc: 44.178% (6192/14016)\n",
      "Loss: 1.546 | Acc: 44.148% (6216/14080)\n",
      "Loss: 1.546 | Acc: 44.139% (6243/14144)\n",
      "Loss: 1.546 | Acc: 44.137% (6271/14208)\n",
      "Loss: 1.545 | Acc: 44.198% (6308/14272)\n",
      "Loss: 1.546 | Acc: 44.189% (6335/14336)\n",
      "Loss: 1.547 | Acc: 44.139% (6356/14400)\n",
      "Loss: 1.547 | Acc: 44.116% (6381/14464)\n",
      "Loss: 1.547 | Acc: 44.156% (6415/14528)\n",
      "Loss: 1.548 | Acc: 44.147% (6442/14592)\n",
      "Loss: 1.548 | Acc: 44.118% (6466/14656)\n",
      "Loss: 1.548 | Acc: 44.151% (6499/14720)\n",
      "Loss: 1.547 | Acc: 44.149% (6527/14784)\n",
      "Loss: 1.547 | Acc: 44.147% (6555/14848)\n",
      "Loss: 1.547 | Acc: 44.152% (6584/14912)\n",
      "Loss: 1.547 | Acc: 44.124% (6608/14976)\n",
      "Loss: 1.547 | Acc: 44.116% (6635/15040)\n",
      "Loss: 1.546 | Acc: 44.147% (6668/15104)\n",
      "Loss: 1.547 | Acc: 44.099% (6689/15168)\n",
      "Loss: 1.547 | Acc: 44.091% (6716/15232)\n",
      "Loss: 1.547 | Acc: 44.129% (6750/15296)\n",
      "Loss: 1.547 | Acc: 44.089% (6772/15360)\n",
      "Loss: 1.547 | Acc: 44.061% (6796/15424)\n",
      "Loss: 1.548 | Acc: 44.060% (6824/15488)\n",
      "Loss: 1.549 | Acc: 44.001% (6843/15552)\n",
      "Loss: 1.549 | Acc: 43.968% (6866/15616)\n",
      "Loss: 1.549 | Acc: 43.973% (6895/15680)\n",
      "Loss: 1.548 | Acc: 43.985% (6925/15744)\n",
      "Loss: 1.548 | Acc: 43.978% (6952/15808)\n",
      "Loss: 1.548 | Acc: 44.040% (6990/15872)\n",
      "Loss: 1.548 | Acc: 44.014% (7014/15936)\n",
      "Loss: 1.548 | Acc: 44.006% (7041/16000)\n",
      "Loss: 1.548 | Acc: 43.980% (7065/16064)\n",
      "Loss: 1.548 | Acc: 43.948% (7088/16128)\n",
      "Loss: 1.548 | Acc: 43.954% (7117/16192)\n",
      "Loss: 1.547 | Acc: 43.978% (7149/16256)\n",
      "Loss: 1.547 | Acc: 44.020% (7184/16320)\n",
      "Loss: 1.546 | Acc: 44.037% (7215/16384)\n",
      "Loss: 1.546 | Acc: 44.036% (7243/16448)\n",
      "Loss: 1.546 | Acc: 44.010% (7267/16512)\n",
      "Loss: 1.546 | Acc: 44.021% (7297/16576)\n",
      "Loss: 1.546 | Acc: 44.008% (7323/16640)\n",
      "Loss: 1.545 | Acc: 44.019% (7353/16704)\n",
      "Loss: 1.545 | Acc: 44.036% (7384/16768)\n",
      "Loss: 1.545 | Acc: 44.023% (7410/16832)\n",
      "Loss: 1.546 | Acc: 43.987% (7432/16896)\n",
      "Loss: 1.546 | Acc: 43.956% (7455/16960)\n",
      "Loss: 1.546 | Acc: 43.985% (7488/17024)\n",
      "Loss: 1.545 | Acc: 44.013% (7521/17088)\n",
      "Loss: 1.545 | Acc: 44.018% (7550/17152)\n",
      "Loss: 1.545 | Acc: 44.017% (7578/17216)\n",
      "Loss: 1.545 | Acc: 44.016% (7606/17280)\n",
      "Loss: 1.544 | Acc: 44.033% (7637/17344)\n",
      "Loss: 1.544 | Acc: 44.037% (7666/17408)\n",
      "Loss: 1.544 | Acc: 44.019% (7691/17472)\n",
      "Loss: 1.544 | Acc: 44.001% (7716/17536)\n",
      "Loss: 1.545 | Acc: 43.960% (7737/17600)\n",
      "Loss: 1.544 | Acc: 43.937% (7761/17664)\n",
      "Loss: 1.544 | Acc: 43.925% (7787/17728)\n",
      "Loss: 1.544 | Acc: 43.952% (7820/17792)\n",
      "Loss: 1.544 | Acc: 43.952% (7848/17856)\n",
      "Loss: 1.544 | Acc: 43.934% (7873/17920)\n",
      "Loss: 1.545 | Acc: 43.928% (7900/17984)\n",
      "Loss: 1.545 | Acc: 43.916% (7926/18048)\n",
      "Loss: 1.545 | Acc: 43.905% (7952/18112)\n",
      "Loss: 1.545 | Acc: 43.893% (7978/18176)\n",
      "Loss: 1.545 | Acc: 43.909% (8009/18240)\n",
      "Loss: 1.546 | Acc: 43.876% (8031/18304)\n",
      "Loss: 1.546 | Acc: 43.864% (8057/18368)\n",
      "Loss: 1.547 | Acc: 43.815% (8076/18432)\n",
      "Loss: 1.547 | Acc: 43.820% (8105/18496)\n",
      "Loss: 1.546 | Acc: 43.847% (8138/18560)\n",
      "Loss: 1.546 | Acc: 43.857% (8168/18624)\n",
      "Loss: 1.547 | Acc: 43.825% (8190/18688)\n",
      "Loss: 1.547 | Acc: 43.846% (8222/18752)\n",
      "Loss: 1.547 | Acc: 43.851% (8251/18816)\n",
      "Loss: 1.547 | Acc: 43.829% (8275/18880)\n",
      "Loss: 1.547 | Acc: 43.792% (8296/18944)\n",
      "Loss: 1.547 | Acc: 43.787% (8323/19008)\n",
      "Loss: 1.547 | Acc: 43.792% (8352/19072)\n",
      "Loss: 1.548 | Acc: 43.787% (8379/19136)\n",
      "Loss: 1.547 | Acc: 43.802% (8410/19200)\n",
      "Loss: 1.548 | Acc: 43.786% (8435/19264)\n",
      "Loss: 1.547 | Acc: 43.812% (8468/19328)\n",
      "Loss: 1.547 | Acc: 43.807% (8495/19392)\n",
      "Loss: 1.547 | Acc: 43.791% (8520/19456)\n",
      "Loss: 1.547 | Acc: 43.781% (8546/19520)\n",
      "Loss: 1.548 | Acc: 43.781% (8574/19584)\n",
      "Loss: 1.548 | Acc: 43.775% (8601/19648)\n",
      "Loss: 1.548 | Acc: 43.760% (8626/19712)\n",
      "Loss: 1.547 | Acc: 43.801% (8662/19776)\n",
      "Loss: 1.547 | Acc: 43.826% (8695/19840)\n",
      "Loss: 1.547 | Acc: 43.825% (8723/19904)\n",
      "Loss: 1.547 | Acc: 43.830% (8752/19968)\n",
      "Loss: 1.548 | Acc: 43.800% (8774/20032)\n",
      "Loss: 1.548 | Acc: 43.820% (8806/20096)\n",
      "Loss: 1.547 | Acc: 43.834% (8837/20160)\n",
      "Loss: 1.547 | Acc: 43.824% (8863/20224)\n",
      "Loss: 1.547 | Acc: 43.839% (8894/20288)\n",
      "Loss: 1.547 | Acc: 43.838% (8922/20352)\n",
      "Loss: 1.547 | Acc: 43.809% (8944/20416)\n",
      "Loss: 1.547 | Acc: 43.779% (8966/20480)\n",
      "Loss: 1.548 | Acc: 43.750% (8988/20544)\n",
      "Loss: 1.548 | Acc: 43.755% (9017/20608)\n",
      "Loss: 1.549 | Acc: 43.721% (9038/20672)\n",
      "Loss: 1.549 | Acc: 43.726% (9067/20736)\n",
      "Loss: 1.550 | Acc: 43.692% (9088/20800)\n",
      "Loss: 1.550 | Acc: 43.688% (9115/20864)\n",
      "Loss: 1.550 | Acc: 43.683% (9142/20928)\n",
      "Loss: 1.550 | Acc: 43.655% (9164/20992)\n",
      "Loss: 1.551 | Acc: 43.631% (9187/21056)\n",
      "Loss: 1.550 | Acc: 43.684% (9226/21120)\n",
      "Loss: 1.550 | Acc: 43.741% (9266/21184)\n",
      "Loss: 1.550 | Acc: 43.722% (9290/21248)\n",
      "Loss: 1.550 | Acc: 43.712% (9316/21312)\n",
      "Loss: 1.550 | Acc: 43.699% (9341/21376)\n",
      "Loss: 1.551 | Acc: 43.717% (9373/21440)\n",
      "Loss: 1.551 | Acc: 43.713% (9400/21504)\n",
      "Loss: 1.551 | Acc: 43.690% (9423/21568)\n",
      "Loss: 1.552 | Acc: 43.699% (9453/21632)\n",
      "Loss: 1.552 | Acc: 43.699% (9481/21696)\n",
      "Loss: 1.552 | Acc: 43.686% (9506/21760)\n",
      "Loss: 1.552 | Acc: 43.654% (9527/21824)\n",
      "Loss: 1.553 | Acc: 43.631% (9550/21888)\n",
      "Loss: 1.553 | Acc: 43.627% (9577/21952)\n",
      "Loss: 1.553 | Acc: 43.605% (9600/22016)\n",
      "Loss: 1.552 | Acc: 43.601% (9627/22080)\n",
      "Loss: 1.553 | Acc: 43.578% (9650/22144)\n",
      "Loss: 1.552 | Acc: 43.615% (9686/22208)\n",
      "Loss: 1.552 | Acc: 43.624% (9716/22272)\n",
      "Loss: 1.552 | Acc: 43.620% (9743/22336)\n",
      "Loss: 1.553 | Acc: 43.616% (9770/22400)\n",
      "Loss: 1.553 | Acc: 43.616% (9798/22464)\n",
      "Loss: 1.553 | Acc: 43.608% (9824/22528)\n",
      "Loss: 1.554 | Acc: 43.569% (9843/22592)\n",
      "Loss: 1.555 | Acc: 43.560% (9869/22656)\n",
      "Loss: 1.555 | Acc: 43.561% (9897/22720)\n",
      "Loss: 1.556 | Acc: 43.522% (9916/22784)\n",
      "Loss: 1.555 | Acc: 43.549% (9950/22848)\n",
      "Loss: 1.555 | Acc: 43.532% (9974/22912)\n",
      "Loss: 1.555 | Acc: 43.519% (9999/22976)\n",
      "Loss: 1.555 | Acc: 43.524% (10028/23040)\n",
      "Loss: 1.555 | Acc: 43.516% (10054/23104)\n",
      "Loss: 1.555 | Acc: 43.513% (10081/23168)\n",
      "Loss: 1.555 | Acc: 43.526% (10112/23232)\n",
      "Loss: 1.555 | Acc: 43.514% (10137/23296)\n",
      "Loss: 1.555 | Acc: 43.515% (10165/23360)\n",
      "Loss: 1.555 | Acc: 43.519% (10194/23424)\n",
      "Loss: 1.555 | Acc: 43.520% (10222/23488)\n",
      "Loss: 1.556 | Acc: 43.499% (10245/23552)\n",
      "Loss: 1.556 | Acc: 43.509% (10275/23616)\n",
      "Loss: 1.555 | Acc: 43.514% (10304/23680)\n",
      "Loss: 1.555 | Acc: 43.523% (10334/23744)\n",
      "Loss: 1.555 | Acc: 43.519% (10361/23808)\n",
      "Loss: 1.555 | Acc: 43.528% (10391/23872)\n",
      "Loss: 1.554 | Acc: 43.549% (10424/23936)\n",
      "Loss: 1.554 | Acc: 43.562% (10455/24000)\n",
      "Loss: 1.554 | Acc: 43.551% (10480/24064)\n",
      "Loss: 1.555 | Acc: 43.526% (10502/24128)\n",
      "Loss: 1.555 | Acc: 43.531% (10531/24192)\n",
      "Loss: 1.554 | Acc: 43.536% (10560/24256)\n",
      "Loss: 1.555 | Acc: 43.528% (10586/24320)\n",
      "Loss: 1.555 | Acc: 43.524% (10613/24384)\n",
      "Loss: 1.555 | Acc: 43.525% (10641/24448)\n",
      "Loss: 1.555 | Acc: 43.509% (10665/24512)\n",
      "Loss: 1.554 | Acc: 43.526% (10697/24576)\n",
      "Loss: 1.554 | Acc: 43.531% (10726/24640)\n",
      "Loss: 1.554 | Acc: 43.535% (10755/24704)\n",
      "Loss: 1.554 | Acc: 43.552% (10787/24768)\n",
      "Loss: 1.553 | Acc: 43.569% (10819/24832)\n",
      "Loss: 1.553 | Acc: 43.593% (10853/24896)\n",
      "Loss: 1.553 | Acc: 43.586% (10879/24960)\n",
      "Loss: 1.554 | Acc: 43.570% (10903/25024)\n",
      "Loss: 1.553 | Acc: 43.603% (10939/25088)\n",
      "Loss: 1.553 | Acc: 43.615% (10970/25152)\n",
      "Loss: 1.553 | Acc: 43.607% (10996/25216)\n",
      "Loss: 1.553 | Acc: 43.612% (11025/25280)\n",
      "Loss: 1.553 | Acc: 43.608% (11052/25344)\n",
      "Loss: 1.553 | Acc: 43.612% (11081/25408)\n",
      "Loss: 1.553 | Acc: 43.620% (11111/25472)\n",
      "Loss: 1.553 | Acc: 43.617% (11138/25536)\n",
      "Loss: 1.554 | Acc: 43.617% (11166/25600)\n",
      "Loss: 1.554 | Acc: 43.614% (11193/25664)\n",
      "Loss: 1.554 | Acc: 43.602% (11218/25728)\n",
      "Loss: 1.554 | Acc: 43.618% (11250/25792)\n",
      "Loss: 1.554 | Acc: 43.599% (11273/25856)\n",
      "Loss: 1.554 | Acc: 43.596% (11300/25920)\n",
      "Loss: 1.554 | Acc: 43.581% (11324/25984)\n",
      "Loss: 1.554 | Acc: 43.577% (11351/26048)\n",
      "Loss: 1.554 | Acc: 43.593% (11383/26112)\n",
      "Loss: 1.554 | Acc: 43.593% (11411/26176)\n",
      "Loss: 1.554 | Acc: 43.598% (11440/26240)\n",
      "Loss: 1.554 | Acc: 43.613% (11472/26304)\n",
      "Loss: 1.554 | Acc: 43.629% (11504/26368)\n",
      "Loss: 1.554 | Acc: 43.614% (11528/26432)\n",
      "Loss: 1.554 | Acc: 43.607% (11554/26496)\n",
      "Loss: 1.554 | Acc: 43.611% (11583/26560)\n",
      "Loss: 1.553 | Acc: 43.630% (11616/26624)\n",
      "Loss: 1.553 | Acc: 43.641% (11647/26688)\n",
      "Loss: 1.553 | Acc: 43.623% (11670/26752)\n",
      "Loss: 1.553 | Acc: 43.638% (11702/26816)\n",
      "Loss: 1.553 | Acc: 43.642% (11731/26880)\n",
      "Loss: 1.553 | Acc: 43.650% (11761/26944)\n",
      "Loss: 1.553 | Acc: 43.657% (11791/27008)\n",
      "Loss: 1.553 | Acc: 43.654% (11818/27072)\n",
      "Loss: 1.552 | Acc: 43.651% (11845/27136)\n",
      "Loss: 1.553 | Acc: 43.625% (11866/27200)\n",
      "Loss: 1.552 | Acc: 43.644% (11899/27264)\n",
      "Loss: 1.552 | Acc: 43.662% (11932/27328)\n",
      "Loss: 1.552 | Acc: 43.670% (11962/27392)\n",
      "Loss: 1.552 | Acc: 43.659% (11987/27456)\n",
      "Loss: 1.552 | Acc: 43.645% (12011/27520)\n",
      "Loss: 1.552 | Acc: 43.638% (12037/27584)\n",
      "Loss: 1.552 | Acc: 43.649% (12068/27648)\n",
      "Loss: 1.552 | Acc: 43.645% (12095/27712)\n",
      "Loss: 1.552 | Acc: 43.664% (12128/27776)\n",
      "Loss: 1.551 | Acc: 43.696% (12165/27840)\n",
      "Loss: 1.551 | Acc: 43.703% (12195/27904)\n",
      "Loss: 1.551 | Acc: 43.721% (12228/27968)\n",
      "Loss: 1.551 | Acc: 43.721% (12256/28032)\n",
      "Loss: 1.551 | Acc: 43.736% (12288/28096)\n",
      "Loss: 1.551 | Acc: 43.750% (12320/28160)\n",
      "Loss: 1.551 | Acc: 43.750% (12348/28224)\n",
      "Loss: 1.551 | Acc: 43.754% (12377/28288)\n",
      "Loss: 1.551 | Acc: 43.757% (12406/28352)\n",
      "Loss: 1.551 | Acc: 43.754% (12433/28416)\n",
      "Loss: 1.551 | Acc: 43.771% (12466/28480)\n",
      "Loss: 1.551 | Acc: 43.757% (12490/28544)\n",
      "Loss: 1.551 | Acc: 43.747% (12515/28608)\n",
      "Loss: 1.551 | Acc: 43.767% (12549/28672)\n",
      "Loss: 1.550 | Acc: 43.806% (12588/28736)\n",
      "Loss: 1.550 | Acc: 43.823% (12621/28800)\n",
      "Loss: 1.550 | Acc: 43.823% (12649/28864)\n",
      "Loss: 1.550 | Acc: 43.812% (12674/28928)\n",
      "Loss: 1.550 | Acc: 43.802% (12699/28992)\n",
      "Loss: 1.550 | Acc: 43.805% (12728/29056)\n",
      "Loss: 1.550 | Acc: 43.802% (12755/29120)\n",
      "Loss: 1.550 | Acc: 43.805% (12784/29184)\n",
      "Loss: 1.550 | Acc: 43.815% (12815/29248)\n",
      "Loss: 1.550 | Acc: 43.798% (12838/29312)\n",
      "Loss: 1.550 | Acc: 43.784% (12862/29376)\n",
      "Loss: 1.550 | Acc: 43.801% (12895/29440)\n",
      "Loss: 1.550 | Acc: 43.825% (12930/29504)\n",
      "Loss: 1.550 | Acc: 43.824% (12958/29568)\n",
      "Loss: 1.550 | Acc: 43.824% (12986/29632)\n",
      "Loss: 1.550 | Acc: 43.821% (13013/29696)\n",
      "Loss: 1.550 | Acc: 43.821% (13041/29760)\n",
      "Loss: 1.549 | Acc: 43.837% (13074/29824)\n",
      "Loss: 1.549 | Acc: 43.847% (13105/29888)\n",
      "Loss: 1.549 | Acc: 43.853% (13135/29952)\n",
      "Loss: 1.549 | Acc: 43.860% (13165/30016)\n",
      "Loss: 1.548 | Acc: 43.873% (13197/30080)\n",
      "Loss: 1.549 | Acc: 43.873% (13225/30144)\n",
      "Loss: 1.549 | Acc: 43.882% (13256/30208)\n",
      "Loss: 1.548 | Acc: 43.912% (13293/30272)\n",
      "Loss: 1.548 | Acc: 43.918% (13323/30336)\n",
      "Loss: 1.549 | Acc: 43.908% (13348/30400)\n",
      "Loss: 1.549 | Acc: 43.908% (13376/30464)\n",
      "Loss: 1.549 | Acc: 43.904% (13403/30528)\n",
      "Loss: 1.549 | Acc: 43.897% (13429/30592)\n",
      "Loss: 1.549 | Acc: 43.903% (13459/30656)\n",
      "Loss: 1.549 | Acc: 43.903% (13487/30720)\n",
      "Loss: 1.549 | Acc: 43.893% (13512/30784)\n",
      "Loss: 1.549 | Acc: 43.886% (13538/30848)\n",
      "Loss: 1.549 | Acc: 43.883% (13565/30912)\n",
      "Loss: 1.549 | Acc: 43.895% (13597/30976)\n",
      "Loss: 1.549 | Acc: 43.895% (13625/31040)\n",
      "Loss: 1.549 | Acc: 43.898% (13654/31104)\n",
      "Loss: 1.548 | Acc: 43.891% (13680/31168)\n",
      "Loss: 1.549 | Acc: 43.888% (13707/31232)\n",
      "Loss: 1.549 | Acc: 43.865% (13728/31296)\n",
      "Loss: 1.549 | Acc: 43.871% (13758/31360)\n",
      "Loss: 1.549 | Acc: 43.874% (13787/31424)\n",
      "Loss: 1.549 | Acc: 43.890% (13820/31488)\n",
      "Loss: 1.549 | Acc: 43.899% (13851/31552)\n",
      "Loss: 1.549 | Acc: 43.886% (13875/31616)\n",
      "Loss: 1.550 | Acc: 43.867% (13897/31680)\n",
      "Loss: 1.550 | Acc: 43.863% (13924/31744)\n",
      "Loss: 1.549 | Acc: 43.888% (13960/31808)\n",
      "Loss: 1.549 | Acc: 43.894% (13990/31872)\n",
      "Loss: 1.549 | Acc: 43.882% (14014/31936)\n",
      "Loss: 1.549 | Acc: 43.888% (14044/32000)\n",
      "Loss: 1.550 | Acc: 43.856% (14062/32064)\n",
      "Loss: 1.550 | Acc: 43.850% (14088/32128)\n",
      "Loss: 1.549 | Acc: 43.865% (14121/32192)\n",
      "Loss: 1.550 | Acc: 43.849% (14144/32256)\n",
      "Loss: 1.550 | Acc: 43.834% (14167/32320)\n",
      "Loss: 1.550 | Acc: 43.836% (14196/32384)\n",
      "Loss: 1.550 | Acc: 43.842% (14226/32448)\n",
      "Loss: 1.550 | Acc: 43.830% (14250/32512)\n",
      "Loss: 1.549 | Acc: 43.851% (14285/32576)\n",
      "Loss: 1.549 | Acc: 43.860% (14316/32640)\n",
      "Loss: 1.549 | Acc: 43.845% (14339/32704)\n",
      "Loss: 1.549 | Acc: 43.845% (14367/32768)\n",
      "Loss: 1.549 | Acc: 43.841% (14394/32832)\n",
      "Loss: 1.549 | Acc: 43.847% (14424/32896)\n",
      "Loss: 1.550 | Acc: 43.838% (14449/32960)\n",
      "Loss: 1.550 | Acc: 43.838% (14477/33024)\n",
      "Loss: 1.549 | Acc: 43.850% (14509/33088)\n",
      "Loss: 1.549 | Acc: 43.853% (14538/33152)\n",
      "Loss: 1.549 | Acc: 43.846% (14564/33216)\n",
      "Loss: 1.550 | Acc: 43.849% (14593/33280)\n",
      "Loss: 1.550 | Acc: 43.840% (14618/33344)\n",
      "Loss: 1.550 | Acc: 43.831% (14643/33408)\n",
      "Loss: 1.550 | Acc: 43.837% (14673/33472)\n",
      "Loss: 1.550 | Acc: 43.833% (14700/33536)\n",
      "Loss: 1.550 | Acc: 43.851% (14734/33600)\n",
      "Loss: 1.550 | Acc: 43.839% (14758/33664)\n",
      "Loss: 1.550 | Acc: 43.836% (14785/33728)\n",
      "Loss: 1.550 | Acc: 43.827% (14810/33792)\n",
      "Loss: 1.550 | Acc: 43.827% (14838/33856)\n",
      "Loss: 1.550 | Acc: 43.824% (14865/33920)\n",
      "Loss: 1.551 | Acc: 43.806% (14887/33984)\n",
      "Loss: 1.551 | Acc: 43.797% (14912/34048)\n",
      "Loss: 1.551 | Acc: 43.797% (14940/34112)\n",
      "Loss: 1.551 | Acc: 43.814% (14974/34176)\n",
      "Loss: 1.551 | Acc: 43.811% (15001/34240)\n",
      "Loss: 1.551 | Acc: 43.794% (15023/34304)\n",
      "Loss: 1.551 | Acc: 43.767% (15042/34368)\n",
      "Loss: 1.551 | Acc: 43.762% (15068/34432)\n",
      "Loss: 1.551 | Acc: 43.756% (15094/34496)\n",
      "Loss: 1.551 | Acc: 43.773% (15128/34560)\n",
      "Loss: 1.551 | Acc: 43.776% (15157/34624)\n",
      "Loss: 1.551 | Acc: 43.785% (15188/34688)\n",
      "Loss: 1.551 | Acc: 43.779% (15214/34752)\n",
      "Loss: 1.550 | Acc: 43.790% (15246/34816)\n",
      "Loss: 1.550 | Acc: 43.804% (15279/34880)\n",
      "Loss: 1.550 | Acc: 43.804% (15307/34944)\n",
      "Loss: 1.550 | Acc: 43.790% (15330/35008)\n",
      "Loss: 1.550 | Acc: 43.798% (15361/35072)\n",
      "Loss: 1.550 | Acc: 43.813% (15394/35136)\n",
      "Loss: 1.551 | Acc: 43.798% (15417/35200)\n",
      "Loss: 1.551 | Acc: 43.787% (15441/35264)\n",
      "Loss: 1.551 | Acc: 43.781% (15467/35328)\n",
      "Loss: 1.551 | Acc: 43.775% (15493/35392)\n",
      "Loss: 1.552 | Acc: 43.764% (15517/35456)\n",
      "Loss: 1.552 | Acc: 43.767% (15546/35520)\n",
      "Loss: 1.552 | Acc: 43.764% (15573/35584)\n",
      "Loss: 1.551 | Acc: 43.784% (15608/35648)\n",
      "Loss: 1.552 | Acc: 43.772% (15632/35712)\n",
      "Loss: 1.552 | Acc: 43.775% (15661/35776)\n",
      "Loss: 1.552 | Acc: 43.781% (15691/35840)\n",
      "Loss: 1.552 | Acc: 43.792% (15723/35904)\n",
      "Loss: 1.552 | Acc: 43.778% (15746/35968)\n",
      "Loss: 1.552 | Acc: 43.792% (15779/36032)\n",
      "Loss: 1.551 | Acc: 43.783% (15804/36096)\n",
      "Loss: 1.551 | Acc: 43.800% (15838/36160)\n",
      "Loss: 1.551 | Acc: 43.797% (15865/36224)\n",
      "Loss: 1.551 | Acc: 43.808% (15897/36288)\n",
      "Loss: 1.551 | Acc: 43.813% (15927/36352)\n",
      "Loss: 1.551 | Acc: 43.813% (15955/36416)\n",
      "Loss: 1.551 | Acc: 43.805% (15980/36480)\n",
      "Loss: 1.551 | Acc: 43.824% (16015/36544)\n",
      "Loss: 1.551 | Acc: 43.807% (16037/36608)\n",
      "Loss: 1.551 | Acc: 43.815% (16068/36672)\n",
      "Loss: 1.551 | Acc: 43.807% (16093/36736)\n",
      "Loss: 1.551 | Acc: 43.791% (16115/36800)\n",
      "Loss: 1.551 | Acc: 43.788% (16142/36864)\n",
      "Loss: 1.551 | Acc: 43.799% (16174/36928)\n",
      "Loss: 1.551 | Acc: 43.782% (16196/36992)\n",
      "Loss: 1.551 | Acc: 43.788% (16226/37056)\n",
      "Loss: 1.551 | Acc: 43.780% (16251/37120)\n",
      "Loss: 1.551 | Acc: 43.777% (16278/37184)\n",
      "Loss: 1.551 | Acc: 43.763% (16301/37248)\n",
      "Loss: 1.551 | Acc: 43.761% (16328/37312)\n",
      "Loss: 1.551 | Acc: 43.763% (16357/37376)\n",
      "Loss: 1.551 | Acc: 43.761% (16384/37440)\n",
      "Loss: 1.551 | Acc: 43.763% (16413/37504)\n",
      "Loss: 1.552 | Acc: 43.755% (16438/37568)\n",
      "Loss: 1.552 | Acc: 43.761% (16468/37632)\n",
      "Loss: 1.551 | Acc: 43.779% (16503/37696)\n",
      "Loss: 1.551 | Acc: 43.771% (16528/37760)\n",
      "Loss: 1.552 | Acc: 43.761% (16552/37824)\n",
      "Loss: 1.551 | Acc: 43.766% (16582/37888)\n",
      "Loss: 1.551 | Acc: 43.761% (16608/37952)\n",
      "Loss: 1.551 | Acc: 43.776% (16642/38016)\n",
      "Loss: 1.551 | Acc: 43.774% (16669/38080)\n",
      "Loss: 1.551 | Acc: 43.774% (16697/38144)\n",
      "Loss: 1.551 | Acc: 43.792% (16732/38208)\n",
      "Loss: 1.551 | Acc: 43.784% (16757/38272)\n",
      "Loss: 1.551 | Acc: 43.779% (16783/38336)\n",
      "Loss: 1.552 | Acc: 43.758% (16803/38400)\n",
      "Loss: 1.552 | Acc: 43.776% (16838/38464)\n",
      "Loss: 1.552 | Acc: 43.776% (16866/38528)\n",
      "Loss: 1.552 | Acc: 43.760% (16888/38592)\n",
      "Loss: 1.552 | Acc: 43.773% (16921/38656)\n",
      "Loss: 1.552 | Acc: 43.773% (16949/38720)\n",
      "Loss: 1.552 | Acc: 43.776% (16978/38784)\n",
      "Loss: 1.551 | Acc: 43.789% (17011/38848)\n",
      "Loss: 1.551 | Acc: 43.799% (17043/38912)\n",
      "Loss: 1.551 | Acc: 43.786% (17066/38976)\n",
      "Loss: 1.551 | Acc: 43.791% (17096/39040)\n",
      "Loss: 1.551 | Acc: 43.791% (17124/39104)\n",
      "Loss: 1.551 | Acc: 43.791% (17152/39168)\n",
      "Loss: 1.551 | Acc: 43.801% (17184/39232)\n",
      "Loss: 1.551 | Acc: 43.809% (17215/39296)\n",
      "Loss: 1.551 | Acc: 43.803% (17241/39360)\n",
      "Loss: 1.551 | Acc: 43.798% (17267/39424)\n",
      "Loss: 1.551 | Acc: 43.793% (17293/39488)\n",
      "Loss: 1.550 | Acc: 43.803% (17325/39552)\n",
      "Loss: 1.550 | Acc: 43.800% (17352/39616)\n",
      "Loss: 1.550 | Acc: 43.800% (17380/39680)\n",
      "Loss: 1.550 | Acc: 43.808% (17411/39744)\n",
      "Loss: 1.550 | Acc: 43.813% (17441/39808)\n",
      "Loss: 1.550 | Acc: 43.823% (17473/39872)\n",
      "Loss: 1.550 | Acc: 43.838% (17507/39936)\n",
      "Loss: 1.550 | Acc: 43.840% (17536/40000)\n",
      "Loss: 1.550 | Acc: 43.847% (17567/40064)\n",
      "Loss: 1.550 | Acc: 43.855% (17598/40128)\n",
      "Loss: 1.549 | Acc: 43.859% (17628/40192)\n",
      "Loss: 1.549 | Acc: 43.859% (17656/40256)\n",
      "Loss: 1.550 | Acc: 43.854% (17682/40320)\n",
      "Loss: 1.550 | Acc: 43.842% (17705/40384)\n",
      "Loss: 1.549 | Acc: 43.846% (17735/40448)\n",
      "Loss: 1.549 | Acc: 43.854% (17766/40512)\n",
      "Loss: 1.549 | Acc: 43.868% (17800/40576)\n",
      "Loss: 1.549 | Acc: 43.871% (17829/40640)\n",
      "Loss: 1.549 | Acc: 43.868% (17856/40704)\n",
      "Loss: 1.549 | Acc: 43.868% (17884/40768)\n",
      "Loss: 1.549 | Acc: 43.885% (17919/40832)\n",
      "Loss: 1.548 | Acc: 43.884% (17947/40896)\n",
      "Loss: 1.548 | Acc: 43.884% (17975/40960)\n",
      "Loss: 1.549 | Acc: 43.879% (18001/41024)\n",
      "Loss: 1.549 | Acc: 43.886% (18032/41088)\n",
      "Loss: 1.549 | Acc: 43.886% (18060/41152)\n",
      "Loss: 1.549 | Acc: 43.888% (18089/41216)\n",
      "Loss: 1.548 | Acc: 43.888% (18117/41280)\n",
      "Loss: 1.549 | Acc: 43.876% (18140/41344)\n",
      "Loss: 1.548 | Acc: 43.883% (18171/41408)\n",
      "Loss: 1.548 | Acc: 43.899% (18206/41472)\n",
      "Loss: 1.548 | Acc: 43.909% (18238/41536)\n",
      "Loss: 1.548 | Acc: 43.906% (18265/41600)\n",
      "Loss: 1.548 | Acc: 43.899% (18290/41664)\n",
      "Loss: 1.548 | Acc: 43.903% (18320/41728)\n",
      "Loss: 1.547 | Acc: 43.908% (18350/41792)\n",
      "Loss: 1.548 | Acc: 43.912% (18380/41856)\n",
      "Loss: 1.548 | Acc: 43.910% (18407/41920)\n",
      "Loss: 1.548 | Acc: 43.912% (18436/41984)\n",
      "Loss: 1.548 | Acc: 43.907% (18462/42048)\n",
      "Loss: 1.548 | Acc: 43.907% (18490/42112)\n",
      "Loss: 1.547 | Acc: 43.916% (18522/42176)\n",
      "Loss: 1.547 | Acc: 43.918% (18551/42240)\n",
      "Loss: 1.547 | Acc: 43.925% (18582/42304)\n",
      "Loss: 1.547 | Acc: 43.906% (18602/42368)\n",
      "Loss: 1.547 | Acc: 43.910% (18632/42432)\n",
      "Loss: 1.547 | Acc: 43.915% (18662/42496)\n",
      "Loss: 1.548 | Acc: 43.893% (18681/42560)\n",
      "Loss: 1.548 | Acc: 43.870% (18699/42624)\n",
      "Loss: 1.548 | Acc: 43.879% (18731/42688)\n",
      "Loss: 1.548 | Acc: 43.872% (18756/42752)\n",
      "Loss: 1.549 | Acc: 43.850% (18775/42816)\n",
      "Loss: 1.549 | Acc: 43.829% (18794/42880)\n",
      "Loss: 1.548 | Acc: 43.827% (18821/42944)\n",
      "Loss: 1.548 | Acc: 43.843% (18856/43008)\n",
      "Loss: 1.548 | Acc: 43.848% (18886/43072)\n",
      "Loss: 1.549 | Acc: 43.840% (18911/43136)\n",
      "Loss: 1.549 | Acc: 43.833% (18936/43200)\n",
      "Loss: 1.549 | Acc: 43.822% (18959/43264)\n",
      "Loss: 1.549 | Acc: 43.824% (18988/43328)\n",
      "Loss: 1.549 | Acc: 43.821% (19015/43392)\n",
      "Loss: 1.549 | Acc: 43.840% (19051/43456)\n",
      "Loss: 1.549 | Acc: 43.837% (19078/43520)\n",
      "Loss: 1.549 | Acc: 43.833% (19104/43584)\n",
      "Loss: 1.549 | Acc: 43.826% (19129/43648)\n",
      "Loss: 1.549 | Acc: 43.814% (19152/43712)\n",
      "Loss: 1.548 | Acc: 43.830% (19187/43776)\n",
      "Loss: 1.549 | Acc: 43.818% (19210/43840)\n",
      "Loss: 1.548 | Acc: 43.825% (19241/43904)\n",
      "Loss: 1.548 | Acc: 43.836% (19274/43968)\n",
      "Loss: 1.548 | Acc: 43.843% (19305/44032)\n",
      "Loss: 1.548 | Acc: 43.852% (19337/44096)\n",
      "Loss: 1.548 | Acc: 43.854% (19366/44160)\n",
      "Loss: 1.548 | Acc: 43.854% (19394/44224)\n",
      "Loss: 1.548 | Acc: 43.856% (19423/44288)\n",
      "Loss: 1.548 | Acc: 43.854% (19450/44352)\n",
      "Loss: 1.548 | Acc: 43.842% (19473/44416)\n",
      "Loss: 1.548 | Acc: 43.853% (19506/44480)\n",
      "Loss: 1.548 | Acc: 43.842% (19529/44544)\n",
      "Loss: 1.548 | Acc: 43.851% (19561/44608)\n",
      "Loss: 1.548 | Acc: 43.835% (19582/44672)\n",
      "Loss: 1.548 | Acc: 43.842% (19613/44736)\n",
      "Loss: 1.548 | Acc: 43.830% (19636/44800)\n",
      "Loss: 1.548 | Acc: 43.830% (19664/44864)\n",
      "Loss: 1.548 | Acc: 43.843% (19698/44928)\n",
      "Loss: 1.548 | Acc: 43.843% (19726/44992)\n",
      "Loss: 1.548 | Acc: 43.848% (19756/45056)\n",
      "Loss: 1.548 | Acc: 43.845% (19783/45120)\n",
      "Loss: 1.548 | Acc: 43.854% (19815/45184)\n",
      "Loss: 1.548 | Acc: 43.847% (19840/45248)\n",
      "Loss: 1.548 | Acc: 43.829% (19860/45312)\n",
      "Loss: 1.548 | Acc: 43.814% (19881/45376)\n",
      "Loss: 1.548 | Acc: 43.805% (19905/45440)\n",
      "Loss: 1.549 | Acc: 43.794% (19928/45504)\n",
      "Loss: 1.548 | Acc: 43.811% (19964/45568)\n",
      "Loss: 1.548 | Acc: 43.811% (19992/45632)\n",
      "Loss: 1.548 | Acc: 43.807% (20018/45696)\n",
      "Loss: 1.548 | Acc: 43.807% (20046/45760)\n",
      "Loss: 1.548 | Acc: 43.798% (20070/45824)\n",
      "Loss: 1.548 | Acc: 43.794% (20096/45888)\n",
      "Loss: 1.548 | Acc: 43.807% (20130/45952)\n",
      "Loss: 1.548 | Acc: 43.820% (20164/46016)\n",
      "Loss: 1.547 | Acc: 43.832% (20198/46080)\n",
      "Loss: 1.547 | Acc: 43.854% (20236/46144)\n",
      "Loss: 1.547 | Acc: 43.852% (20263/46208)\n",
      "Loss: 1.547 | Acc: 43.862% (20296/46272)\n",
      "Loss: 1.546 | Acc: 43.888% (20336/46336)\n",
      "Loss: 1.546 | Acc: 43.877% (20359/46400)\n",
      "Loss: 1.546 | Acc: 43.883% (20390/46464)\n",
      "Loss: 1.546 | Acc: 43.896% (20424/46528)\n",
      "Loss: 1.546 | Acc: 43.900% (20454/46592)\n",
      "Loss: 1.546 | Acc: 43.902% (20483/46656)\n",
      "Loss: 1.546 | Acc: 43.900% (20510/46720)\n",
      "Loss: 1.546 | Acc: 43.912% (20544/46784)\n",
      "Loss: 1.545 | Acc: 43.919% (20575/46848)\n",
      "Loss: 1.546 | Acc: 43.906% (20597/46912)\n",
      "Loss: 1.545 | Acc: 43.910% (20627/46976)\n",
      "Loss: 1.545 | Acc: 43.920% (20660/47040)\n",
      "Loss: 1.545 | Acc: 43.918% (20687/47104)\n",
      "Loss: 1.545 | Acc: 43.907% (20710/47168)\n",
      "Loss: 1.546 | Acc: 43.898% (20734/47232)\n",
      "Loss: 1.546 | Acc: 43.890% (20758/47296)\n",
      "Loss: 1.546 | Acc: 43.887% (20785/47360)\n",
      "Loss: 1.546 | Acc: 43.881% (20810/47424)\n",
      "Loss: 1.546 | Acc: 43.883% (20839/47488)\n",
      "Loss: 1.546 | Acc: 43.870% (20861/47552)\n",
      "Loss: 1.546 | Acc: 43.876% (20892/47616)\n",
      "Loss: 1.546 | Acc: 43.874% (20919/47680)\n",
      "Loss: 1.546 | Acc: 43.867% (20944/47744)\n",
      "Loss: 1.547 | Acc: 43.861% (20969/47808)\n",
      "Loss: 1.546 | Acc: 43.867% (21000/47872)\n",
      "Loss: 1.546 | Acc: 43.858% (21024/47936)\n",
      "Loss: 1.546 | Acc: 43.871% (21058/48000)\n",
      "Loss: 1.546 | Acc: 43.869% (21085/48064)\n",
      "Loss: 1.546 | Acc: 43.852% (21105/48128)\n",
      "Loss: 1.547 | Acc: 43.833% (21124/48192)\n",
      "Loss: 1.547 | Acc: 43.827% (21149/48256)\n",
      "Loss: 1.547 | Acc: 43.841% (21184/48320)\n",
      "Loss: 1.547 | Acc: 43.837% (21210/48384)\n",
      "Loss: 1.547 | Acc: 43.814% (21227/48448)\n",
      "Loss: 1.547 | Acc: 43.799% (21248/48512)\n",
      "Loss: 1.547 | Acc: 43.804% (21278/48576)\n",
      "Loss: 1.547 | Acc: 43.808% (21308/48640)\n",
      "Loss: 1.547 | Acc: 43.805% (21335/48704)\n",
      "Loss: 1.547 | Acc: 43.809% (21365/48768)\n",
      "Loss: 1.547 | Acc: 43.799% (21388/48832)\n",
      "Loss: 1.547 | Acc: 43.801% (21417/48896)\n",
      "Loss: 1.547 | Acc: 43.797% (21443/48960)\n",
      "Loss: 1.547 | Acc: 43.792% (21458/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 43.79183673469388\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.431 | Acc: 40.625% (26/64)\n",
      "Loss: 1.437 | Acc: 46.094% (59/128)\n",
      "Loss: 1.496 | Acc: 45.312% (87/192)\n",
      "Loss: 1.570 | Acc: 42.188% (108/256)\n",
      "Loss: 1.542 | Acc: 43.750% (140/320)\n",
      "Loss: 1.561 | Acc: 42.969% (165/384)\n",
      "Loss: 1.570 | Acc: 42.634% (191/448)\n",
      "Loss: 1.567 | Acc: 42.383% (217/512)\n",
      "Loss: 1.545 | Acc: 43.403% (250/576)\n",
      "Loss: 1.547 | Acc: 43.281% (277/640)\n",
      "Loss: 1.560 | Acc: 42.472% (299/704)\n",
      "Loss: 1.556 | Acc: 42.448% (326/768)\n",
      "Loss: 1.559 | Acc: 41.947% (349/832)\n",
      "Loss: 1.548 | Acc: 42.634% (382/896)\n",
      "Loss: 1.546 | Acc: 43.229% (415/960)\n",
      "Loss: 1.538 | Acc: 43.945% (450/1024)\n",
      "Loss: 1.534 | Acc: 44.026% (479/1088)\n",
      "Loss: 1.533 | Acc: 44.010% (507/1152)\n",
      "Loss: 1.530 | Acc: 44.655% (543/1216)\n",
      "Loss: 1.539 | Acc: 44.375% (568/1280)\n",
      "Loss: 1.544 | Acc: 43.601% (586/1344)\n",
      "Loss: 1.543 | Acc: 43.750% (616/1408)\n",
      "Loss: 1.536 | Acc: 43.818% (645/1472)\n",
      "Loss: 1.534 | Acc: 43.945% (675/1536)\n",
      "Loss: 1.535 | Acc: 43.750% (700/1600)\n",
      "Loss: 1.536 | Acc: 43.630% (726/1664)\n",
      "Loss: 1.536 | Acc: 43.808% (757/1728)\n",
      "Loss: 1.539 | Acc: 43.694% (783/1792)\n",
      "Loss: 1.538 | Acc: 43.858% (814/1856)\n",
      "Loss: 1.537 | Acc: 43.958% (844/1920)\n",
      "Loss: 1.540 | Acc: 43.548% (864/1984)\n",
      "Loss: 1.543 | Acc: 43.457% (890/2048)\n",
      "Loss: 1.538 | Acc: 43.608% (921/2112)\n",
      "Loss: 1.537 | Acc: 43.566% (948/2176)\n",
      "Loss: 1.538 | Acc: 43.438% (973/2240)\n",
      "Loss: 1.539 | Acc: 43.533% (1003/2304)\n",
      "Loss: 1.540 | Acc: 43.623% (1033/2368)\n",
      "Loss: 1.541 | Acc: 43.627% (1061/2432)\n",
      "Loss: 1.540 | Acc: 43.710% (1091/2496)\n",
      "Loss: 1.547 | Acc: 43.594% (1116/2560)\n",
      "Loss: 1.548 | Acc: 43.483% (1141/2624)\n",
      "Loss: 1.550 | Acc: 43.564% (1171/2688)\n",
      "Loss: 1.546 | Acc: 43.605% (1200/2752)\n",
      "Loss: 1.547 | Acc: 43.501% (1225/2816)\n",
      "Loss: 1.547 | Acc: 43.333% (1248/2880)\n",
      "Loss: 1.546 | Acc: 43.478% (1280/2944)\n",
      "Loss: 1.543 | Acc: 43.750% (1316/3008)\n",
      "Loss: 1.541 | Acc: 43.880% (1348/3072)\n",
      "Loss: 1.539 | Acc: 43.973% (1379/3136)\n",
      "Loss: 1.535 | Acc: 44.094% (1411/3200)\n",
      "Loss: 1.535 | Acc: 44.271% (1445/3264)\n",
      "Loss: 1.534 | Acc: 44.321% (1475/3328)\n",
      "Loss: 1.534 | Acc: 44.192% (1499/3392)\n",
      "Loss: 1.538 | Acc: 43.953% (1519/3456)\n",
      "Loss: 1.539 | Acc: 43.949% (1547/3520)\n",
      "Loss: 1.539 | Acc: 44.029% (1578/3584)\n",
      "Loss: 1.540 | Acc: 43.997% (1605/3648)\n",
      "Loss: 1.536 | Acc: 44.100% (1637/3712)\n",
      "Loss: 1.537 | Acc: 44.121% (1666/3776)\n",
      "Loss: 1.536 | Acc: 44.115% (1694/3840)\n",
      "Loss: 1.537 | Acc: 44.083% (1721/3904)\n",
      "Loss: 1.537 | Acc: 43.926% (1743/3968)\n",
      "Loss: 1.538 | Acc: 43.948% (1772/4032)\n",
      "Loss: 1.540 | Acc: 43.970% (1801/4096)\n",
      "Loss: 1.544 | Acc: 43.894% (1826/4160)\n",
      "Loss: 1.541 | Acc: 43.939% (1856/4224)\n",
      "Loss: 1.541 | Acc: 43.797% (1878/4288)\n",
      "Loss: 1.539 | Acc: 44.026% (1916/4352)\n",
      "Loss: 1.537 | Acc: 44.090% (1947/4416)\n",
      "Loss: 1.534 | Acc: 44.263% (1983/4480)\n",
      "Loss: 1.535 | Acc: 44.256% (2011/4544)\n",
      "Loss: 1.537 | Acc: 44.227% (2038/4608)\n",
      "Loss: 1.535 | Acc: 44.371% (2073/4672)\n",
      "Loss: 1.534 | Acc: 44.320% (2099/4736)\n",
      "Loss: 1.537 | Acc: 44.229% (2123/4800)\n",
      "Loss: 1.534 | Acc: 44.326% (2156/4864)\n",
      "Loss: 1.534 | Acc: 44.359% (2186/4928)\n",
      "Loss: 1.534 | Acc: 44.311% (2212/4992)\n",
      "Loss: 1.533 | Acc: 44.304% (2240/5056)\n",
      "Loss: 1.536 | Acc: 44.160% (2261/5120)\n",
      "Loss: 1.537 | Acc: 44.174% (2290/5184)\n",
      "Loss: 1.538 | Acc: 44.036% (2311/5248)\n",
      "Loss: 1.537 | Acc: 43.995% (2337/5312)\n",
      "Loss: 1.538 | Acc: 43.955% (2363/5376)\n",
      "Loss: 1.539 | Acc: 43.860% (2386/5440)\n",
      "Loss: 1.540 | Acc: 43.823% (2412/5504)\n",
      "Loss: 1.542 | Acc: 43.750% (2436/5568)\n",
      "Loss: 1.545 | Acc: 43.643% (2458/5632)\n",
      "Loss: 1.546 | Acc: 43.592% (2483/5696)\n",
      "Loss: 1.544 | Acc: 43.681% (2516/5760)\n",
      "Loss: 1.543 | Acc: 43.750% (2548/5824)\n",
      "Loss: 1.545 | Acc: 43.648% (2570/5888)\n",
      "Loss: 1.545 | Acc: 43.649% (2598/5952)\n",
      "Loss: 1.546 | Acc: 43.650% (2626/6016)\n",
      "Loss: 1.547 | Acc: 43.586% (2650/6080)\n",
      "Loss: 1.546 | Acc: 43.636% (2681/6144)\n",
      "Loss: 1.546 | Acc: 43.653% (2710/6208)\n",
      "Loss: 1.546 | Acc: 43.622% (2736/6272)\n",
      "Loss: 1.546 | Acc: 43.671% (2767/6336)\n",
      "Loss: 1.547 | Acc: 43.641% (2793/6400)\n",
      "Loss: 1.549 | Acc: 43.518% (2813/6464)\n",
      "Loss: 1.549 | Acc: 43.551% (2843/6528)\n",
      "Loss: 1.550 | Acc: 43.553% (2871/6592)\n",
      "Loss: 1.549 | Acc: 43.615% (2903/6656)\n",
      "Loss: 1.550 | Acc: 43.616% (2931/6720)\n",
      "Loss: 1.550 | Acc: 43.632% (2960/6784)\n",
      "Loss: 1.547 | Acc: 43.735% (2995/6848)\n",
      "Loss: 1.550 | Acc: 43.678% (3019/6912)\n",
      "Loss: 1.553 | Acc: 43.549% (3038/6976)\n",
      "Loss: 1.553 | Acc: 43.537% (3065/7040)\n",
      "Loss: 1.553 | Acc: 43.553% (3094/7104)\n",
      "Loss: 1.553 | Acc: 43.597% (3125/7168)\n",
      "Loss: 1.555 | Acc: 43.556% (3150/7232)\n",
      "Loss: 1.555 | Acc: 43.490% (3173/7296)\n",
      "Loss: 1.553 | Acc: 43.546% (3205/7360)\n",
      "Loss: 1.555 | Acc: 43.440% (3225/7424)\n",
      "Loss: 1.553 | Acc: 43.483% (3256/7488)\n",
      "Loss: 1.553 | Acc: 43.432% (3280/7552)\n",
      "Loss: 1.554 | Acc: 43.435% (3308/7616)\n",
      "Loss: 1.553 | Acc: 43.490% (3340/7680)\n",
      "Loss: 1.552 | Acc: 43.569% (3374/7744)\n",
      "Loss: 1.550 | Acc: 43.673% (3410/7808)\n",
      "Loss: 1.551 | Acc: 43.598% (3432/7872)\n",
      "Loss: 1.551 | Acc: 43.599% (3460/7936)\n",
      "Loss: 1.552 | Acc: 43.625% (3490/8000)\n",
      "Loss: 1.551 | Acc: 43.700% (3524/8064)\n",
      "Loss: 1.549 | Acc: 43.762% (3557/8128)\n",
      "Loss: 1.549 | Acc: 43.762% (3585/8192)\n",
      "Loss: 1.550 | Acc: 43.714% (3609/8256)\n",
      "Loss: 1.552 | Acc: 43.606% (3628/8320)\n",
      "Loss: 1.551 | Acc: 43.643% (3659/8384)\n",
      "Loss: 1.550 | Acc: 43.703% (3692/8448)\n",
      "Loss: 1.550 | Acc: 43.644% (3715/8512)\n",
      "Loss: 1.551 | Acc: 43.645% (3743/8576)\n",
      "Loss: 1.550 | Acc: 43.704% (3776/8640)\n",
      "Loss: 1.550 | Acc: 43.681% (3802/8704)\n",
      "Loss: 1.551 | Acc: 43.625% (3825/8768)\n",
      "Loss: 1.550 | Acc: 43.603% (3851/8832)\n",
      "Loss: 1.549 | Acc: 43.649% (3883/8896)\n",
      "Loss: 1.548 | Acc: 43.705% (3916/8960)\n",
      "Loss: 1.549 | Acc: 43.728% (3946/9024)\n",
      "Loss: 1.548 | Acc: 43.684% (3970/9088)\n",
      "Loss: 1.548 | Acc: 43.695% (3999/9152)\n",
      "Loss: 1.547 | Acc: 43.783% (4035/9216)\n",
      "Loss: 1.546 | Acc: 43.793% (4064/9280)\n",
      "Loss: 1.546 | Acc: 43.804% (4093/9344)\n",
      "Loss: 1.547 | Acc: 43.771% (4118/9408)\n",
      "Loss: 1.547 | Acc: 43.792% (4148/9472)\n",
      "Loss: 1.547 | Acc: 43.802% (4177/9536)\n",
      "Loss: 1.546 | Acc: 43.844% (4209/9600)\n",
      "Loss: 1.546 | Acc: 43.885% (4241/9664)\n",
      "Loss: 1.546 | Acc: 43.884% (4269/9728)\n",
      "Loss: 1.546 | Acc: 43.852% (4294/9792)\n",
      "Loss: 1.546 | Acc: 43.862% (4323/9856)\n",
      "Loss: 1.546 | Acc: 43.851% (4350/9920)\n",
      "Loss: 1.547 | Acc: 43.840% (4377/9984)\n",
      "Loss: 1.546 | Acc: 43.860% (4386/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 43.86\n",
      "\n",
      "Final train set accuracy is 43.79183673469388\n",
      "Final test set accuracy is 43.86\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
