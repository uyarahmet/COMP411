{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'comp411/assignments/assignment2/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/comp411/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Dropout\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some output activations to zero during the forward pass. In this exercise, you will implement a dropout layer and modify your fully connected network to optionally use dropout.\n",
    "\n",
    "[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== You can safely ignore the message below if you are NOT working on ConvolutionalNetworks.ipynb ===========\n",
      "\tYou will need to compile a Cython extension for a portion of this assignment.\n",
      "\tThe instructions to do this will be given in a section of the notebook below.\n"
     ]
    }
   ],
   "source": [
    "# Setup cell.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from comp411.classifiers.fc_net import *\n",
    "from comp411.data_utils import get_CIFAR10_data\n",
    "from comp411.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from comp411.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # Set default size of plots.\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error.\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32)\n",
      "y_train: (49000,)\n",
      "X_val: (1000, 3, 32, 32)\n",
      "y_val: (1000,)\n",
      "X_test: (1000, 3, 32, 32)\n",
      "y_test: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR-10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout: Forward Pass\n",
    "In the file `comp411/layers.py`, implement the forward pass for dropout. Since dropout behaves differently during training and testing, make sure to implement the operation for both modes.\n",
    "\n",
    "Once you have done so, run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests with p =  0.25\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  3.338019705659094\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.749784\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.4\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  6.651945105840772\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.600796\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.7\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  23.304894461705334\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.30074\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(500, 500) + 10\n",
    "\n",
    "for p in [0.25, 0.4, 0.7]:\n",
    "    out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n",
    "    out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "    print('Running tests with p = ', p)\n",
    "    print('Mean of input: ', x.mean())\n",
    "    print('Mean of train-time output: ', out.mean())\n",
    "    print('Mean of test-time output: ', out_test.mean())\n",
    "    print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "    print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout: Backward Pass\n",
    "In the file `comp411/layers.py`, implement the backward pass for dropout. After doing so, run the following cell to numerically gradient-check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx relative error:  5.44560814873387e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 123}\n",
    "out, cache = dropout_forward(x, dropout_param)\n",
    "dx = dropout_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "# Error should be around e-10 or less.\n",
    "print('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 1:\n",
    "What happens if we do not divide the values being passed through inverse dropout by `p` in the dropout layer? Why does that happen?\n",
    "\n",
    "## Answer:\n",
    "If you neglect to divide by p during training, the model might exhibit suboptimal performance during testing due to the absence of proper scaling for the units. This scaling is essential for the model to effectively generalize to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Networks with Dropout\n",
    "In the file `comp411/classifiers/fc_net.py`, modify your implementation to use dropout. Specifically, if the constructor of the network receives a value that is not 1 for the `dropout_keep_ratio` parameter, then the net should add a dropout layer immediately after every ReLU nonlinearity. After doing so, run the following to numerically gradient-check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with dropout =  1\n",
      "Initial loss:  2.300479089768492\n",
      "W1 relative error: 1.03e-07\n",
      "W2 relative error: 2.21e-05\n",
      "W3 relative error: 4.56e-07\n",
      "b1 relative error: 4.66e-09\n",
      "b2 relative error: 2.09e-09\n",
      "b3 relative error: 1.69e-10\n",
      "\n",
      "Running check with dropout =  0.75\n",
      "Initial loss:  2.3016482157750753\n",
      "W1 relative error: 3.44e-07\n",
      "W2 relative error: 5.01e-06\n",
      "W3 relative error: 1.40e-07\n",
      "b1 relative error: 1.48e-08\n",
      "b2 relative error: 9.48e-10\n",
      "b3 relative error: 1.28e-10\n",
      "\n",
      "Running check with dropout =  0.5\n",
      "Initial loss:  2.294963257976082\n",
      "W1 relative error: 1.20e-07\n",
      "W2 relative error: 5.54e-07\n",
      "W3 relative error: 3.75e-07\n",
      "b1 relative error: 3.35e-09\n",
      "b2 relative error: 4.40e-09\n",
      "b3 relative error: 1.44e-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for dropout_keep_ratio in [1, 0.75, 0.5]:\n",
    "    print('Running check with dropout = ', dropout_keep_ratio)\n",
    "    model = FullyConnectedNet(\n",
    "        [H1, H2],\n",
    "        input_dim=D,\n",
    "        num_classes=C,\n",
    "        weight_scale=5e-2,\n",
    "        dtype=np.float64,\n",
    "        dropout_keep_ratio=dropout_keep_ratio,\n",
    "        seed=123\n",
    "    )\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print('Initial loss: ', loss)\n",
    "\n",
    "    # Relative errors should be around e-6 or less.\n",
    "    # Note that it's fine if for dropout_keep_ratio=1 you have W2 error be on the order of e-5.\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Experiment\n",
    "As an experiment, we will train a pair of two-layer networks on 500 training examples: one will use no dropout, and one will use a keep probability of 0.25. We will then visualize the training and validation accuracies of the two networks over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m dropout_choices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.25\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dropout_keep_ratio \u001b[38;5;129;01min\u001b[39;00m dropout_choices:\n\u001b[1;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m FullyConnectedNet(\n\u001b[0;32m     15\u001b[0m         [\u001b[38;5;241m500\u001b[39m],\n\u001b[0;32m     16\u001b[0m         dropout_keep_ratio\u001b[38;5;241m=\u001b[39mdropout_keep_ratio\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dropout_keep_ratio)\n\u001b[0;32m     20\u001b[0m     solver \u001b[38;5;241m=\u001b[39m Solver(\n\u001b[0;32m     21\u001b[0m         model,\n\u001b[0;32m     22\u001b[0m         small_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m         print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     29\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\COMP411\\Assignment-2\\comp411\\classifiers\\fc_net.py:81\u001b[0m, in \u001b[0;36mFullyConnectedNet.__init__\u001b[1;34m(self, hidden_dims, input_dim, num_classes, dropout_keep_ratio, normalization, reg, weight_scale, dtype, seed)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39mweight_scale, size\u001b[38;5;241m=\u001b[39m(input_dim, hidden_dims[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(hidden_dims[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39mweight_scale, size\u001b[38;5;241m=\u001b[39m(hidden_dims[\u001b[38;5;241m0\u001b[39m], hidden_dims[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(hidden_dims[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW3\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39mweight_scale, size\u001b[38;5;241m=\u001b[39m(hidden_dims[\u001b[38;5;241m1\u001b[39m], num_classes))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Train two identical nets, one with dropout and one without.\n",
    "np.random.seed(231)\n",
    "num_train = 500\n",
    "small_data = {\n",
    "    'X_train': data['X_train'][:num_train],\n",
    "    'y_train': data['y_train'][:num_train],\n",
    "    'X_val': data['X_val'],\n",
    "    'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "dropout_choices = [1, 0.25]\n",
    "for dropout_keep_ratio in dropout_choices:\n",
    "    model = FullyConnectedNet(\n",
    "        [500],\n",
    "        dropout_keep_ratio=dropout_keep_ratio\n",
    "    )\n",
    "    print(dropout_keep_ratio)\n",
    "\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=25,\n",
    "        batch_size=100,\n",
    "        update_rule='adam',\n",
    "        optim_config={'learning_rate': 5e-4,},\n",
    "        verbose=True,\n",
    "        print_every=100\n",
    "    )\n",
    "    solver.train()\n",
    "    solvers[dropout_keep_ratio] = solver\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m val_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dropout_keep_ratio \u001b[38;5;129;01min\u001b[39;00m dropout_choices:\n\u001b[1;32m----> 5\u001b[0m     solver \u001b[38;5;241m=\u001b[39m solvers[dropout_keep_ratio]\n\u001b[0;32m      6\u001b[0m     train_accs\u001b[38;5;241m.\u001b[39mappend(solver\u001b[38;5;241m.\u001b[39mtrain_acc_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m     val_accs\u001b[38;5;241m.\u001b[39mappend(solver\u001b[38;5;241m.\u001b[39mval_acc_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Plot train and validation accuracies of the two models.\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for dropout_keep_ratio in dropout_choices:\n",
    "    solver = solvers[dropout_keep_ratio]\n",
    "    train_accs.append(solver.train_acc_history[-1])\n",
    "    val_accs.append(solver.val_acc_history[-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for dropout_keep_ratio in dropout_choices:\n",
    "    plt.plot(\n",
    "        solvers[dropout_keep_ratio].train_acc_history, 'o', label='%.2f dropout_keep_ratio' % dropout_keep_ratio)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for dropout_keep_ratio in dropout_choices:\n",
    "    plt.plot(\n",
    "        solvers[dropout_keep_ratio].val_acc_history, 'o', label='%.2f dropout_keep_ratio' % dropout_keep_ratio)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Inline Question 2:\n",
    "Compare the validation and training accuracies with and without dropout -- what do your results suggest about dropout as a regularizer?\n",
    "\n",
    "## Answer:\n",
    "When comparing our CNN model with and without dropout, we're checking if dropout helps prevent the model from getting too good at the training data but struggling with new data. If, without dropout, our model is doing much better on training data than on new data (validation), that's a sign of overfitting. However, if we use dropout and see the training and validation scores getting closer, it means dropout is doing its job of preventing overfitting. If the dropout model also has a higher validation score, it suggests our model is getting better at handling new, unseen information. Even though dropout might make training take a bit longer, the benefit of having a more reliable and adaptable model is usually worth it for better real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
